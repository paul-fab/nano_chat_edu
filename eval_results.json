{
  "top50": {
    "wandb_name": "edu-d26-top50",
    "run_ids": [
      "zoebqeam"
    ],
    "host": "192.222.50.180",
    "data_variant": "Full filtered + extra top-50% quality layer",
    "eval_steps": {
      "1000": {
        "core": 0.128,
        "val_bpb": 0.800964,
        "cdpk": {
          "acc": 0.2803,
          "n": 252,
          "d": 899
        },
        "tasks": [
          {
            "task": "hellaswag_zeroshot",
            "shots": 0,
            "type": "multiple_choice",
            "accuracy": 0.402,
            "centered": 0.2027
          },
          {
            "task": "jeopardy",
            "shots": 10,
            "type": "language_modeling",
            "accuracy": 0.0,
            "centered": 0.0
          },
          {
            "task": "bigbench_qa_wikidata",
            "shots": 10,
            "type": "language_modeling",
            "accuracy": 0.138,
            "centered": 0.138
          },
          {
            "task": "arc_easy",
            "shots": 10,
            "type": "multiple_choice",
            "accuracy": 0.534,
            "centered": 0.3787
          },
          {
            "task": "arc_challenge",
            "shots": 10,
            "type": "multiple_choice",
            "accuracy": 0.328,
            "centered": 0.104
          },
          {
            "task": "copa",
            "shots": 0,
            "type": "multiple_choice",
            "accuracy": 0.59,
            "centered": 0.18
          },
          {
            "task": "commonsense_qa",
            "shots": 10,
            "type": "multiple_choice",
            "accuracy": 0.278,
            "centered": 0.0975
          },
          {
            "task": "piqa",
            "shots": 10,
            "type": "multiple_choice",
            "accuracy": 0.666,
            "centered": 0.332
          },
          {
            "task": "openbook_qa",
            "shots": 0,
            "type": "multiple_choice",
            "accuracy": 0.332,
            "centered": 0.1093
          },
          {
            "task": "lambada_openai",
            "shots": 0,
            "type": "language_modeling",
            "accuracy": 0.224,
            "centered": 0.224
          },
          {
            "task": "hellaswag",
            "shots": 10,
            "type": "multiple_choice",
            "accuracy": 0.384,
            "centered": 0.1787
          },
          {
            "task": "winograd",
            "shots": 0,
            "type": "schema",
            "accuracy": 0.5824,
            "centered": 0.1648
          },
          {
            "task": "winogrande",
            "shots": 0,
            "type": "schema",
            "accuracy": 0.526,
            "centered": 0.052
          },
          {
            "task": "bigbench_dyck_languages",
            "shots": 10,
            "type": "language_modeling",
            "accuracy": 0.048,
            "centered": 0.048
          },
          {
            "task": "agi_eval_lsat_ar",
            "shots": 3,
            "type": "multiple_choice",
            "accuracy": 0.213,
            "centered": 0.0163
          },
          {
            "task": "bigbench_cs_algorithms",
            "shots": 10,
            "type": "language_modeling",
            "accuracy": 0.4,
            "centered": 0.4
          },
          {
            "task": "bigbench_operators",
            "shots": 10,
            "type": "language_modeling",
            "accuracy": 0.0571,
            "centered": 0.0571
          },
          {
            "task": "bigbench_repeat_copy_logic",
            "shots": 10,
            "type": "language_modeling",
            "accuracy": 0.0,
            "centered": 0.0
          },
          {
            "task": "squad",
            "shots": 10,
            "type": "language_modeling",
            "accuracy": 0.02,
            "centered": 0.02
          },
          {
            "task": "coqa",
            "shots": 0,
            "type": "language_modeling",
            "accuracy": 0.074,
            "centered": 0.074
          },
          {
            "task": "boolq",
            "shots": 10,
            "type": "multiple_choice",
            "accuracy": 0.564,
            "centered": -0.1474
          },
          {
            "task": "bigbench_language_identification",
            "shots": 10,
            "type": "multiple_choice",
            "accuracy": 0.26,
            "centered": 0.1859
          }
        ]
      },
      "2000": {
        "core": 0.1577,
        "val_bpb": 0.74783,
        "cdpk": {
          "acc": 0.2892,
          "n": 260,
          "d": 899
        },
        "tasks": [
          {
            "task": "hellaswag_zeroshot",
            "shots": 0,
            "type": "multiple_choice",
            "accuracy": 0.456,
            "centered": 0.2747
          },
          {
            "task": "jeopardy",
            "shots": 10,
            "type": "language_modeling",
            "accuracy": 0.004,
            "centered": 0.004
          },
          {
            "task": "bigbench_qa_wikidata",
            "shots": 10,
            "type": "language_modeling",
            "accuracy": 0.16,
            "centered": 0.16
          },
          {
            "task": "arc_easy",
            "shots": 10,
            "type": "multiple_choice",
            "accuracy": 0.634,
            "centered": 0.512
          },
          {
            "task": "arc_challenge",
            "shots": 10,
            "type": "multiple_choice",
            "accuracy": 0.326,
            "centered": 0.1013
          },
          {
            "task": "copa",
            "shots": 0,
            "type": "multiple_choice",
            "accuracy": 0.61,
            "centered": 0.22
          },
          {
            "task": "commonsense_qa",
            "shots": 10,
            "type": "multiple_choice",
            "accuracy": 0.188,
            "centered": -0.015
          },
          {
            "task": "piqa",
            "shots": 10,
            "type": "multiple_choice",
            "accuracy": 0.7,
            "centered": 0.4
          },
          {
            "task": "openbook_qa",
            "shots": 0,
            "type": "multiple_choice",
            "accuracy": 0.346,
            "centered": 0.128
          },
          {
            "task": "lambada_openai",
            "shots": 0,
            "type": "language_modeling",
            "accuracy": 0.266,
            "centered": 0.266
          },
          {
            "task": "hellaswag",
            "shots": 10,
            "type": "multiple_choice",
            "accuracy": 0.462,
            "centered": 0.2827
          },
          {
            "task": "winograd",
            "shots": 0,
            "type": "schema",
            "accuracy": 0.5934,
            "centered": 0.1868
          },
          {
            "task": "winogrande",
            "shots": 0,
            "type": "schema",
            "accuracy": 0.532,
            "centered": 0.064
          },
          {
            "task": "bigbench_dyck_languages",
            "shots": 10,
            "type": "language_modeling",
            "accuracy": 0.014,
            "centered": 0.014
          },
          {
            "task": "agi_eval_lsat_ar",
            "shots": 3,
            "type": "multiple_choice",
            "accuracy": 0.3,
            "centered": 0.125
          },
          {
            "task": "bigbench_cs_algorithms",
            "shots": 10,
            "type": "language_modeling",
            "accuracy": 0.398,
            "centered": 0.398
          },
          {
            "task": "bigbench_operators",
            "shots": 10,
            "type": "language_modeling",
            "accuracy": 0.0714,
            "centered": 0.0714
          },
          {
            "task": "bigbench_repeat_copy_logic",
            "shots": 10,
            "type": "language_modeling",
            "accuracy": 0.0,
            "centered": 0.0
          },
          {
            "task": "squad",
            "shots": 10,
            "type": "language_modeling",
            "accuracy": 0.054,
            "centered": 0.054
          },
          {
            "task": "coqa",
            "shots": 0,
            "type": "language_modeling",
            "accuracy": 0.114,
            "centered": 0.114
          },
          {
            "task": "boolq",
            "shots": 10,
            "type": "multiple_choice",
            "accuracy": 0.586,
            "centered": -0.0895
          },
          {
            "task": "bigbench_language_identification",
            "shots": 10,
            "type": "multiple_choice",
            "accuracy": 0.27,
            "centered": 0.1969
          }
        ]
      },
      "3000": {
        "core": 0.1854,
        "val_bpb": 0.723018,
        "cdpk": {
          "acc": 0.277,
          "n": 249,
          "d": 899
        },
        "tasks": [
          {
            "task": "hellaswag_zeroshot",
            "shots": 0,
            "type": "multiple_choice",
            "accuracy": 0.478,
            "centered": 0.304
          },
          {
            "task": "jeopardy",
            "shots": 10,
            "type": "language_modeling",
            "accuracy": 0.002,
            "centered": 0.002
          },
          {
            "task": "bigbench_qa_wikidata",
            "shots": 10,
            "type": "language_modeling",
            "accuracy": 0.204,
            "centered": 0.204
          },
          {
            "task": "arc_easy",
            "shots": 10,
            "type": "multiple_choice",
            "accuracy": 0.668,
            "centered": 0.5573
          },
          {
            "task": "arc_challenge",
            "shots": 10,
            "type": "multiple_choice",
            "accuracy": 0.392,
            "centered": 0.1893
          },
          {
            "task": "copa",
            "shots": 0,
            "type": "multiple_choice",
            "accuracy": 0.63,
            "centered": 0.26
          },
          {
            "task": "commonsense_qa",
            "shots": 10,
            "type": "multiple_choice",
            "accuracy": 0.272,
            "centered": 0.09
          },
          {
            "task": "piqa",
            "shots": 10,
            "type": "multiple_choice",
            "accuracy": 0.694,
            "centered": 0.388
          },
          {
            "task": "openbook_qa",
            "shots": 0,
            "type": "multiple_choice",
            "accuracy": 0.376,
            "centered": 0.168
          },
          {
            "task": "lambada_openai",
            "shots": 0,
            "type": "language_modeling",
            "accuracy": 0.286,
            "centered": 0.286
          },
          {
            "task": "hellaswag",
            "shots": 10,
            "type": "multiple_choice",
            "accuracy": 0.462,
            "centered": 0.2827
          },
          {
            "task": "winograd",
            "shots": 0,
            "type": "schema",
            "accuracy": 0.6264,
            "centered": 0.2527
          },
          {
            "task": "winogrande",
            "shots": 0,
            "type": "schema",
            "accuracy": 0.53,
            "centered": 0.06
          },
          {
            "task": "bigbench_dyck_languages",
            "shots": 10,
            "type": "language_modeling",
            "accuracy": 0.06,
            "centered": 0.06
          },
          {
            "task": "agi_eval_lsat_ar",
            "shots": 3,
            "type": "multiple_choice",
            "accuracy": 0.2522,
            "centered": 0.0652
          },
          {
            "task": "bigbench_cs_algorithms",
            "shots": 10,
            "type": "language_modeling",
            "accuracy": 0.41,
            "centered": 0.41
          },
          {
            "task": "bigbench_operators",
            "shots": 10,
            "type": "language_modeling",
            "accuracy": 0.1143,
            "centered": 0.1143
          },
          {
            "task": "bigbench_repeat_copy_logic",
            "shots": 10,
            "type": "language_modeling",
            "accuracy": 0.0,
            "centered": 0.0
          },
          {
            "task": "squad",
            "shots": 10,
            "type": "language_modeling",
            "accuracy": 0.086,
            "centered": 0.086
          },
          {
            "task": "coqa",
            "shots": 0,
            "type": "language_modeling",
            "accuracy": 0.126,
            "centered": 0.126
          },
          {
            "task": "boolq",
            "shots": 10,
            "type": "multiple_choice",
            "accuracy": 0.624,
            "centered": 0.0105
          },
          {
            "task": "bigbench_language_identification",
            "shots": 10,
            "type": "multiple_choice",
            "accuracy": 0.238,
            "centered": 0.1617
          }
        ]
      },
      "4000": {
        "core": 0.1932,
        "val_bpb": 0.704092,
        "cdpk": {
          "acc": 0.2814,
          "n": 253,
          "d": 899
        },
        "tasks": [
          {
            "task": "hellaswag_zeroshot",
            "shots": 0,
            "type": "multiple_choice",
            "accuracy": 0.488,
            "centered": 0.3173
          },
          {
            "task": "jeopardy",
            "shots": 10,
            "type": "language_modeling",
            "accuracy": 0.01,
            "centered": 0.01
          },
          {
            "task": "bigbench_qa_wikidata",
            "shots": 10,
            "type": "language_modeling",
            "accuracy": 0.226,
            "centered": 0.226
          },
          {
            "task": "arc_easy",
            "shots": 10,
            "type": "multiple_choice",
            "accuracy": 0.662,
            "centered": 0.5493
          },
          {
            "task": "arc_challenge",
            "shots": 10,
            "type": "multiple_choice",
            "accuracy": 0.386,
            "centered": 0.1813
          },
          {
            "task": "copa",
            "shots": 0,
            "type": "multiple_choice",
            "accuracy": 0.64,
            "centered": 0.28
          },
          {
            "task": "commonsense_qa",
            "shots": 10,
            "type": "multiple_choice",
            "accuracy": 0.27,
            "centered": 0.0875
          },
          {
            "task": "piqa",
            "shots": 10,
            "type": "multiple_choice",
            "accuracy": 0.73,
            "centered": 0.46
          },
          {
            "task": "openbook_qa",
            "shots": 0,
            "type": "multiple_choice",
            "accuracy": 0.392,
            "centered": 0.1893
          },
          {
            "task": "lambada_openai",
            "shots": 0,
            "type": "language_modeling",
            "accuracy": 0.302,
            "centered": 0.302
          },
          {
            "task": "hellaswag",
            "shots": 10,
            "type": "multiple_choice",
            "accuracy": 0.472,
            "centered": 0.296
          },
          {
            "task": "winograd",
            "shots": 0,
            "type": "schema",
            "accuracy": 0.5971,
            "centered": 0.1941
          },
          {
            "task": "winogrande",
            "shots": 0,
            "type": "schema",
            "accuracy": 0.548,
            "centered": 0.096
          },
          {
            "task": "bigbench_dyck_languages",
            "shots": 10,
            "type": "language_modeling",
            "accuracy": 0.048,
            "centered": 0.048
          },
          {
            "task": "agi_eval_lsat_ar",
            "shots": 3,
            "type": "multiple_choice",
            "accuracy": 0.2826,
            "centered": 0.1033
          },
          {
            "task": "bigbench_cs_algorithms",
            "shots": 10,
            "type": "language_modeling",
            "accuracy": 0.422,
            "centered": 0.422
          },
          {
            "task": "bigbench_operators",
            "shots": 10,
            "type": "language_modeling",
            "accuracy": 0.1238,
            "centered": 0.1238
          },
          {
            "task": "bigbench_repeat_copy_logic",
            "shots": 10,
            "type": "language_modeling",
            "accuracy": 0.0,
            "centered": 0.0
          },
          {
            "task": "squad",
            "shots": 10,
            "type": "language_modeling",
            "accuracy": 0.106,
            "centered": 0.106
          },
          {
            "task": "coqa",
            "shots": 0,
            "type": "language_modeling",
            "accuracy": 0.144,
            "centered": 0.144
          },
          {
            "task": "boolq",
            "shots": 10,
            "type": "multiple_choice",
            "accuracy": 0.6,
            "centered": -0.0526
          },
          {
            "task": "bigbench_language_identification",
            "shots": 10,
            "type": "multiple_choice",
            "accuracy": 0.242,
            "centered": 0.1661
          }
        ]
      },
      "5000": {
        "core": 0.2059,
        "val_bpb": 0.686484,
        "cdpk": {
          "acc": 0.2747,
          "n": 247,
          "d": 899
        },
        "tasks": [
          {
            "task": "hellaswag_zeroshot",
            "shots": 0,
            "type": "multiple_choice",
            "accuracy": 0.524,
            "centered": 0.3653
          },
          {
            "task": "jeopardy",
            "shots": 10,
            "type": "language_modeling",
            "accuracy": 0.006,
            "centered": 0.006
          },
          {
            "task": "bigbench_qa_wikidata",
            "shots": 10,
            "type": "language_modeling",
            "accuracy": 0.252,
            "centered": 0.252
          },
          {
            "task": "arc_easy",
            "shots": 10,
            "type": "multiple_choice",
            "accuracy": 0.66,
            "centered": 0.5467
          },
          {
            "task": "arc_challenge",
            "shots": 10,
            "type": "multiple_choice",
            "accuracy": 0.39,
            "centered": 0.1867
          },
          {
            "task": "copa",
            "shots": 0,
            "type": "multiple_choice",
            "accuracy": 0.62,
            "centered": 0.24
          },
          {
            "task": "commonsense_qa",
            "shots": 10,
            "type": "multiple_choice",
            "accuracy": 0.422,
            "centered": 0.2775
          },
          {
            "task": "piqa",
            "shots": 10,
            "type": "multiple_choice",
            "accuracy": 0.71,
            "centered": 0.42
          },
          {
            "task": "openbook_qa",
            "shots": 0,
            "type": "multiple_choice",
            "accuracy": 0.378,
            "centered": 0.1707
          },
          {
            "task": "lambada_openai",
            "shots": 0,
            "type": "language_modeling",
            "accuracy": 0.306,
            "centered": 0.306
          },
          {
            "task": "hellaswag",
            "shots": 10,
            "type": "multiple_choice",
            "accuracy": 0.496,
            "centered": 0.328
          },
          {
            "task": "winograd",
            "shots": 0,
            "type": "schema",
            "accuracy": 0.6374,
            "centered": 0.2747
          },
          {
            "task": "winogrande",
            "shots": 0,
            "type": "schema",
            "accuracy": 0.536,
            "centered": 0.072
          },
          {
            "task": "bigbench_dyck_languages",
            "shots": 10,
            "type": "language_modeling",
            "accuracy": 0.08,
            "centered": 0.08
          },
          {
            "task": "agi_eval_lsat_ar",
            "shots": 3,
            "type": "multiple_choice",
            "accuracy": 0.2435,
            "centered": 0.0543
          },
          {
            "task": "bigbench_cs_algorithms",
            "shots": 10,
            "type": "language_modeling",
            "accuracy": 0.404,
            "centered": 0.404
          },
          {
            "task": "bigbench_operators",
            "shots": 10,
            "type": "language_modeling",
            "accuracy": 0.1381,
            "centered": 0.1381
          },
          {
            "task": "bigbench_repeat_copy_logic",
            "shots": 10,
            "type": "language_modeling",
            "accuracy": 0.0312,
            "centered": 0.0312
          },
          {
            "task": "squad",
            "shots": 10,
            "type": "language_modeling",
            "accuracy": 0.11,
            "centered": 0.11
          },
          {
            "task": "coqa",
            "shots": 0,
            "type": "language_modeling",
            "accuracy": 0.138,
            "centered": 0.138
          },
          {
            "task": "boolq",
            "shots": 10,
            "type": "multiple_choice",
            "accuracy": 0.602,
            "centered": -0.0474
          },
          {
            "task": "bigbench_language_identification",
            "shots": 10,
            "type": "multiple_choice",
            "accuracy": 0.25,
            "centered": 0.1749
          }
        ]
      },
      "6000": {
        "core": 0.2036,
        "val_bpb": 0.668497,
        "cdpk": {
          "acc": 0.2848,
          "n": 256,
          "d": 899
        },
        "tasks": [
          {
            "task": "hellaswag_zeroshot",
            "shots": 0,
            "type": "multiple_choice",
            "accuracy": 0.51,
            "centered": 0.3467
          },
          {
            "task": "jeopardy",
            "shots": 10,
            "type": "language_modeling",
            "accuracy": 0.01,
            "centered": 0.01
          },
          {
            "task": "bigbench_qa_wikidata",
            "shots": 10,
            "type": "language_modeling",
            "accuracy": 0.266,
            "centered": 0.266
          },
          {
            "task": "arc_easy",
            "shots": 10,
            "type": "multiple_choice",
            "accuracy": 0.676,
            "centered": 0.568
          },
          {
            "task": "arc_challenge",
            "shots": 10,
            "type": "multiple_choice",
            "accuracy": 0.404,
            "centered": 0.2053
          },
          {
            "task": "copa",
            "shots": 0,
            "type": "multiple_choice",
            "accuracy": 0.68,
            "centered": 0.36
          },
          {
            "task": "commonsense_qa",
            "shots": 10,
            "type": "multiple_choice",
            "accuracy": 0.31,
            "centered": 0.1375
          },
          {
            "task": "piqa",
            "shots": 10,
            "type": "multiple_choice",
            "accuracy": 0.714,
            "centered": 0.428
          },
          {
            "task": "openbook_qa",
            "shots": 0,
            "type": "multiple_choice",
            "accuracy": 0.388,
            "centered": 0.184
          },
          {
            "task": "lambada_openai",
            "shots": 0,
            "type": "language_modeling",
            "accuracy": 0.28,
            "centered": 0.28
          },
          {
            "task": "hellaswag",
            "shots": 10,
            "type": "multiple_choice",
            "accuracy": 0.498,
            "centered": 0.3307
          },
          {
            "task": "winograd",
            "shots": 0,
            "type": "schema",
            "accuracy": 0.6593,
            "centered": 0.3187
          },
          {
            "task": "winogrande",
            "shots": 0,
            "type": "schema",
            "accuracy": 0.502,
            "centered": 0.004
          },
          {
            "task": "bigbench_dyck_languages",
            "shots": 10,
            "type": "language_modeling",
            "accuracy": 0.11,
            "centered": 0.11
          },
          {
            "task": "agi_eval_lsat_ar",
            "shots": 3,
            "type": "multiple_choice",
            "accuracy": 0.1696,
            "centered": -0.038
          },
          {
            "task": "bigbench_cs_algorithms",
            "shots": 10,
            "type": "language_modeling",
            "accuracy": 0.438,
            "centered": 0.438
          },
          {
            "task": "bigbench_operators",
            "shots": 10,
            "type": "language_modeling",
            "accuracy": 0.1619,
            "centered": 0.1619
          },
          {
            "task": "bigbench_repeat_copy_logic",
            "shots": 10,
            "type": "language_modeling",
            "accuracy": 0.0,
            "centered": 0.0
          },
          {
            "task": "squad",
            "shots": 10,
            "type": "language_modeling",
            "accuracy": 0.154,
            "centered": 0.154
          },
          {
            "task": "coqa",
            "shots": 0,
            "type": "language_modeling",
            "accuracy": 0.18,
            "centered": 0.18
          },
          {
            "task": "boolq",
            "shots": 10,
            "type": "multiple_choice",
            "accuracy": 0.576,
            "centered": -0.1158
          },
          {
            "task": "bigbench_language_identification",
            "shots": 10,
            "type": "multiple_choice",
            "accuracy": 0.228,
            "centered": 0.1507
          }
        ]
      },
      "7000": {
        "core": 0.2067,
        "val_bpb": 0.652993,
        "cdpk": {
          "acc": 0.2859,
          "n": 257,
          "d": 899
        },
        "tasks": [
          {
            "task": "hellaswag_zeroshot",
            "shots": 0,
            "type": "multiple_choice",
            "accuracy": 0.54,
            "centered": 0.3867
          },
          {
            "task": "jeopardy",
            "shots": 10,
            "type": "language_modeling",
            "accuracy": 0.014,
            "centered": 0.014
          },
          {
            "task": "bigbench_qa_wikidata",
            "shots": 10,
            "type": "language_modeling",
            "accuracy": 0.252,
            "centered": 0.252
          },
          {
            "task": "arc_easy",
            "shots": 10,
            "type": "multiple_choice",
            "accuracy": 0.7,
            "centered": 0.6
          },
          {
            "task": "arc_challenge",
            "shots": 10,
            "type": "multiple_choice",
            "accuracy": 0.428,
            "centered": 0.2373
          },
          {
            "task": "copa",
            "shots": 0,
            "type": "multiple_choice",
            "accuracy": 0.66,
            "centered": 0.32
          },
          {
            "task": "commonsense_qa",
            "shots": 10,
            "type": "multiple_choice",
            "accuracy": 0.26,
            "centered": 0.075
          },
          {
            "task": "piqa",
            "shots": 10,
            "type": "multiple_choice",
            "accuracy": 0.738,
            "centered": 0.476
          },
          {
            "task": "openbook_qa",
            "shots": 0,
            "type": "multiple_choice",
            "accuracy": 0.388,
            "centered": 0.184
          },
          {
            "task": "lambada_openai",
            "shots": 0,
            "type": "language_modeling",
            "accuracy": 0.314,
            "centered": 0.314
          },
          {
            "task": "hellaswag",
            "shots": 10,
            "type": "multiple_choice",
            "accuracy": 0.52,
            "centered": 0.36
          },
          {
            "task": "winograd",
            "shots": 0,
            "type": "schema",
            "accuracy": 0.6557,
            "centered": 0.3114
          },
          {
            "task": "winogrande",
            "shots": 0,
            "type": "schema",
            "accuracy": 0.536,
            "centered": 0.072
          },
          {
            "task": "bigbench_dyck_languages",
            "shots": 10,
            "type": "language_modeling",
            "accuracy": 0.1,
            "centered": 0.1
          },
          {
            "task": "agi_eval_lsat_ar",
            "shots": 3,
            "type": "multiple_choice",
            "accuracy": 0.2087,
            "centered": 0.0109
          },
          {
            "task": "bigbench_cs_algorithms",
            "shots": 10,
            "type": "language_modeling",
            "accuracy": 0.382,
            "centered": 0.382
          },
          {
            "task": "bigbench_operators",
            "shots": 10,
            "type": "language_modeling",
            "accuracy": 0.1286,
            "centered": 0.1286
          },
          {
            "task": "bigbench_repeat_copy_logic",
            "shots": 10,
            "type": "language_modeling",
            "accuracy": 0.0,
            "centered": 0.0
          },
          {
            "task": "squad",
            "shots": 10,
            "type": "language_modeling",
            "accuracy": 0.148,
            "centered": 0.148
          },
          {
            "task": "coqa",
            "shots": 0,
            "type": "language_modeling",
            "accuracy": 0.192,
            "centered": 0.192
          },
          {
            "task": "boolq",
            "shots": 10,
            "type": "multiple_choice",
            "accuracy": 0.548,
            "centered": -0.1895
          },
          {
            "task": "bigbench_language_identification",
            "shots": 10,
            "type": "multiple_choice",
            "accuracy": 0.248,
            "centered": 0.1727
          }
        ]
      },
      "8000": {
        "core": 0.2154,
        "val_bpb": 0.639461,
        "cdpk": {
          "acc": 0.2803,
          "n": 252,
          "d": 899
        },
        "tasks": [
          {
            "task": "hellaswag_zeroshot",
            "shots": 0,
            "type": "multiple_choice",
            "accuracy": 0.534,
            "centered": 0.3787
          },
          {
            "task": "jeopardy",
            "shots": 10,
            "type": "language_modeling",
            "accuracy": 0.016,
            "centered": 0.016
          },
          {
            "task": "bigbench_qa_wikidata",
            "shots": 10,
            "type": "language_modeling",
            "accuracy": 0.274,
            "centered": 0.274
          },
          {
            "task": "arc_easy",
            "shots": 10,
            "type": "multiple_choice",
            "accuracy": 0.726,
            "centered": 0.6347
          },
          {
            "task": "arc_challenge",
            "shots": 10,
            "type": "multiple_choice",
            "accuracy": 0.432,
            "centered": 0.2427
          },
          {
            "task": "copa",
            "shots": 0,
            "type": "multiple_choice",
            "accuracy": 0.68,
            "centered": 0.36
          },
          {
            "task": "commonsense_qa",
            "shots": 10,
            "type": "multiple_choice",
            "accuracy": 0.302,
            "centered": 0.1275
          },
          {
            "task": "piqa",
            "shots": 10,
            "type": "multiple_choice",
            "accuracy": 0.722,
            "centered": 0.444
          },
          {
            "task": "openbook_qa",
            "shots": 0,
            "type": "multiple_choice",
            "accuracy": 0.402,
            "centered": 0.2027
          },
          {
            "task": "lambada_openai",
            "shots": 0,
            "type": "language_modeling",
            "accuracy": 0.322,
            "centered": 0.322
          },
          {
            "task": "hellaswag",
            "shots": 10,
            "type": "multiple_choice",
            "accuracy": 0.53,
            "centered": 0.3733
          },
          {
            "task": "winograd",
            "shots": 0,
            "type": "schema",
            "accuracy": 0.663,
            "centered": 0.326
          },
          {
            "task": "winogrande",
            "shots": 0,
            "type": "schema",
            "accuracy": 0.528,
            "centered": 0.056
          },
          {
            "task": "bigbench_dyck_languages",
            "shots": 10,
            "type": "language_modeling",
            "accuracy": 0.1,
            "centered": 0.1
          },
          {
            "task": "agi_eval_lsat_ar",
            "shots": 3,
            "type": "multiple_choice",
            "accuracy": 0.2522,
            "centered": 0.0652
          },
          {
            "task": "bigbench_cs_algorithms",
            "shots": 10,
            "type": "language_modeling",
            "accuracy": 0.386,
            "centered": 0.386
          },
          {
            "task": "bigbench_operators",
            "shots": 10,
            "type": "language_modeling",
            "accuracy": 0.1762,
            "centered": 0.1762
          },
          {
            "task": "bigbench_repeat_copy_logic",
            "shots": 10,
            "type": "language_modeling",
            "accuracy": 0.0,
            "centered": 0.0
          },
          {
            "task": "squad",
            "shots": 10,
            "type": "language_modeling",
            "accuracy": 0.164,
            "centered": 0.164
          },
          {
            "task": "coqa",
            "shots": 0,
            "type": "language_modeling",
            "accuracy": 0.212,
            "centered": 0.212
          },
          {
            "task": "boolq",
            "shots": 10,
            "type": "multiple_choice",
            "accuracy": 0.502,
            "centered": -0.3105
          },
          {
            "task": "bigbench_language_identification",
            "shots": 10,
            "type": "multiple_choice",
            "accuracy": 0.262,
            "centered": 0.1881
          }
        ]
      },
      "9000": {
        "core": 0.2249,
        "val_bpb": 0.629051,
        "cdpk": {
          "acc": 0.2836,
          "n": 255,
          "d": 899
        },
        "tasks": [
          {
            "task": "hellaswag_zeroshot",
            "shots": 0,
            "type": "multiple_choice",
            "accuracy": 0.546,
            "centered": 0.3947
          },
          {
            "task": "jeopardy",
            "shots": 10,
            "type": "language_modeling",
            "accuracy": 0.024,
            "centered": 0.024
          },
          {
            "task": "bigbench_qa_wikidata",
            "shots": 10,
            "type": "language_modeling",
            "accuracy": 0.292,
            "centered": 0.292
          },
          {
            "task": "arc_easy",
            "shots": 10,
            "type": "multiple_choice",
            "accuracy": 0.702,
            "centered": 0.6027
          },
          {
            "task": "arc_challenge",
            "shots": 10,
            "type": "multiple_choice",
            "accuracy": 0.424,
            "centered": 0.232
          },
          {
            "task": "copa",
            "shots": 0,
            "type": "multiple_choice",
            "accuracy": 0.68,
            "centered": 0.36
          },
          {
            "task": "commonsense_qa",
            "shots": 10,
            "type": "multiple_choice",
            "accuracy": 0.232,
            "centered": 0.04
          },
          {
            "task": "piqa",
            "shots": 10,
            "type": "multiple_choice",
            "accuracy": 0.746,
            "centered": 0.492
          },
          {
            "task": "openbook_qa",
            "shots": 0,
            "type": "multiple_choice",
            "accuracy": 0.4,
            "centered": 0.2
          },
          {
            "task": "lambada_openai",
            "shots": 0,
            "type": "language_modeling",
            "accuracy": 0.338,
            "centered": 0.338
          },
          {
            "task": "hellaswag",
            "shots": 10,
            "type": "multiple_choice",
            "accuracy": 0.534,
            "centered": 0.3787
          },
          {
            "task": "winograd",
            "shots": 0,
            "type": "schema",
            "accuracy": 0.674,
            "centered": 0.348
          },
          {
            "task": "winogrande",
            "shots": 0,
            "type": "schema",
            "accuracy": 0.544,
            "centered": 0.088
          },
          {
            "task": "bigbench_dyck_languages",
            "shots": 10,
            "type": "language_modeling",
            "accuracy": 0.114,
            "centered": 0.114
          },
          {
            "task": "agi_eval_lsat_ar",
            "shots": 3,
            "type": "multiple_choice",
            "accuracy": 0.2435,
            "centered": 0.0543
          },
          {
            "task": "bigbench_cs_algorithms",
            "shots": 10,
            "type": "language_modeling",
            "accuracy": 0.378,
            "centered": 0.378
          },
          {
            "task": "bigbench_operators",
            "shots": 10,
            "type": "language_modeling",
            "accuracy": 0.1429,
            "centered": 0.1429
          },
          {
            "task": "bigbench_repeat_copy_logic",
            "shots": 10,
            "type": "language_modeling",
            "accuracy": 0.0312,
            "centered": 0.0312
          },
          {
            "task": "squad",
            "shots": 10,
            "type": "language_modeling",
            "accuracy": 0.198,
            "centered": 0.198
          },
          {
            "task": "coqa",
            "shots": 0,
            "type": "language_modeling",
            "accuracy": 0.208,
            "centered": 0.208
          },
          {
            "task": "boolq",
            "shots": 10,
            "type": "multiple_choice",
            "accuracy": 0.56,
            "centered": -0.1579
          },
          {
            "task": "bigbench_language_identification",
            "shots": 10,
            "type": "multiple_choice",
            "accuracy": 0.264,
            "centered": 0.1903
          }
        ]
      },
      "9196": {
        "core": 0.2239,
        "val_bpb": 0.628162,
        "cdpk": {
          "acc": 0.2803,
          "n": 252,
          "d": 899
        },
        "tasks": [
          {
            "task": "hellaswag_zeroshot",
            "shots": 0,
            "type": "multiple_choice",
            "accuracy": 0.542,
            "centered": 0.3893
          },
          {
            "task": "jeopardy",
            "shots": 10,
            "type": "language_modeling",
            "accuracy": 0.022,
            "centered": 0.022
          },
          {
            "task": "bigbench_qa_wikidata",
            "shots": 10,
            "type": "language_modeling",
            "accuracy": 0.286,
            "centered": 0.286
          },
          {
            "task": "arc_easy",
            "shots": 10,
            "type": "multiple_choice",
            "accuracy": 0.712,
            "centered": 0.616
          },
          {
            "task": "arc_challenge",
            "shots": 10,
            "type": "multiple_choice",
            "accuracy": 0.42,
            "centered": 0.2267
          },
          {
            "task": "copa",
            "shots": 0,
            "type": "multiple_choice",
            "accuracy": 0.7,
            "centered": 0.4
          },
          {
            "task": "commonsense_qa",
            "shots": 10,
            "type": "multiple_choice",
            "accuracy": 0.244,
            "centered": 0.055
          },
          {
            "task": "piqa",
            "shots": 10,
            "type": "multiple_choice",
            "accuracy": 0.748,
            "centered": 0.496
          },
          {
            "task": "openbook_qa",
            "shots": 0,
            "type": "multiple_choice",
            "accuracy": 0.394,
            "centered": 0.192
          },
          {
            "task": "lambada_openai",
            "shots": 0,
            "type": "language_modeling",
            "accuracy": 0.346,
            "centered": 0.346
          },
          {
            "task": "hellaswag",
            "shots": 10,
            "type": "multiple_choice",
            "accuracy": 0.532,
            "centered": 0.376
          },
          {
            "task": "winograd",
            "shots": 0,
            "type": "schema",
            "accuracy": 0.6777,
            "centered": 0.3553
          },
          {
            "task": "winogrande",
            "shots": 0,
            "type": "schema",
            "accuracy": 0.532,
            "centered": 0.064
          },
          {
            "task": "bigbench_dyck_languages",
            "shots": 10,
            "type": "language_modeling",
            "accuracy": 0.116,
            "centered": 0.116
          },
          {
            "task": "agi_eval_lsat_ar",
            "shots": 3,
            "type": "multiple_choice",
            "accuracy": 0.2261,
            "centered": 0.0326
          },
          {
            "task": "bigbench_cs_algorithms",
            "shots": 10,
            "type": "language_modeling",
            "accuracy": 0.376,
            "centered": 0.376
          },
          {
            "task": "bigbench_operators",
            "shots": 10,
            "type": "language_modeling",
            "accuracy": 0.1571,
            "centered": 0.1571
          },
          {
            "task": "bigbench_repeat_copy_logic",
            "shots": 10,
            "type": "language_modeling",
            "accuracy": 0.0,
            "centered": 0.0
          },
          {
            "task": "squad",
            "shots": 10,
            "type": "language_modeling",
            "accuracy": 0.204,
            "centered": 0.204
          },
          {
            "task": "coqa",
            "shots": 0,
            "type": "language_modeling",
            "accuracy": 0.21,
            "centered": 0.21
          },
          {
            "task": "boolq",
            "shots": 10,
            "type": "multiple_choice",
            "accuracy": 0.552,
            "centered": -0.1789
          },
          {
            "task": "bigbench_language_identification",
            "shots": 10,
            "type": "multiple_choice",
            "accuracy": 0.258,
            "centered": 0.1837
          }
        ]
      }
    }
  },
  "top20": {
    "wandb_name": "edu-d26-top20",
    "run_ids": [
      "pb0ezqrx",
      "g9vugh79"
    ],
    "host": "192.222.58.132",
    "data_variant": "Full filtered + extra top-20% quality layer",
    "eval_steps": {
      "1000": {
        "core": 0.1381,
        "val_bpb": 0.753023,
        "cdpk": {
          "acc": 0.2681,
          "n": 241,
          "d": 899
        },
        "tasks": [
          {
            "task": "hellaswag_zeroshot",
            "shots": 0,
            "type": "multiple_choice",
            "accuracy": 0.418,
            "centered": 0.224
          },
          {
            "task": "jeopardy",
            "shots": 10,
            "type": "language_modeling",
            "accuracy": 0.0,
            "centered": 0.0
          },
          {
            "task": "bigbench_qa_wikidata",
            "shots": 10,
            "type": "language_modeling",
            "accuracy": 0.154,
            "centered": 0.154
          },
          {
            "task": "arc_easy",
            "shots": 10,
            "type": "multiple_choice",
            "accuracy": 0.584,
            "centered": 0.4453
          },
          {
            "task": "arc_challenge",
            "shots": 10,
            "type": "multiple_choice",
            "accuracy": 0.302,
            "centered": 0.0693
          },
          {
            "task": "copa",
            "shots": 0,
            "type": "multiple_choice",
            "accuracy": 0.62,
            "centered": 0.24
          },
          {
            "task": "commonsense_qa",
            "shots": 10,
            "type": "multiple_choice",
            "accuracy": 0.278,
            "centered": 0.0975
          },
          {
            "task": "piqa",
            "shots": 10,
            "type": "multiple_choice",
            "accuracy": 0.642,
            "centered": 0.284
          },
          {
            "task": "openbook_qa",
            "shots": 0,
            "type": "multiple_choice",
            "accuracy": 0.328,
            "centered": 0.104
          },
          {
            "task": "lambada_openai",
            "shots": 0,
            "type": "language_modeling",
            "accuracy": 0.266,
            "centered": 0.266
          },
          {
            "task": "hellaswag",
            "shots": 10,
            "type": "multiple_choice",
            "accuracy": 0.38,
            "centered": 0.1733
          },
          {
            "task": "winograd",
            "shots": 0,
            "type": "schema",
            "accuracy": 0.5897,
            "centered": 0.1795
          },
          {
            "task": "winogrande",
            "shots": 0,
            "type": "schema",
            "accuracy": 0.522,
            "centered": 0.044
          },
          {
            "task": "bigbench_dyck_languages",
            "shots": 10,
            "type": "language_modeling",
            "accuracy": 0.03,
            "centered": 0.03
          },
          {
            "task": "agi_eval_lsat_ar",
            "shots": 3,
            "type": "multiple_choice",
            "accuracy": 0.2348,
            "centered": 0.0435
          },
          {
            "task": "bigbench_cs_algorithms",
            "shots": 10,
            "type": "language_modeling",
            "accuracy": 0.368,
            "centered": 0.368
          },
          {
            "task": "bigbench_operators",
            "shots": 10,
            "type": "language_modeling",
            "accuracy": 0.1095,
            "centered": 0.1095
          },
          {
            "task": "bigbench_repeat_copy_logic",
            "shots": 10,
            "type": "language_modeling",
            "accuracy": 0.0,
            "centered": 0.0
          },
          {
            "task": "squad",
            "shots": 10,
            "type": "language_modeling",
            "accuracy": 0.008,
            "centered": 0.008
          },
          {
            "task": "coqa",
            "shots": 0,
            "type": "language_modeling",
            "accuracy": 0.104,
            "centered": 0.104
          },
          {
            "task": "boolq",
            "shots": 10,
            "type": "multiple_choice",
            "accuracy": 0.588,
            "centered": -0.0842
          },
          {
            "task": "bigbench_language_identification",
            "shots": 10,
            "type": "multiple_choice",
            "accuracy": 0.254,
            "centered": 0.1793
          }
        ]
      },
      "2000": {
        "core": 0.1481,
        "val_bpb": 0.702149,
        "cdpk": {
          "acc": 0.2747,
          "n": 247,
          "d": 899
        },
        "tasks": [
          {
            "task": "hellaswag_zeroshot",
            "shots": 0,
            "type": "multiple_choice",
            "accuracy": 0.454,
            "centered": 0.272
          },
          {
            "task": "jeopardy",
            "shots": 10,
            "type": "language_modeling",
            "accuracy": 0.002,
            "centered": 0.002
          },
          {
            "task": "bigbench_qa_wikidata",
            "shots": 10,
            "type": "language_modeling",
            "accuracy": 0.132,
            "centered": 0.132
          },
          {
            "task": "arc_easy",
            "shots": 10,
            "type": "multiple_choice",
            "accuracy": 0.618,
            "centered": 0.4907
          },
          {
            "task": "arc_challenge",
            "shots": 10,
            "type": "multiple_choice",
            "accuracy": 0.332,
            "centered": 0.1093
          },
          {
            "task": "copa",
            "shots": 0,
            "type": "multiple_choice",
            "accuracy": 0.6,
            "centered": 0.2
          },
          {
            "task": "commonsense_qa",
            "shots": 10,
            "type": "multiple_choice",
            "accuracy": 0.194,
            "centered": -0.0075
          },
          {
            "task": "piqa",
            "shots": 10,
            "type": "multiple_choice",
            "accuracy": 0.682,
            "centered": 0.364
          },
          {
            "task": "openbook_qa",
            "shots": 0,
            "type": "multiple_choice",
            "accuracy": 0.36,
            "centered": 0.1467
          },
          {
            "task": "lambada_openai",
            "shots": 0,
            "type": "language_modeling",
            "accuracy": 0.296,
            "centered": 0.296
          },
          {
            "task": "hellaswag",
            "shots": 10,
            "type": "multiple_choice",
            "accuracy": 0.428,
            "centered": 0.2373
          },
          {
            "task": "winograd",
            "shots": 0,
            "type": "schema",
            "accuracy": 0.6154,
            "centered": 0.2308
          },
          {
            "task": "winogrande",
            "shots": 0,
            "type": "schema",
            "accuracy": 0.508,
            "centered": 0.016
          },
          {
            "task": "bigbench_dyck_languages",
            "shots": 10,
            "type": "language_modeling",
            "accuracy": 0.054,
            "centered": 0.054
          },
          {
            "task": "agi_eval_lsat_ar",
            "shots": 3,
            "type": "multiple_choice",
            "accuracy": 0.3,
            "centered": 0.125
          },
          {
            "task": "bigbench_cs_algorithms",
            "shots": 10,
            "type": "language_modeling",
            "accuracy": 0.378,
            "centered": 0.378
          },
          {
            "task": "bigbench_operators",
            "shots": 10,
            "type": "language_modeling",
            "accuracy": 0.0857,
            "centered": 0.0857
          },
          {
            "task": "bigbench_repeat_copy_logic",
            "shots": 10,
            "type": "language_modeling",
            "accuracy": 0.0,
            "centered": 0.0
          },
          {
            "task": "squad",
            "shots": 10,
            "type": "language_modeling",
            "accuracy": 0.036,
            "centered": 0.036
          },
          {
            "task": "coqa",
            "shots": 0,
            "type": "language_modeling",
            "accuracy": 0.134,
            "centered": 0.134
          },
          {
            "task": "boolq",
            "shots": 10,
            "type": "multiple_choice",
            "accuracy": 0.54,
            "centered": -0.2105
          },
          {
            "task": "bigbench_language_identification",
            "shots": 10,
            "type": "multiple_choice",
            "accuracy": 0.242,
            "centered": 0.1661
          }
        ]
      },
      "3000": {
        "core": 0.1806,
        "val_bpb": 0.67608,
        "cdpk": {
          "acc": 0.2836,
          "n": 255,
          "d": 899
        },
        "tasks": [
          {
            "task": "hellaswag_zeroshot",
            "shots": 0,
            "type": "multiple_choice",
            "accuracy": 0.47,
            "centered": 0.2933
          },
          {
            "task": "jeopardy",
            "shots": 10,
            "type": "language_modeling",
            "accuracy": 0.006,
            "centered": 0.006
          },
          {
            "task": "bigbench_qa_wikidata",
            "shots": 10,
            "type": "language_modeling",
            "accuracy": 0.218,
            "centered": 0.218
          },
          {
            "task": "arc_easy",
            "shots": 10,
            "type": "multiple_choice",
            "accuracy": 0.662,
            "centered": 0.5493
          },
          {
            "task": "arc_challenge",
            "shots": 10,
            "type": "multiple_choice",
            "accuracy": 0.39,
            "centered": 0.1867
          },
          {
            "task": "copa",
            "shots": 0,
            "type": "multiple_choice",
            "accuracy": 0.63,
            "centered": 0.26
          },
          {
            "task": "commonsense_qa",
            "shots": 10,
            "type": "multiple_choice",
            "accuracy": 0.258,
            "centered": 0.0725
          },
          {
            "task": "piqa",
            "shots": 10,
            "type": "multiple_choice",
            "accuracy": 0.702,
            "centered": 0.404
          },
          {
            "task": "openbook_qa",
            "shots": 0,
            "type": "multiple_choice",
            "accuracy": 0.338,
            "centered": 0.1173
          },
          {
            "task": "lambada_openai",
            "shots": 0,
            "type": "language_modeling",
            "accuracy": 0.328,
            "centered": 0.328
          },
          {
            "task": "hellaswag",
            "shots": 10,
            "type": "multiple_choice",
            "accuracy": 0.454,
            "centered": 0.272
          },
          {
            "task": "winograd",
            "shots": 0,
            "type": "schema",
            "accuracy": 0.6264,
            "centered": 0.2527
          },
          {
            "task": "winogrande",
            "shots": 0,
            "type": "schema",
            "accuracy": 0.532,
            "centered": 0.064
          },
          {
            "task": "bigbench_dyck_languages",
            "shots": 10,
            "type": "language_modeling",
            "accuracy": 0.114,
            "centered": 0.114
          },
          {
            "task": "agi_eval_lsat_ar",
            "shots": 3,
            "type": "multiple_choice",
            "accuracy": 0.2609,
            "centered": 0.0761
          },
          {
            "task": "bigbench_cs_algorithms",
            "shots": 10,
            "type": "language_modeling",
            "accuracy": 0.3,
            "centered": 0.3
          },
          {
            "task": "bigbench_operators",
            "shots": 10,
            "type": "language_modeling",
            "accuracy": 0.1286,
            "centered": 0.1286
          },
          {
            "task": "bigbench_repeat_copy_logic",
            "shots": 10,
            "type": "language_modeling",
            "accuracy": 0.0,
            "centered": 0.0
          },
          {
            "task": "squad",
            "shots": 10,
            "type": "language_modeling",
            "accuracy": 0.058,
            "centered": 0.058
          },
          {
            "task": "coqa",
            "shots": 0,
            "type": "language_modeling",
            "accuracy": 0.116,
            "centered": 0.116
          },
          {
            "task": "boolq",
            "shots": 10,
            "type": "multiple_choice",
            "accuracy": 0.618,
            "centered": -0.0053
          },
          {
            "task": "bigbench_language_identification",
            "shots": 10,
            "type": "multiple_choice",
            "accuracy": 0.238,
            "centered": 0.1617
          }
        ]
      },
      "4000": {
        "core": 0.1977,
        "val_bpb": 0.652731,
        "cdpk": {
          "acc": 0.2714,
          "n": 244,
          "d": 899
        },
        "tasks": [
          {
            "task": "hellaswag_zeroshot",
            "shots": 0,
            "type": "multiple_choice",
            "accuracy": 0.478,
            "centered": 0.304
          },
          {
            "task": "jeopardy",
            "shots": 10,
            "type": "language_modeling",
            "accuracy": 0.016,
            "centered": 0.016
          },
          {
            "task": "bigbench_qa_wikidata",
            "shots": 10,
            "type": "language_modeling",
            "accuracy": 0.182,
            "centered": 0.182
          },
          {
            "task": "arc_easy",
            "shots": 10,
            "type": "multiple_choice",
            "accuracy": 0.68,
            "centered": 0.5733
          },
          {
            "task": "arc_challenge",
            "shots": 10,
            "type": "multiple_choice",
            "accuracy": 0.386,
            "centered": 0.1813
          },
          {
            "task": "copa",
            "shots": 0,
            "type": "multiple_choice",
            "accuracy": 0.64,
            "centered": 0.28
          },
          {
            "task": "commonsense_qa",
            "shots": 10,
            "type": "multiple_choice",
            "accuracy": 0.33,
            "centered": 0.1625
          },
          {
            "task": "piqa",
            "shots": 10,
            "type": "multiple_choice",
            "accuracy": 0.688,
            "centered": 0.376
          },
          {
            "task": "openbook_qa",
            "shots": 0,
            "type": "multiple_choice",
            "accuracy": 0.36,
            "centered": 0.1467
          },
          {
            "task": "lambada_openai",
            "shots": 0,
            "type": "language_modeling",
            "accuracy": 0.274,
            "centered": 0.274
          },
          {
            "task": "hellaswag",
            "shots": 10,
            "type": "multiple_choice",
            "accuracy": 0.46,
            "centered": 0.28
          },
          {
            "task": "winograd",
            "shots": 0,
            "type": "schema",
            "accuracy": 0.6337,
            "centered": 0.2674
          },
          {
            "task": "winogrande",
            "shots": 0,
            "type": "schema",
            "accuracy": 0.538,
            "centered": 0.076
          },
          {
            "task": "bigbench_dyck_languages",
            "shots": 10,
            "type": "language_modeling",
            "accuracy": 0.108,
            "centered": 0.108
          },
          {
            "task": "agi_eval_lsat_ar",
            "shots": 3,
            "type": "multiple_choice",
            "accuracy": 0.2826,
            "centered": 0.1033
          },
          {
            "task": "bigbench_cs_algorithms",
            "shots": 10,
            "type": "language_modeling",
            "accuracy": 0.426,
            "centered": 0.426
          },
          {
            "task": "bigbench_operators",
            "shots": 10,
            "type": "language_modeling",
            "accuracy": 0.119,
            "centered": 0.119
          },
          {
            "task": "bigbench_repeat_copy_logic",
            "shots": 10,
            "type": "language_modeling",
            "accuracy": 0.0,
            "centered": 0.0
          },
          {
            "task": "squad",
            "shots": 10,
            "type": "language_modeling",
            "accuracy": 0.104,
            "centered": 0.104
          },
          {
            "task": "coqa",
            "shots": 0,
            "type": "language_modeling",
            "accuracy": 0.176,
            "centered": 0.176
          },
          {
            "task": "boolq",
            "shots": 10,
            "type": "multiple_choice",
            "accuracy": 0.62,
            "centered": 0.0
          },
          {
            "task": "bigbench_language_identification",
            "shots": 10,
            "type": "multiple_choice",
            "accuracy": 0.268,
            "centered": 0.1947
          }
        ]
      },
      "5000": {
        "core": 0.2012,
        "val_bpb": 0.631056,
        "cdpk": {
          "acc": 0.2881,
          "n": 259,
          "d": 899
        },
        "tasks": [
          {
            "task": "hellaswag_zeroshot",
            "shots": 0,
            "type": "multiple_choice",
            "accuracy": 0.518,
            "centered": 0.3573
          },
          {
            "task": "jeopardy",
            "shots": 10,
            "type": "language_modeling",
            "accuracy": 0.012,
            "centered": 0.012
          },
          {
            "task": "bigbench_qa_wikidata",
            "shots": 10,
            "type": "language_modeling",
            "accuracy": 0.242,
            "centered": 0.242
          },
          {
            "task": "arc_easy",
            "shots": 10,
            "type": "multiple_choice",
            "accuracy": 0.652,
            "centered": 0.536
          },
          {
            "task": "arc_challenge",
            "shots": 10,
            "type": "multiple_choice",
            "accuracy": 0.406,
            "centered": 0.208
          },
          {
            "task": "copa",
            "shots": 0,
            "type": "multiple_choice",
            "accuracy": 0.65,
            "centered": 0.3
          },
          {
            "task": "commonsense_qa",
            "shots": 10,
            "type": "multiple_choice",
            "accuracy": 0.304,
            "centered": 0.13
          },
          {
            "task": "piqa",
            "shots": 10,
            "type": "multiple_choice",
            "accuracy": 0.73,
            "centered": 0.46
          },
          {
            "task": "openbook_qa",
            "shots": 0,
            "type": "multiple_choice",
            "accuracy": 0.366,
            "centered": 0.1547
          },
          {
            "task": "lambada_openai",
            "shots": 0,
            "type": "language_modeling",
            "accuracy": 0.3,
            "centered": 0.3
          },
          {
            "task": "hellaswag",
            "shots": 10,
            "type": "multiple_choice",
            "accuracy": 0.476,
            "centered": 0.3013
          },
          {
            "task": "winograd",
            "shots": 0,
            "type": "schema",
            "accuracy": 0.663,
            "centered": 0.326
          },
          {
            "task": "winogrande",
            "shots": 0,
            "type": "schema",
            "accuracy": 0.54,
            "centered": 0.08
          },
          {
            "task": "bigbench_dyck_languages",
            "shots": 10,
            "type": "language_modeling",
            "accuracy": 0.096,
            "centered": 0.096
          },
          {
            "task": "agi_eval_lsat_ar",
            "shots": 3,
            "type": "multiple_choice",
            "accuracy": 0.2217,
            "centered": 0.0272
          },
          {
            "task": "bigbench_cs_algorithms",
            "shots": 10,
            "type": "language_modeling",
            "accuracy": 0.418,
            "centered": 0.418
          },
          {
            "task": "bigbench_operators",
            "shots": 10,
            "type": "language_modeling",
            "accuracy": 0.1714,
            "centered": 0.1714
          },
          {
            "task": "bigbench_repeat_copy_logic",
            "shots": 10,
            "type": "language_modeling",
            "accuracy": 0.0,
            "centered": 0.0
          },
          {
            "task": "squad",
            "shots": 10,
            "type": "language_modeling",
            "accuracy": 0.072,
            "centered": 0.072
          },
          {
            "task": "coqa",
            "shots": 0,
            "type": "language_modeling",
            "accuracy": 0.16,
            "centered": 0.16
          },
          {
            "task": "boolq",
            "shots": 10,
            "type": "multiple_choice",
            "accuracy": 0.582,
            "centered": -0.1
          },
          {
            "task": "bigbench_language_identification",
            "shots": 10,
            "type": "multiple_choice",
            "accuracy": 0.25,
            "centered": 0.1749
          }
        ]
      },
      "6000": {
        "core": 0.2113,
        "val_bpb": 0.599841,
        "cdpk": {
          "acc": 0.277,
          "n": 249,
          "d": 899
        },
        "tasks": [
          {
            "task": "hellaswag_zeroshot",
            "shots": 0,
            "type": "multiple_choice",
            "accuracy": 0.528,
            "centered": 0.3707
          },
          {
            "task": "jeopardy",
            "shots": 10,
            "type": "language_modeling",
            "accuracy": 0.022,
            "centered": 0.022
          },
          {
            "task": "bigbench_qa_wikidata",
            "shots": 10,
            "type": "language_modeling",
            "accuracy": 0.218,
            "centered": 0.218
          },
          {
            "task": "arc_easy",
            "shots": 10,
            "type": "multiple_choice",
            "accuracy": 0.698,
            "centered": 0.5973
          },
          {
            "task": "arc_challenge",
            "shots": 10,
            "type": "multiple_choice",
            "accuracy": 0.406,
            "centered": 0.208
          },
          {
            "task": "copa",
            "shots": 0,
            "type": "multiple_choice",
            "accuracy": 0.65,
            "centered": 0.3
          },
          {
            "task": "commonsense_qa",
            "shots": 10,
            "type": "multiple_choice",
            "accuracy": 0.292,
            "centered": 0.115
          },
          {
            "task": "piqa",
            "shots": 10,
            "type": "multiple_choice",
            "accuracy": 0.734,
            "centered": 0.468
          },
          {
            "task": "openbook_qa",
            "shots": 0,
            "type": "multiple_choice",
            "accuracy": 0.374,
            "centered": 0.1653
          },
          {
            "task": "lambada_openai",
            "shots": 0,
            "type": "language_modeling",
            "accuracy": 0.328,
            "centered": 0.328
          },
          {
            "task": "hellaswag",
            "shots": 10,
            "type": "multiple_choice",
            "accuracy": 0.51,
            "centered": 0.3467
          },
          {
            "task": "winograd",
            "shots": 0,
            "type": "schema",
            "accuracy": 0.5971,
            "centered": 0.1941
          },
          {
            "task": "winogrande",
            "shots": 0,
            "type": "schema",
            "accuracy": 0.53,
            "centered": 0.06
          },
          {
            "task": "bigbench_dyck_languages",
            "shots": 10,
            "type": "language_modeling",
            "accuracy": 0.11,
            "centered": 0.11
          },
          {
            "task": "agi_eval_lsat_ar",
            "shots": 3,
            "type": "multiple_choice",
            "accuracy": 0.287,
            "centered": 0.1087
          },
          {
            "task": "bigbench_cs_algorithms",
            "shots": 10,
            "type": "language_modeling",
            "accuracy": 0.418,
            "centered": 0.418
          },
          {
            "task": "bigbench_operators",
            "shots": 10,
            "type": "language_modeling",
            "accuracy": 0.1619,
            "centered": 0.1619
          },
          {
            "task": "bigbench_repeat_copy_logic",
            "shots": 10,
            "type": "language_modeling",
            "accuracy": 0.0,
            "centered": 0.0
          },
          {
            "task": "squad",
            "shots": 10,
            "type": "language_modeling",
            "accuracy": 0.154,
            "centered": 0.154
          },
          {
            "task": "coqa",
            "shots": 0,
            "type": "language_modeling",
            "accuracy": 0.194,
            "centered": 0.194
          },
          {
            "task": "boolq",
            "shots": 10,
            "type": "multiple_choice",
            "accuracy": 0.588,
            "centered": -0.0842
          },
          {
            "task": "bigbench_language_identification",
            "shots": 10,
            "type": "multiple_choice",
            "accuracy": 0.266,
            "centered": 0.1925
          }
        ]
      },
      "7000": {
        "core": 0.2284,
        "val_bpb": 0.6055,
        "cdpk": {
          "acc": 0.267,
          "n": 240,
          "d": 899
        },
        "tasks": [
          {
            "task": "hellaswag_zeroshot",
            "shots": 0,
            "type": "multiple_choice",
            "accuracy": 0.536,
            "centered": 0.3813
          },
          {
            "task": "jeopardy",
            "shots": 10,
            "type": "language_modeling",
            "accuracy": 0.01,
            "centered": 0.01
          },
          {
            "task": "bigbench_qa_wikidata",
            "shots": 10,
            "type": "language_modeling",
            "accuracy": 0.244,
            "centered": 0.244
          },
          {
            "task": "arc_easy",
            "shots": 10,
            "type": "multiple_choice",
            "accuracy": 0.682,
            "centered": 0.576
          },
          {
            "task": "arc_challenge",
            "shots": 10,
            "type": "multiple_choice",
            "accuracy": 0.418,
            "centered": 0.224
          },
          {
            "task": "copa",
            "shots": 0,
            "type": "multiple_choice",
            "accuracy": 0.7,
            "centered": 0.4
          },
          {
            "task": "commonsense_qa",
            "shots": 10,
            "type": "multiple_choice",
            "accuracy": 0.28,
            "centered": 0.1
          },
          {
            "task": "piqa",
            "shots": 10,
            "type": "multiple_choice",
            "accuracy": 0.72,
            "centered": 0.44
          },
          {
            "task": "openbook_qa",
            "shots": 0,
            "type": "multiple_choice",
            "accuracy": 0.384,
            "centered": 0.1787
          },
          {
            "task": "lambada_openai",
            "shots": 0,
            "type": "language_modeling",
            "accuracy": 0.332,
            "centered": 0.332
          },
          {
            "task": "hellaswag",
            "shots": 10,
            "type": "multiple_choice",
            "accuracy": 0.524,
            "centered": 0.3653
          },
          {
            "task": "winograd",
            "shots": 0,
            "type": "schema",
            "accuracy": 0.6374,
            "centered": 0.2747
          },
          {
            "task": "winogrande",
            "shots": 0,
            "type": "schema",
            "accuracy": 0.528,
            "centered": 0.056
          },
          {
            "task": "bigbench_dyck_languages",
            "shots": 10,
            "type": "language_modeling",
            "accuracy": 0.078,
            "centered": 0.078
          },
          {
            "task": "agi_eval_lsat_ar",
            "shots": 3,
            "type": "multiple_choice",
            "accuracy": 0.3043,
            "centered": 0.1304
          },
          {
            "task": "bigbench_cs_algorithms",
            "shots": 10,
            "type": "language_modeling",
            "accuracy": 0.424,
            "centered": 0.424
          },
          {
            "task": "bigbench_operators",
            "shots": 10,
            "type": "language_modeling",
            "accuracy": 0.181,
            "centered": 0.181
          },
          {
            "task": "bigbench_repeat_copy_logic",
            "shots": 10,
            "type": "language_modeling",
            "accuracy": 0.0312,
            "centered": 0.0312
          },
          {
            "task": "squad",
            "shots": 10,
            "type": "language_modeling",
            "accuracy": 0.098,
            "centered": 0.098
          },
          {
            "task": "coqa",
            "shots": 0,
            "type": "language_modeling",
            "accuracy": 0.19,
            "centered": 0.19
          },
          {
            "task": "boolq",
            "shots": 10,
            "type": "multiple_choice",
            "accuracy": 0.668,
            "centered": 0.1263
          },
          {
            "task": "bigbench_language_identification",
            "shots": 10,
            "type": "multiple_choice",
            "accuracy": 0.258,
            "centered": 0.1837
          }
        ]
      },
      "8000": {
        "core": 0.2294,
        "val_bpb": 0.595498,
        "cdpk": {
          "acc": 0.2759,
          "n": 248,
          "d": 899
        },
        "tasks": [
          {
            "task": "hellaswag_zeroshot",
            "shots": 0,
            "type": "multiple_choice",
            "accuracy": 0.542,
            "centered": 0.3893
          },
          {
            "task": "jeopardy",
            "shots": 10,
            "type": "language_modeling",
            "accuracy": 0.01,
            "centered": 0.01
          },
          {
            "task": "bigbench_qa_wikidata",
            "shots": 10,
            "type": "language_modeling",
            "accuracy": 0.264,
            "centered": 0.264
          },
          {
            "task": "arc_easy",
            "shots": 10,
            "type": "multiple_choice",
            "accuracy": 0.706,
            "centered": 0.608
          },
          {
            "task": "arc_challenge",
            "shots": 10,
            "type": "multiple_choice",
            "accuracy": 0.404,
            "centered": 0.2053
          },
          {
            "task": "copa",
            "shots": 0,
            "type": "multiple_choice",
            "accuracy": 0.72,
            "centered": 0.44
          },
          {
            "task": "commonsense_qa",
            "shots": 10,
            "type": "multiple_choice",
            "accuracy": 0.32,
            "centered": 0.15
          },
          {
            "task": "piqa",
            "shots": 10,
            "type": "multiple_choice",
            "accuracy": 0.72,
            "centered": 0.44
          },
          {
            "task": "openbook_qa",
            "shots": 0,
            "type": "multiple_choice",
            "accuracy": 0.406,
            "centered": 0.208
          },
          {
            "task": "lambada_openai",
            "shots": 0,
            "type": "language_modeling",
            "accuracy": 0.364,
            "centered": 0.364
          },
          {
            "task": "hellaswag",
            "shots": 10,
            "type": "multiple_choice",
            "accuracy": 0.54,
            "centered": 0.3867
          },
          {
            "task": "winograd",
            "shots": 0,
            "type": "schema",
            "accuracy": 0.6154,
            "centered": 0.2308
          },
          {
            "task": "winogrande",
            "shots": 0,
            "type": "schema",
            "accuracy": 0.566,
            "centered": 0.132
          },
          {
            "task": "bigbench_dyck_languages",
            "shots": 10,
            "type": "language_modeling",
            "accuracy": 0.072,
            "centered": 0.072
          },
          {
            "task": "agi_eval_lsat_ar",
            "shots": 3,
            "type": "multiple_choice",
            "accuracy": 0.313,
            "centered": 0.1413
          },
          {
            "task": "bigbench_cs_algorithms",
            "shots": 10,
            "type": "language_modeling",
            "accuracy": 0.36,
            "centered": 0.36
          },
          {
            "task": "bigbench_operators",
            "shots": 10,
            "type": "language_modeling",
            "accuracy": 0.181,
            "centered": 0.181
          },
          {
            "task": "bigbench_repeat_copy_logic",
            "shots": 10,
            "type": "language_modeling",
            "accuracy": 0.0,
            "centered": 0.0
          },
          {
            "task": "squad",
            "shots": 10,
            "type": "language_modeling",
            "accuracy": 0.14,
            "centered": 0.14
          },
          {
            "task": "coqa",
            "shots": 0,
            "type": "language_modeling",
            "accuracy": 0.198,
            "centered": 0.198
          },
          {
            "task": "boolq",
            "shots": 10,
            "type": "multiple_choice",
            "accuracy": 0.612,
            "centered": -0.0211
          },
          {
            "task": "bigbench_language_identification",
            "shots": 10,
            "type": "multiple_choice",
            "accuracy": 0.226,
            "centered": 0.1485
          }
        ]
      },
      "9000": {
        "core": 0.2395,
        "val_bpb": 0.582885,
        "cdpk": {
          "acc": 0.2714,
          "n": 244,
          "d": 899
        },
        "tasks": [
          {
            "task": "hellaswag_zeroshot",
            "shots": 0,
            "type": "multiple_choice",
            "accuracy": 0.552,
            "centered": 0.4027
          },
          {
            "task": "jeopardy",
            "shots": 10,
            "type": "language_modeling",
            "accuracy": 0.018,
            "centered": 0.018
          },
          {
            "task": "bigbench_qa_wikidata",
            "shots": 10,
            "type": "language_modeling",
            "accuracy": 0.248,
            "centered": 0.248
          },
          {
            "task": "arc_easy",
            "shots": 10,
            "type": "multiple_choice",
            "accuracy": 0.716,
            "centered": 0.6213
          },
          {
            "task": "arc_challenge",
            "shots": 10,
            "type": "multiple_choice",
            "accuracy": 0.418,
            "centered": 0.224
          },
          {
            "task": "copa",
            "shots": 0,
            "type": "multiple_choice",
            "accuracy": 0.71,
            "centered": 0.42
          },
          {
            "task": "commonsense_qa",
            "shots": 10,
            "type": "multiple_choice",
            "accuracy": 0.316,
            "centered": 0.145
          },
          {
            "task": "piqa",
            "shots": 10,
            "type": "multiple_choice",
            "accuracy": 0.732,
            "centered": 0.464
          },
          {
            "task": "openbook_qa",
            "shots": 0,
            "type": "multiple_choice",
            "accuracy": 0.416,
            "centered": 0.2213
          },
          {
            "task": "lambada_openai",
            "shots": 0,
            "type": "language_modeling",
            "accuracy": 0.37,
            "centered": 0.37
          },
          {
            "task": "hellaswag",
            "shots": 10,
            "type": "multiple_choice",
            "accuracy": 0.538,
            "centered": 0.384
          },
          {
            "task": "winograd",
            "shots": 0,
            "type": "schema",
            "accuracy": 0.6777,
            "centered": 0.3553
          },
          {
            "task": "winogrande",
            "shots": 0,
            "type": "schema",
            "accuracy": 0.542,
            "centered": 0.084
          },
          {
            "task": "bigbench_dyck_languages",
            "shots": 10,
            "type": "language_modeling",
            "accuracy": 0.064,
            "centered": 0.064
          },
          {
            "task": "agi_eval_lsat_ar",
            "shots": 3,
            "type": "multiple_choice",
            "accuracy": 0.3087,
            "centered": 0.1359
          },
          {
            "task": "bigbench_cs_algorithms",
            "shots": 10,
            "type": "language_modeling",
            "accuracy": 0.378,
            "centered": 0.378
          },
          {
            "task": "bigbench_operators",
            "shots": 10,
            "type": "language_modeling",
            "accuracy": 0.1714,
            "centered": 0.1714
          },
          {
            "task": "bigbench_repeat_copy_logic",
            "shots": 10,
            "type": "language_modeling",
            "accuracy": 0.0,
            "centered": 0.0
          },
          {
            "task": "squad",
            "shots": 10,
            "type": "language_modeling",
            "accuracy": 0.142,
            "centered": 0.142
          },
          {
            "task": "coqa",
            "shots": 0,
            "type": "language_modeling",
            "accuracy": 0.218,
            "centered": 0.218
          },
          {
            "task": "boolq",
            "shots": 10,
            "type": "multiple_choice",
            "accuracy": 0.634,
            "centered": 0.0368
          },
          {
            "task": "bigbench_language_identification",
            "shots": 10,
            "type": "multiple_choice",
            "accuracy": 0.242,
            "centered": 0.1661
          }
        ]
      },
      "9196": {
        "core": 0.24,
        "val_bpb": 0.581783,
        "cdpk": {
          "acc": 0.2759,
          "n": 248,
          "d": 899
        },
        "tasks": [
          {
            "task": "hellaswag_zeroshot",
            "shots": 0,
            "type": "multiple_choice",
            "accuracy": 0.546,
            "centered": 0.3947
          },
          {
            "task": "jeopardy",
            "shots": 10,
            "type": "language_modeling",
            "accuracy": 0.02,
            "centered": 0.02
          },
          {
            "task": "bigbench_qa_wikidata",
            "shots": 10,
            "type": "language_modeling",
            "accuracy": 0.248,
            "centered": 0.248
          },
          {
            "task": "arc_easy",
            "shots": 10,
            "type": "multiple_choice",
            "accuracy": 0.71,
            "centered": 0.6133
          },
          {
            "task": "arc_challenge",
            "shots": 10,
            "type": "multiple_choice",
            "accuracy": 0.424,
            "centered": 0.232
          },
          {
            "task": "copa",
            "shots": 0,
            "type": "multiple_choice",
            "accuracy": 0.71,
            "centered": 0.42
          },
          {
            "task": "commonsense_qa",
            "shots": 10,
            "type": "multiple_choice",
            "accuracy": 0.34,
            "centered": 0.175
          },
          {
            "task": "piqa",
            "shots": 10,
            "type": "multiple_choice",
            "accuracy": 0.738,
            "centered": 0.476
          },
          {
            "task": "openbook_qa",
            "shots": 0,
            "type": "multiple_choice",
            "accuracy": 0.412,
            "centered": 0.216
          },
          {
            "task": "lambada_openai",
            "shots": 0,
            "type": "language_modeling",
            "accuracy": 0.364,
            "centered": 0.364
          },
          {
            "task": "hellaswag",
            "shots": 10,
            "type": "multiple_choice",
            "accuracy": 0.532,
            "centered": 0.376
          },
          {
            "task": "winograd",
            "shots": 0,
            "type": "schema",
            "accuracy": 0.674,
            "centered": 0.348
          },
          {
            "task": "winogrande",
            "shots": 0,
            "type": "schema",
            "accuracy": 0.542,
            "centered": 0.084
          },
          {
            "task": "bigbench_dyck_languages",
            "shots": 10,
            "type": "language_modeling",
            "accuracy": 0.052,
            "centered": 0.052
          },
          {
            "task": "agi_eval_lsat_ar",
            "shots": 3,
            "type": "multiple_choice",
            "accuracy": 0.3,
            "centered": 0.125
          },
          {
            "task": "bigbench_cs_algorithms",
            "shots": 10,
            "type": "language_modeling",
            "accuracy": 0.38,
            "centered": 0.38
          },
          {
            "task": "bigbench_operators",
            "shots": 10,
            "type": "language_modeling",
            "accuracy": 0.1667,
            "centered": 0.1667
          },
          {
            "task": "bigbench_repeat_copy_logic",
            "shots": 10,
            "type": "language_modeling",
            "accuracy": 0.0312,
            "centered": 0.0312
          },
          {
            "task": "squad",
            "shots": 10,
            "type": "language_modeling",
            "accuracy": 0.152,
            "centered": 0.152
          },
          {
            "task": "coqa",
            "shots": 0,
            "type": "language_modeling",
            "accuracy": 0.214,
            "centered": 0.214
          },
          {
            "task": "boolq",
            "shots": 10,
            "type": "multiple_choice",
            "accuracy": 0.628,
            "centered": 0.0211
          },
          {
            "task": "bigbench_language_identification",
            "shots": 10,
            "type": "multiple_choice",
            "accuracy": 0.246,
            "centered": 0.1705
          }
        ]
      },
      "9500": {
        "core": 0.213,
        "val_bpb": 0.618771,
        "cdpk": null,
        "tasks": [
          {
            "task": "hellaswag_zeroshot",
            "shots": 0,
            "type": "multiple_choice",
            "accuracy": 0.524,
            "centered": 0.3653
          },
          {
            "task": "jeopardy",
            "shots": 10,
            "type": "language_modeling",
            "accuracy": 0.012,
            "centered": 0.012
          },
          {
            "task": "bigbench_qa_wikidata",
            "shots": 10,
            "type": "language_modeling",
            "accuracy": 0.238,
            "centered": 0.238
          },
          {
            "task": "arc_easy",
            "shots": 10,
            "type": "multiple_choice",
            "accuracy": 0.676,
            "centered": 0.568
          },
          {
            "task": "arc_challenge",
            "shots": 10,
            "type": "multiple_choice",
            "accuracy": 0.402,
            "centered": 0.2027
          },
          {
            "task": "copa",
            "shots": 0,
            "type": "multiple_choice",
            "accuracy": 0.62,
            "centered": 0.24
          },
          {
            "task": "commonsense_qa",
            "shots": 10,
            "type": "multiple_choice",
            "accuracy": 0.242,
            "centered": 0.0525
          },
          {
            "task": "piqa",
            "shots": 10,
            "type": "multiple_choice",
            "accuracy": 0.736,
            "centered": 0.472
          },
          {
            "task": "openbook_qa",
            "shots": 0,
            "type": "multiple_choice",
            "accuracy": 0.384,
            "centered": 0.1787
          },
          {
            "task": "lambada_openai",
            "shots": 0,
            "type": "language_modeling",
            "accuracy": 0.32,
            "centered": 0.32
          },
          {
            "task": "hellaswag",
            "shots": 10,
            "type": "multiple_choice",
            "accuracy": 0.504,
            "centered": 0.3387
          },
          {
            "task": "winograd",
            "shots": 0,
            "type": "schema",
            "accuracy": 0.6557,
            "centered": 0.3114
          },
          {
            "task": "winogrande",
            "shots": 0,
            "type": "schema",
            "accuracy": 0.56,
            "centered": 0.12
          },
          {
            "task": "bigbench_dyck_languages",
            "shots": 10,
            "type": "language_modeling",
            "accuracy": 0.056,
            "centered": 0.056
          },
          {
            "task": "agi_eval_lsat_ar",
            "shots": 3,
            "type": "multiple_choice",
            "accuracy": 0.2913,
            "centered": 0.1141
          },
          {
            "task": "bigbench_cs_algorithms",
            "shots": 10,
            "type": "language_modeling",
            "accuracy": 0.392,
            "centered": 0.392
          },
          {
            "task": "bigbench_operators",
            "shots": 10,
            "type": "language_modeling",
            "accuracy": 0.1905,
            "centered": 0.1905
          },
          {
            "task": "bigbench_repeat_copy_logic",
            "shots": 10,
            "type": "language_modeling",
            "accuracy": 0.0,
            "centered": 0.0
          },
          {
            "task": "squad",
            "shots": 10,
            "type": "language_modeling",
            "accuracy": 0.092,
            "centered": 0.092
          },
          {
            "task": "coqa",
            "shots": 0,
            "type": "language_modeling",
            "accuracy": 0.172,
            "centered": 0.172
          },
          {
            "task": "boolq",
            "shots": 10,
            "type": "multiple_choice",
            "accuracy": 0.644,
            "centered": 0.0632
          },
          {
            "task": "bigbench_language_identification",
            "shots": 10,
            "type": "multiple_choice",
            "accuracy": 0.262,
            "centered": 0.1881
          }
        ]
      },
      "10000": {
        "core": 0.2083,
        "val_bpb": 0.619237,
        "cdpk": {
          "acc": 0.2814,
          "n": 253,
          "d": 899
        },
        "tasks": [
          {
            "task": "hellaswag_zeroshot",
            "shots": 0,
            "type": "multiple_choice",
            "accuracy": 0.514,
            "centered": 0.352
          },
          {
            "task": "jeopardy",
            "shots": 10,
            "type": "language_modeling",
            "accuracy": 0.018,
            "centered": 0.018
          },
          {
            "task": "bigbench_qa_wikidata",
            "shots": 10,
            "type": "language_modeling",
            "accuracy": 0.282,
            "centered": 0.282
          },
          {
            "task": "arc_easy",
            "shots": 10,
            "type": "multiple_choice",
            "accuracy": 0.67,
            "centered": 0.56
          },
          {
            "task": "arc_challenge",
            "shots": 10,
            "type": "multiple_choice",
            "accuracy": 0.452,
            "centered": 0.2693
          },
          {
            "task": "copa",
            "shots": 0,
            "type": "multiple_choice",
            "accuracy": 0.63,
            "centered": 0.26
          },
          {
            "task": "commonsense_qa",
            "shots": 10,
            "type": "multiple_choice",
            "accuracy": 0.336,
            "centered": 0.17
          },
          {
            "task": "piqa",
            "shots": 10,
            "type": "multiple_choice",
            "accuracy": 0.726,
            "centered": 0.452
          },
          {
            "task": "openbook_qa",
            "shots": 0,
            "type": "multiple_choice",
            "accuracy": 0.372,
            "centered": 0.1627
          },
          {
            "task": "lambada_openai",
            "shots": 0,
            "type": "language_modeling",
            "accuracy": 0.312,
            "centered": 0.312
          },
          {
            "task": "hellaswag",
            "shots": 10,
            "type": "multiple_choice",
            "accuracy": 0.514,
            "centered": 0.352
          },
          {
            "task": "winograd",
            "shots": 0,
            "type": "schema",
            "accuracy": 0.6447,
            "centered": 0.2894
          },
          {
            "task": "winogrande",
            "shots": 0,
            "type": "schema",
            "accuracy": 0.544,
            "centered": 0.088
          },
          {
            "task": "bigbench_dyck_languages",
            "shots": 10,
            "type": "language_modeling",
            "accuracy": 0.068,
            "centered": 0.068
          },
          {
            "task": "agi_eval_lsat_ar",
            "shots": 3,
            "type": "multiple_choice",
            "accuracy": 0.3043,
            "centered": 0.1304
          },
          {
            "task": "bigbench_cs_algorithms",
            "shots": 10,
            "type": "language_modeling",
            "accuracy": 0.382,
            "centered": 0.382
          },
          {
            "task": "bigbench_operators",
            "shots": 10,
            "type": "language_modeling",
            "accuracy": 0.1429,
            "centered": 0.1429
          },
          {
            "task": "bigbench_repeat_copy_logic",
            "shots": 10,
            "type": "language_modeling",
            "accuracy": 0.0,
            "centered": 0.0
          },
          {
            "task": "squad",
            "shots": 10,
            "type": "language_modeling",
            "accuracy": 0.122,
            "centered": 0.122
          },
          {
            "task": "coqa",
            "shots": 0,
            "type": "language_modeling",
            "accuracy": 0.188,
            "centered": 0.188
          },
          {
            "task": "boolq",
            "shots": 10,
            "type": "multiple_choice",
            "accuracy": 0.546,
            "centered": -0.1947
          },
          {
            "task": "bigbench_language_identification",
            "shots": 10,
            "type": "multiple_choice",
            "accuracy": 0.252,
            "centered": 0.1771
          }
        ]
      },
      "10500": {
        "core": 0.223,
        "val_bpb": 0.614135,
        "cdpk": null,
        "tasks": [
          {
            "task": "hellaswag_zeroshot",
            "shots": 0,
            "type": "multiple_choice",
            "accuracy": 0.52,
            "centered": 0.36
          },
          {
            "task": "jeopardy",
            "shots": 10,
            "type": "language_modeling",
            "accuracy": 0.01,
            "centered": 0.01
          },
          {
            "task": "bigbench_qa_wikidata",
            "shots": 10,
            "type": "language_modeling",
            "accuracy": 0.322,
            "centered": 0.322
          },
          {
            "task": "arc_easy",
            "shots": 10,
            "type": "multiple_choice",
            "accuracy": 0.696,
            "centered": 0.5947
          },
          {
            "task": "arc_challenge",
            "shots": 10,
            "type": "multiple_choice",
            "accuracy": 0.426,
            "centered": 0.2347
          },
          {
            "task": "copa",
            "shots": 0,
            "type": "multiple_choice",
            "accuracy": 0.63,
            "centered": 0.26
          },
          {
            "task": "commonsense_qa",
            "shots": 10,
            "type": "multiple_choice",
            "accuracy": 0.32,
            "centered": 0.15
          },
          {
            "task": "piqa",
            "shots": 10,
            "type": "multiple_choice",
            "accuracy": 0.732,
            "centered": 0.464
          },
          {
            "task": "openbook_qa",
            "shots": 0,
            "type": "multiple_choice",
            "accuracy": 0.384,
            "centered": 0.1787
          },
          {
            "task": "lambada_openai",
            "shots": 0,
            "type": "language_modeling",
            "accuracy": 0.352,
            "centered": 0.352
          },
          {
            "task": "hellaswag",
            "shots": 10,
            "type": "multiple_choice",
            "accuracy": 0.522,
            "centered": 0.3627
          },
          {
            "task": "winograd",
            "shots": 0,
            "type": "schema",
            "accuracy": 0.6813,
            "centered": 0.3626
          },
          {
            "task": "winogrande",
            "shots": 0,
            "type": "schema",
            "accuracy": 0.516,
            "centered": 0.032
          },
          {
            "task": "bigbench_dyck_languages",
            "shots": 10,
            "type": "language_modeling",
            "accuracy": 0.108,
            "centered": 0.108
          },
          {
            "task": "agi_eval_lsat_ar",
            "shots": 3,
            "type": "multiple_choice",
            "accuracy": 0.313,
            "centered": 0.1413
          },
          {
            "task": "bigbench_cs_algorithms",
            "shots": 10,
            "type": "language_modeling",
            "accuracy": 0.416,
            "centered": 0.416
          },
          {
            "task": "bigbench_operators",
            "shots": 10,
            "type": "language_modeling",
            "accuracy": 0.119,
            "centered": 0.119
          },
          {
            "task": "bigbench_repeat_copy_logic",
            "shots": 10,
            "type": "language_modeling",
            "accuracy": 0.0,
            "centered": 0.0
          },
          {
            "task": "squad",
            "shots": 10,
            "type": "language_modeling",
            "accuracy": 0.102,
            "centered": 0.102
          },
          {
            "task": "coqa",
            "shots": 0,
            "type": "language_modeling",
            "accuracy": 0.192,
            "centered": 0.192
          },
          {
            "task": "boolq",
            "shots": 10,
            "type": "multiple_choice",
            "accuracy": 0.614,
            "centered": -0.0158
          },
          {
            "task": "bigbench_language_identification",
            "shots": 10,
            "type": "multiple_choice",
            "accuracy": 0.236,
            "centered": 0.1595
          }
        ]
      }
    }
  },
  "full-filtered (split)": {
    "wandb_name": "exp-top16-d26-8gpu",
    "run_ids": [
      "dbwtodle",
      "n9hithr0",
      "bk8zeag4"
    ],
    "host": "192.222.53.38",
    "data_variant": "Full filtered (no extra top20/top50 layer)",
    "eval_steps": {
      "1000": {
        "core": 0.1264,
        "val_bpb": 0.723263,
        "cdpk": null,
        "tasks": [
          {
            "task": "hellaswag_zeroshot",
            "shots": 0,
            "type": "multiple_choice",
            "accuracy": 0.404,
            "centered": 0.2053
          },
          {
            "task": "jeopardy",
            "shots": 10,
            "type": "language_modeling",
            "accuracy": 0.002,
            "centered": 0.002
          },
          {
            "task": "bigbench_qa_wikidata",
            "shots": 10,
            "type": "language_modeling",
            "accuracy": 0.18,
            "centered": 0.18
          },
          {
            "task": "arc_easy",
            "shots": 10,
            "type": "multiple_choice",
            "accuracy": 0.532,
            "centered": 0.376
          },
          {
            "task": "arc_challenge",
            "shots": 10,
            "type": "multiple_choice",
            "accuracy": 0.3,
            "centered": 0.0667
          },
          {
            "task": "copa",
            "shots": 0,
            "type": "multiple_choice",
            "accuracy": 0.57,
            "centered": 0.14
          },
          {
            "task": "commonsense_qa",
            "shots": 10,
            "type": "multiple_choice",
            "accuracy": 0.318,
            "centered": 0.1475
          },
          {
            "task": "piqa",
            "shots": 10,
            "type": "multiple_choice",
            "accuracy": 0.66,
            "centered": 0.32
          },
          {
            "task": "openbook_qa",
            "shots": 0,
            "type": "multiple_choice",
            "accuracy": 0.344,
            "centered": 0.1253
          },
          {
            "task": "lambada_openai",
            "shots": 0,
            "type": "language_modeling",
            "accuracy": 0.262,
            "centered": 0.262
          },
          {
            "task": "hellaswag",
            "shots": 10,
            "type": "multiple_choice",
            "accuracy": 0.372,
            "centered": 0.1627
          },
          {
            "task": "winograd",
            "shots": 0,
            "type": "schema",
            "accuracy": 0.6117,
            "centered": 0.2234
          },
          {
            "task": "winogrande",
            "shots": 0,
            "type": "schema",
            "accuracy": 0.52,
            "centered": 0.04
          },
          {
            "task": "bigbench_dyck_languages",
            "shots": 10,
            "type": "language_modeling",
            "accuracy": 0.04,
            "centered": 0.04
          },
          {
            "task": "agi_eval_lsat_ar",
            "shots": 3,
            "type": "multiple_choice",
            "accuracy": 0.2783,
            "centered": 0.0978
          },
          {
            "task": "bigbench_cs_algorithms",
            "shots": 10,
            "type": "language_modeling",
            "accuracy": 0.418,
            "centered": 0.418
          },
          {
            "task": "bigbench_operators",
            "shots": 10,
            "type": "language_modeling",
            "accuracy": 0.1429,
            "centered": 0.1429
          },
          {
            "task": "bigbench_repeat_copy_logic",
            "shots": 10,
            "type": "language_modeling",
            "accuracy": 0.0,
            "centered": 0.0
          },
          {
            "task": "squad",
            "shots": 10,
            "type": "language_modeling",
            "accuracy": 0.044,
            "centered": 0.044
          },
          {
            "task": "coqa",
            "shots": 0,
            "type": "language_modeling",
            "accuracy": 0.1,
            "centered": 0.1
          },
          {
            "task": "boolq",
            "shots": 10,
            "type": "multiple_choice",
            "accuracy": 0.422,
            "centered": -0.5211
          },
          {
            "task": "bigbench_language_identification",
            "shots": 10,
            "type": "multiple_choice",
            "accuracy": 0.28,
            "centered": 0.2079
          }
        ]
      },
      "2000": {
        "core": 0.1641,
        "val_bpb": 0.684965,
        "cdpk": {
          "acc": 0.2781,
          "n": 250,
          "d": 899
        },
        "tasks": [
          {
            "task": "hellaswag_zeroshot",
            "shots": 0,
            "type": "multiple_choice",
            "accuracy": 0.43,
            "centered": 0.24
          },
          {
            "task": "jeopardy",
            "shots": 10,
            "type": "language_modeling",
            "accuracy": 0.002,
            "centered": 0.002
          },
          {
            "task": "bigbench_qa_wikidata",
            "shots": 10,
            "type": "language_modeling",
            "accuracy": 0.2,
            "centered": 0.2
          },
          {
            "task": "arc_easy",
            "shots": 10,
            "type": "multiple_choice",
            "accuracy": 0.624,
            "centered": 0.4987
          },
          {
            "task": "arc_challenge",
            "shots": 10,
            "type": "multiple_choice",
            "accuracy": 0.32,
            "centered": 0.0933
          },
          {
            "task": "copa",
            "shots": 0,
            "type": "multiple_choice",
            "accuracy": 0.6,
            "centered": 0.2
          },
          {
            "task": "commonsense_qa",
            "shots": 10,
            "type": "multiple_choice",
            "accuracy": 0.352,
            "centered": 0.19
          },
          {
            "task": "piqa",
            "shots": 10,
            "type": "multiple_choice",
            "accuracy": 0.706,
            "centered": 0.412
          },
          {
            "task": "openbook_qa",
            "shots": 0,
            "type": "multiple_choice",
            "accuracy": 0.346,
            "centered": 0.128
          },
          {
            "task": "lambada_openai",
            "shots": 0,
            "type": "language_modeling",
            "accuracy": 0.254,
            "centered": 0.254
          },
          {
            "task": "hellaswag",
            "shots": 10,
            "type": "multiple_choice",
            "accuracy": 0.432,
            "centered": 0.2427
          },
          {
            "task": "winograd",
            "shots": 0,
            "type": "schema",
            "accuracy": 0.5971,
            "centered": 0.1941
          },
          {
            "task": "winogrande",
            "shots": 0,
            "type": "schema",
            "accuracy": 0.514,
            "centered": 0.028
          },
          {
            "task": "bigbench_dyck_languages",
            "shots": 10,
            "type": "language_modeling",
            "accuracy": 0.076,
            "centered": 0.076
          },
          {
            "task": "agi_eval_lsat_ar",
            "shots": 3,
            "type": "multiple_choice",
            "accuracy": 0.2957,
            "centered": 0.1196
          },
          {
            "task": "bigbench_cs_algorithms",
            "shots": 10,
            "type": "language_modeling",
            "accuracy": 0.414,
            "centered": 0.414
          },
          {
            "task": "bigbench_operators",
            "shots": 10,
            "type": "language_modeling",
            "accuracy": 0.1667,
            "centered": 0.1667
          },
          {
            "task": "bigbench_repeat_copy_logic",
            "shots": 10,
            "type": "language_modeling",
            "accuracy": 0.0312,
            "centered": 0.0312
          },
          {
            "task": "squad",
            "shots": 10,
            "type": "language_modeling",
            "accuracy": 0.066,
            "centered": 0.066
          },
          {
            "task": "coqa",
            "shots": 0,
            "type": "language_modeling",
            "accuracy": 0.144,
            "centered": 0.144
          },
          {
            "task": "boolq",
            "shots": 10,
            "type": "multiple_choice",
            "accuracy": 0.512,
            "centered": -0.2842
          },
          {
            "task": "bigbench_language_identification",
            "shots": 10,
            "type": "multiple_choice",
            "accuracy": 0.268,
            "centered": 0.1947
          }
        ]
      },
      "3000": {
        "core": 0.1853,
        "val_bpb": 0.670461,
        "cdpk": {
          "acc": 0.2914,
          "n": 262,
          "d": 899
        },
        "tasks": [
          {
            "task": "hellaswag_zeroshot",
            "shots": 0,
            "type": "multiple_choice",
            "accuracy": 0.464,
            "centered": 0.2853
          },
          {
            "task": "jeopardy",
            "shots": 10,
            "type": "language_modeling",
            "accuracy": 0.004,
            "centered": 0.004
          },
          {
            "task": "bigbench_qa_wikidata",
            "shots": 10,
            "type": "language_modeling",
            "accuracy": 0.248,
            "centered": 0.248
          },
          {
            "task": "arc_easy",
            "shots": 10,
            "type": "multiple_choice",
            "accuracy": 0.628,
            "centered": 0.504
          },
          {
            "task": "arc_challenge",
            "shots": 10,
            "type": "multiple_choice",
            "accuracy": 0.35,
            "centered": 0.1333
          },
          {
            "task": "copa",
            "shots": 0,
            "type": "multiple_choice",
            "accuracy": 0.63,
            "centered": 0.26
          },
          {
            "task": "commonsense_qa",
            "shots": 10,
            "type": "multiple_choice",
            "accuracy": 0.19,
            "centered": -0.0125
          },
          {
            "task": "piqa",
            "shots": 10,
            "type": "multiple_choice",
            "accuracy": 0.706,
            "centered": 0.412
          },
          {
            "task": "openbook_qa",
            "shots": 0,
            "type": "multiple_choice",
            "accuracy": 0.362,
            "centered": 0.1493
          },
          {
            "task": "lambada_openai",
            "shots": 0,
            "type": "language_modeling",
            "accuracy": 0.322,
            "centered": 0.322
          },
          {
            "task": "hellaswag",
            "shots": 10,
            "type": "multiple_choice",
            "accuracy": 0.438,
            "centered": 0.2507
          },
          {
            "task": "winograd",
            "shots": 0,
            "type": "schema",
            "accuracy": 0.663,
            "centered": 0.326
          },
          {
            "task": "winogrande",
            "shots": 0,
            "type": "schema",
            "accuracy": 0.516,
            "centered": 0.032
          },
          {
            "task": "bigbench_dyck_languages",
            "shots": 10,
            "type": "language_modeling",
            "accuracy": 0.098,
            "centered": 0.098
          },
          {
            "task": "agi_eval_lsat_ar",
            "shots": 3,
            "type": "multiple_choice",
            "accuracy": 0.2826,
            "centered": 0.1033
          },
          {
            "task": "bigbench_cs_algorithms",
            "shots": 10,
            "type": "language_modeling",
            "accuracy": 0.406,
            "centered": 0.406
          },
          {
            "task": "bigbench_operators",
            "shots": 10,
            "type": "language_modeling",
            "accuracy": 0.1429,
            "centered": 0.1429
          },
          {
            "task": "bigbench_repeat_copy_logic",
            "shots": 10,
            "type": "language_modeling",
            "accuracy": 0.0312,
            "centered": 0.0312
          },
          {
            "task": "squad",
            "shots": 10,
            "type": "language_modeling",
            "accuracy": 0.082,
            "centered": 0.082
          },
          {
            "task": "coqa",
            "shots": 0,
            "type": "language_modeling",
            "accuracy": 0.162,
            "centered": 0.162
          },
          {
            "task": "boolq",
            "shots": 10,
            "type": "multiple_choice",
            "accuracy": 0.61,
            "centered": -0.0263
          },
          {
            "task": "bigbench_language_identification",
            "shots": 10,
            "type": "multiple_choice",
            "accuracy": 0.24,
            "centered": 0.1639
          }
        ]
      },
      "4000": {
        "core": 0.186,
        "val_bpb": 0.676572,
        "cdpk": {
          "acc": 0.2803,
          "n": 252,
          "d": 899
        },
        "tasks": [
          {
            "task": "hellaswag_zeroshot",
            "shots": 0,
            "type": "multiple_choice",
            "accuracy": 0.49,
            "centered": 0.32
          },
          {
            "task": "jeopardy",
            "shots": 10,
            "type": "language_modeling",
            "accuracy": 0.008,
            "centered": 0.008
          },
          {
            "task": "bigbench_qa_wikidata",
            "shots": 10,
            "type": "language_modeling",
            "accuracy": 0.224,
            "centered": 0.224
          },
          {
            "task": "arc_easy",
            "shots": 10,
            "type": "multiple_choice",
            "accuracy": 0.654,
            "centered": 0.5387
          },
          {
            "task": "arc_challenge",
            "shots": 10,
            "type": "multiple_choice",
            "accuracy": 0.328,
            "centered": 0.104
          },
          {
            "task": "copa",
            "shots": 0,
            "type": "multiple_choice",
            "accuracy": 0.62,
            "centered": 0.24
          },
          {
            "task": "commonsense_qa",
            "shots": 10,
            "type": "multiple_choice",
            "accuracy": 0.246,
            "centered": 0.0575
          },
          {
            "task": "piqa",
            "shots": 10,
            "type": "multiple_choice",
            "accuracy": 0.704,
            "centered": 0.408
          },
          {
            "task": "openbook_qa",
            "shots": 0,
            "type": "multiple_choice",
            "accuracy": 0.364,
            "centered": 0.152
          },
          {
            "task": "lambada_openai",
            "shots": 0,
            "type": "language_modeling",
            "accuracy": 0.308,
            "centered": 0.308
          },
          {
            "task": "hellaswag",
            "shots": 10,
            "type": "multiple_choice",
            "accuracy": 0.468,
            "centered": 0.2907
          },
          {
            "task": "winograd",
            "shots": 0,
            "type": "schema",
            "accuracy": 0.6154,
            "centered": 0.2308
          },
          {
            "task": "winogrande",
            "shots": 0,
            "type": "schema",
            "accuracy": 0.55,
            "centered": 0.1
          },
          {
            "task": "bigbench_dyck_languages",
            "shots": 10,
            "type": "language_modeling",
            "accuracy": 0.088,
            "centered": 0.088
          },
          {
            "task": "agi_eval_lsat_ar",
            "shots": 3,
            "type": "multiple_choice",
            "accuracy": 0.2087,
            "centered": 0.0109
          },
          {
            "task": "bigbench_cs_algorithms",
            "shots": 10,
            "type": "language_modeling",
            "accuracy": 0.418,
            "centered": 0.418
          },
          {
            "task": "bigbench_operators",
            "shots": 10,
            "type": "language_modeling",
            "accuracy": 0.1476,
            "centered": 0.1476
          },
          {
            "task": "bigbench_repeat_copy_logic",
            "shots": 10,
            "type": "language_modeling",
            "accuracy": 0.0312,
            "centered": 0.0312
          },
          {
            "task": "squad",
            "shots": 10,
            "type": "language_modeling",
            "accuracy": 0.086,
            "centered": 0.086
          },
          {
            "task": "coqa",
            "shots": 0,
            "type": "language_modeling",
            "accuracy": 0.148,
            "centered": 0.148
          },
          {
            "task": "boolq",
            "shots": 10,
            "type": "multiple_choice",
            "accuracy": 0.63,
            "centered": 0.0263
          },
          {
            "task": "bigbench_language_identification",
            "shots": 10,
            "type": "multiple_choice",
            "accuracy": 0.232,
            "centered": 0.1551
          }
        ]
      },
      "5000": {
        "core": 0.1835,
        "val_bpb": 0.68886,
        "cdpk": {
          "acc": 0.2692,
          "n": 242,
          "d": 899
        },
        "tasks": [
          {
            "task": "hellaswag_zeroshot",
            "shots": 0,
            "type": "multiple_choice",
            "accuracy": 0.454,
            "centered": 0.272
          },
          {
            "task": "jeopardy",
            "shots": 10,
            "type": "language_modeling",
            "accuracy": 0.01,
            "centered": 0.01
          },
          {
            "task": "bigbench_qa_wikidata",
            "shots": 10,
            "type": "language_modeling",
            "accuracy": 0.206,
            "centered": 0.206
          },
          {
            "task": "arc_easy",
            "shots": 10,
            "type": "multiple_choice",
            "accuracy": 0.63,
            "centered": 0.5067
          },
          {
            "task": "arc_challenge",
            "shots": 10,
            "type": "multiple_choice",
            "accuracy": 0.35,
            "centered": 0.1333
          },
          {
            "task": "copa",
            "shots": 0,
            "type": "multiple_choice",
            "accuracy": 0.64,
            "centered": 0.28
          },
          {
            "task": "commonsense_qa",
            "shots": 10,
            "type": "multiple_choice",
            "accuracy": 0.226,
            "centered": 0.0325
          },
          {
            "task": "piqa",
            "shots": 10,
            "type": "multiple_choice",
            "accuracy": 0.722,
            "centered": 0.444
          },
          {
            "task": "openbook_qa",
            "shots": 0,
            "type": "multiple_choice",
            "accuracy": 0.37,
            "centered": 0.16
          },
          {
            "task": "lambada_openai",
            "shots": 0,
            "type": "language_modeling",
            "accuracy": 0.28,
            "centered": 0.28
          },
          {
            "task": "hellaswag",
            "shots": 10,
            "type": "multiple_choice",
            "accuracy": 0.442,
            "centered": 0.256
          },
          {
            "task": "winograd",
            "shots": 0,
            "type": "schema",
            "accuracy": 0.5934,
            "centered": 0.1868
          },
          {
            "task": "winogrande",
            "shots": 0,
            "type": "schema",
            "accuracy": 0.522,
            "centered": 0.044
          },
          {
            "task": "bigbench_dyck_languages",
            "shots": 10,
            "type": "language_modeling",
            "accuracy": 0.102,
            "centered": 0.102
          },
          {
            "task": "agi_eval_lsat_ar",
            "shots": 3,
            "type": "multiple_choice",
            "accuracy": 0.2696,
            "centered": 0.087
          },
          {
            "task": "bigbench_cs_algorithms",
            "shots": 10,
            "type": "language_modeling",
            "accuracy": 0.436,
            "centered": 0.436
          },
          {
            "task": "bigbench_operators",
            "shots": 10,
            "type": "language_modeling",
            "accuracy": 0.1286,
            "centered": 0.1286
          },
          {
            "task": "bigbench_repeat_copy_logic",
            "shots": 10,
            "type": "language_modeling",
            "accuracy": 0.0312,
            "centered": 0.0312
          },
          {
            "task": "squad",
            "shots": 10,
            "type": "language_modeling",
            "accuracy": 0.106,
            "centered": 0.106
          },
          {
            "task": "coqa",
            "shots": 0,
            "type": "language_modeling",
            "accuracy": 0.15,
            "centered": 0.15
          },
          {
            "task": "boolq",
            "shots": 10,
            "type": "multiple_choice",
            "accuracy": 0.628,
            "centered": 0.0211
          },
          {
            "task": "bigbench_language_identification",
            "shots": 10,
            "type": "multiple_choice",
            "accuracy": 0.24,
            "centered": 0.1639
          }
        ]
      },
      "6000": {
        "core": 0.1653,
        "val_bpb": 0.689664,
        "cdpk": {
          "acc": 0.2736,
          "n": 246,
          "d": 899
        },
        "tasks": [
          {
            "task": "hellaswag_zeroshot",
            "shots": 0,
            "type": "multiple_choice",
            "accuracy": 0.458,
            "centered": 0.2773
          },
          {
            "task": "jeopardy",
            "shots": 10,
            "type": "language_modeling",
            "accuracy": 0.0,
            "centered": 0.0
          },
          {
            "task": "bigbench_qa_wikidata",
            "shots": 10,
            "type": "language_modeling",
            "accuracy": 0.196,
            "centered": 0.196
          },
          {
            "task": "arc_easy",
            "shots": 10,
            "type": "multiple_choice",
            "accuracy": 0.628,
            "centered": 0.504
          },
          {
            "task": "arc_challenge",
            "shots": 10,
            "type": "multiple_choice",
            "accuracy": 0.334,
            "centered": 0.112
          },
          {
            "task": "copa",
            "shots": 0,
            "type": "multiple_choice",
            "accuracy": 0.62,
            "centered": 0.24
          },
          {
            "task": "commonsense_qa",
            "shots": 10,
            "type": "multiple_choice",
            "accuracy": 0.226,
            "centered": 0.0325
          },
          {
            "task": "piqa",
            "shots": 10,
            "type": "multiple_choice",
            "accuracy": 0.692,
            "centered": 0.384
          },
          {
            "task": "openbook_qa",
            "shots": 0,
            "type": "multiple_choice",
            "accuracy": 0.366,
            "centered": 0.1547
          },
          {
            "task": "lambada_openai",
            "shots": 0,
            "type": "language_modeling",
            "accuracy": 0.254,
            "centered": 0.254
          },
          {
            "task": "hellaswag",
            "shots": 10,
            "type": "multiple_choice",
            "accuracy": 0.442,
            "centered": 0.256
          },
          {
            "task": "winograd",
            "shots": 0,
            "type": "schema",
            "accuracy": 0.6081,
            "centered": 0.2161
          },
          {
            "task": "winogrande",
            "shots": 0,
            "type": "schema",
            "accuracy": 0.502,
            "centered": 0.004
          },
          {
            "task": "bigbench_dyck_languages",
            "shots": 10,
            "type": "language_modeling",
            "accuracy": 0.096,
            "centered": 0.096
          },
          {
            "task": "agi_eval_lsat_ar",
            "shots": 3,
            "type": "multiple_choice",
            "accuracy": 0.2826,
            "centered": 0.1033
          },
          {
            "task": "bigbench_cs_algorithms",
            "shots": 10,
            "type": "language_modeling",
            "accuracy": 0.422,
            "centered": 0.422
          },
          {
            "task": "bigbench_operators",
            "shots": 10,
            "type": "language_modeling",
            "accuracy": 0.1619,
            "centered": 0.1619
          },
          {
            "task": "bigbench_repeat_copy_logic",
            "shots": 10,
            "type": "language_modeling",
            "accuracy": 0.0312,
            "centered": 0.0312
          },
          {
            "task": "squad",
            "shots": 10,
            "type": "language_modeling",
            "accuracy": 0.106,
            "centered": 0.106
          },
          {
            "task": "coqa",
            "shots": 0,
            "type": "language_modeling",
            "accuracy": 0.132,
            "centered": 0.132
          },
          {
            "task": "boolq",
            "shots": 10,
            "type": "multiple_choice",
            "accuracy": 0.542,
            "centered": -0.2053
          },
          {
            "task": "bigbench_language_identification",
            "shots": 10,
            "type": "multiple_choice",
            "accuracy": 0.236,
            "centered": 0.1595
          }
        ]
      },
      "7000": {
        "core": 0.1839,
        "val_bpb": 0.664711,
        "cdpk": {
          "acc": 0.2881,
          "n": 259,
          "d": 899
        },
        "tasks": [
          {
            "task": "hellaswag_zeroshot",
            "shots": 0,
            "type": "multiple_choice",
            "accuracy": 0.458,
            "centered": 0.2773
          },
          {
            "task": "jeopardy",
            "shots": 10,
            "type": "language_modeling",
            "accuracy": 0.008,
            "centered": 0.008
          },
          {
            "task": "bigbench_qa_wikidata",
            "shots": 10,
            "type": "language_modeling",
            "accuracy": 0.24,
            "centered": 0.24
          },
          {
            "task": "arc_easy",
            "shots": 10,
            "type": "multiple_choice",
            "accuracy": 0.658,
            "centered": 0.544
          },
          {
            "task": "arc_challenge",
            "shots": 10,
            "type": "multiple_choice",
            "accuracy": 0.352,
            "centered": 0.136
          },
          {
            "task": "copa",
            "shots": 0,
            "type": "multiple_choice",
            "accuracy": 0.68,
            "centered": 0.36
          },
          {
            "task": "commonsense_qa",
            "shots": 10,
            "type": "multiple_choice",
            "accuracy": 0.332,
            "centered": 0.165
          },
          {
            "task": "piqa",
            "shots": 10,
            "type": "multiple_choice",
            "accuracy": 0.688,
            "centered": 0.376
          },
          {
            "task": "openbook_qa",
            "shots": 0,
            "type": "multiple_choice",
            "accuracy": 0.354,
            "centered": 0.1387
          },
          {
            "task": "lambada_openai",
            "shots": 0,
            "type": "language_modeling",
            "accuracy": 0.306,
            "centered": 0.306
          },
          {
            "task": "hellaswag",
            "shots": 10,
            "type": "multiple_choice",
            "accuracy": 0.448,
            "centered": 0.264
          },
          {
            "task": "winograd",
            "shots": 0,
            "type": "schema",
            "accuracy": 0.6117,
            "centered": 0.2234
          },
          {
            "task": "winogrande",
            "shots": 0,
            "type": "schema",
            "accuracy": 0.544,
            "centered": 0.088
          },
          {
            "task": "bigbench_dyck_languages",
            "shots": 10,
            "type": "language_modeling",
            "accuracy": 0.092,
            "centered": 0.092
          },
          {
            "task": "agi_eval_lsat_ar",
            "shots": 3,
            "type": "multiple_choice",
            "accuracy": 0.2478,
            "centered": 0.0598
          },
          {
            "task": "bigbench_cs_algorithms",
            "shots": 10,
            "type": "language_modeling",
            "accuracy": 0.41,
            "centered": 0.41
          },
          {
            "task": "bigbench_operators",
            "shots": 10,
            "type": "language_modeling",
            "accuracy": 0.181,
            "centered": 0.181
          },
          {
            "task": "bigbench_repeat_copy_logic",
            "shots": 10,
            "type": "language_modeling",
            "accuracy": 0.0312,
            "centered": 0.0312
          },
          {
            "task": "squad",
            "shots": 10,
            "type": "language_modeling",
            "accuracy": 0.082,
            "centered": 0.082
          },
          {
            "task": "coqa",
            "shots": 0,
            "type": "language_modeling",
            "accuracy": 0.146,
            "centered": 0.146
          },
          {
            "task": "boolq",
            "shots": 10,
            "type": "multiple_choice",
            "accuracy": 0.532,
            "centered": -0.2316
          },
          {
            "task": "bigbench_language_identification",
            "shots": 10,
            "type": "multiple_choice",
            "accuracy": 0.226,
            "centered": 0.1485
          }
        ]
      },
      "8000": {
        "core": 0.2126,
        "val_bpb": 0.651849,
        "cdpk": {
          "acc": 0.2747,
          "n": 247,
          "d": 899
        },
        "tasks": [
          {
            "task": "hellaswag_zeroshot",
            "shots": 0,
            "type": "multiple_choice",
            "accuracy": 0.476,
            "centered": 0.3013
          },
          {
            "task": "jeopardy",
            "shots": 10,
            "type": "language_modeling",
            "accuracy": 0.028,
            "centered": 0.028
          },
          {
            "task": "bigbench_qa_wikidata",
            "shots": 10,
            "type": "language_modeling",
            "accuracy": 0.294,
            "centered": 0.294
          },
          {
            "task": "arc_easy",
            "shots": 10,
            "type": "multiple_choice",
            "accuracy": 0.686,
            "centered": 0.5813
          },
          {
            "task": "arc_challenge",
            "shots": 10,
            "type": "multiple_choice",
            "accuracy": 0.384,
            "centered": 0.1787
          },
          {
            "task": "copa",
            "shots": 0,
            "type": "multiple_choice",
            "accuracy": 0.66,
            "centered": 0.32
          },
          {
            "task": "commonsense_qa",
            "shots": 10,
            "type": "multiple_choice",
            "accuracy": 0.294,
            "centered": 0.1175
          },
          {
            "task": "piqa",
            "shots": 10,
            "type": "multiple_choice",
            "accuracy": 0.708,
            "centered": 0.416
          },
          {
            "task": "openbook_qa",
            "shots": 0,
            "type": "multiple_choice",
            "accuracy": 0.374,
            "centered": 0.1653
          },
          {
            "task": "lambada_openai",
            "shots": 0,
            "type": "language_modeling",
            "accuracy": 0.3,
            "centered": 0.3
          },
          {
            "task": "hellaswag",
            "shots": 10,
            "type": "multiple_choice",
            "accuracy": 0.464,
            "centered": 0.2853
          },
          {
            "task": "winograd",
            "shots": 0,
            "type": "schema",
            "accuracy": 0.6703,
            "centered": 0.3407
          },
          {
            "task": "winogrande",
            "shots": 0,
            "type": "schema",
            "accuracy": 0.542,
            "centered": 0.084
          },
          {
            "task": "bigbench_dyck_languages",
            "shots": 10,
            "type": "language_modeling",
            "accuracy": 0.12,
            "centered": 0.12
          },
          {
            "task": "agi_eval_lsat_ar",
            "shots": 3,
            "type": "multiple_choice",
            "accuracy": 0.2522,
            "centered": 0.0652
          },
          {
            "task": "bigbench_cs_algorithms",
            "shots": 10,
            "type": "language_modeling",
            "accuracy": 0.438,
            "centered": 0.438
          },
          {
            "task": "bigbench_operators",
            "shots": 10,
            "type": "language_modeling",
            "accuracy": 0.1714,
            "centered": 0.1714
          },
          {
            "task": "bigbench_repeat_copy_logic",
            "shots": 10,
            "type": "language_modeling",
            "accuracy": 0.0,
            "centered": 0.0
          },
          {
            "task": "squad",
            "shots": 10,
            "type": "language_modeling",
            "accuracy": 0.142,
            "centered": 0.142
          },
          {
            "task": "coqa",
            "shots": 0,
            "type": "language_modeling",
            "accuracy": 0.154,
            "centered": 0.154
          },
          {
            "task": "boolq",
            "shots": 10,
            "type": "multiple_choice",
            "accuracy": 0.62,
            "centered": 0.0
          },
          {
            "task": "bigbench_language_identification",
            "shots": 10,
            "type": "multiple_choice",
            "accuracy": 0.25,
            "centered": 0.1749
          }
        ]
      },
      "9000": {
        "core": 0.2178,
        "val_bpb": 0.642506,
        "cdpk": {
          "acc": 0.277,
          "n": 249,
          "d": 899
        },
        "tasks": [
          {
            "task": "hellaswag_zeroshot",
            "shots": 0,
            "type": "multiple_choice",
            "accuracy": 0.492,
            "centered": 0.3227
          },
          {
            "task": "jeopardy",
            "shots": 10,
            "type": "language_modeling",
            "accuracy": 0.034,
            "centered": 0.034
          },
          {
            "task": "bigbench_qa_wikidata",
            "shots": 10,
            "type": "language_modeling",
            "accuracy": 0.306,
            "centered": 0.306
          },
          {
            "task": "arc_easy",
            "shots": 10,
            "type": "multiple_choice",
            "accuracy": 0.676,
            "centered": 0.568
          },
          {
            "task": "arc_challenge",
            "shots": 10,
            "type": "multiple_choice",
            "accuracy": 0.4,
            "centered": 0.2
          },
          {
            "task": "copa",
            "shots": 0,
            "type": "multiple_choice",
            "accuracy": 0.68,
            "centered": 0.36
          },
          {
            "task": "commonsense_qa",
            "shots": 10,
            "type": "multiple_choice",
            "accuracy": 0.412,
            "centered": 0.265
          },
          {
            "task": "piqa",
            "shots": 10,
            "type": "multiple_choice",
            "accuracy": 0.708,
            "centered": 0.416
          },
          {
            "task": "openbook_qa",
            "shots": 0,
            "type": "multiple_choice",
            "accuracy": 0.372,
            "centered": 0.1627
          },
          {
            "task": "lambada_openai",
            "shots": 0,
            "type": "language_modeling",
            "accuracy": 0.3,
            "centered": 0.3
          },
          {
            "task": "hellaswag",
            "shots": 10,
            "type": "multiple_choice",
            "accuracy": 0.466,
            "centered": 0.288
          },
          {
            "task": "winograd",
            "shots": 0,
            "type": "schema",
            "accuracy": 0.6264,
            "centered": 0.2527
          },
          {
            "task": "winogrande",
            "shots": 0,
            "type": "schema",
            "accuracy": 0.522,
            "centered": 0.044
          },
          {
            "task": "bigbench_dyck_languages",
            "shots": 10,
            "type": "language_modeling",
            "accuracy": 0.102,
            "centered": 0.102
          },
          {
            "task": "agi_eval_lsat_ar",
            "shots": 3,
            "type": "multiple_choice",
            "accuracy": 0.2478,
            "centered": 0.0598
          },
          {
            "task": "bigbench_cs_algorithms",
            "shots": 10,
            "type": "language_modeling",
            "accuracy": 0.44,
            "centered": 0.44
          },
          {
            "task": "bigbench_operators",
            "shots": 10,
            "type": "language_modeling",
            "accuracy": 0.1619,
            "centered": 0.1619
          },
          {
            "task": "bigbench_repeat_copy_logic",
            "shots": 10,
            "type": "language_modeling",
            "accuracy": 0.0,
            "centered": 0.0
          },
          {
            "task": "squad",
            "shots": 10,
            "type": "language_modeling",
            "accuracy": 0.138,
            "centered": 0.138
          },
          {
            "task": "coqa",
            "shots": 0,
            "type": "language_modeling",
            "accuracy": 0.182,
            "centered": 0.182
          },
          {
            "task": "boolq",
            "shots": 10,
            "type": "multiple_choice",
            "accuracy": 0.63,
            "centered": 0.0263
          },
          {
            "task": "bigbench_language_identification",
            "shots": 10,
            "type": "multiple_choice",
            "accuracy": 0.238,
            "centered": 0.1617
          }
        ]
      },
      "10000": {
        "core": 0.2221,
        "val_bpb": 0.633873,
        "cdpk": {
          "acc": 0.2792,
          "n": 251,
          "d": 899
        },
        "tasks": [
          {
            "task": "hellaswag_zeroshot",
            "shots": 0,
            "type": "multiple_choice",
            "accuracy": 0.514,
            "centered": 0.352
          },
          {
            "task": "jeopardy",
            "shots": 10,
            "type": "language_modeling",
            "accuracy": 0.04,
            "centered": 0.04
          },
          {
            "task": "bigbench_qa_wikidata",
            "shots": 10,
            "type": "language_modeling",
            "accuracy": 0.246,
            "centered": 0.246
          },
          {
            "task": "arc_easy",
            "shots": 10,
            "type": "multiple_choice",
            "accuracy": 0.694,
            "centered": 0.592
          },
          {
            "task": "arc_challenge",
            "shots": 10,
            "type": "multiple_choice",
            "accuracy": 0.374,
            "centered": 0.1653
          },
          {
            "task": "copa",
            "shots": 0,
            "type": "multiple_choice",
            "accuracy": 0.69,
            "centered": 0.38
          },
          {
            "task": "commonsense_qa",
            "shots": 10,
            "type": "multiple_choice",
            "accuracy": 0.29,
            "centered": 0.1125
          },
          {
            "task": "piqa",
            "shots": 10,
            "type": "multiple_choice",
            "accuracy": 0.72,
            "centered": 0.44
          },
          {
            "task": "openbook_qa",
            "shots": 0,
            "type": "multiple_choice",
            "accuracy": 0.374,
            "centered": 0.1653
          },
          {
            "task": "lambada_openai",
            "shots": 0,
            "type": "language_modeling",
            "accuracy": 0.342,
            "centered": 0.342
          },
          {
            "task": "hellaswag",
            "shots": 10,
            "type": "multiple_choice",
            "accuracy": 0.496,
            "centered": 0.328
          },
          {
            "task": "winograd",
            "shots": 0,
            "type": "schema",
            "accuracy": 0.663,
            "centered": 0.326
          },
          {
            "task": "winogrande",
            "shots": 0,
            "type": "schema",
            "accuracy": 0.538,
            "centered": 0.076
          },
          {
            "task": "bigbench_dyck_languages",
            "shots": 10,
            "type": "language_modeling",
            "accuracy": 0.12,
            "centered": 0.12
          },
          {
            "task": "agi_eval_lsat_ar",
            "shots": 3,
            "type": "multiple_choice",
            "accuracy": 0.2609,
            "centered": 0.0761
          },
          {
            "task": "bigbench_cs_algorithms",
            "shots": 10,
            "type": "language_modeling",
            "accuracy": 0.418,
            "centered": 0.418
          },
          {
            "task": "bigbench_operators",
            "shots": 10,
            "type": "language_modeling",
            "accuracy": 0.1857,
            "centered": 0.1857
          },
          {
            "task": "bigbench_repeat_copy_logic",
            "shots": 10,
            "type": "language_modeling",
            "accuracy": 0.0,
            "centered": 0.0
          },
          {
            "task": "squad",
            "shots": 10,
            "type": "language_modeling",
            "accuracy": 0.176,
            "centered": 0.176
          },
          {
            "task": "coqa",
            "shots": 0,
            "type": "language_modeling",
            "accuracy": 0.154,
            "centered": 0.154
          },
          {
            "task": "boolq",
            "shots": 10,
            "type": "multiple_choice",
            "accuracy": 0.63,
            "centered": 0.0263
          },
          {
            "task": "bigbench_language_identification",
            "shots": 10,
            "type": "multiple_choice",
            "accuracy": 0.24,
            "centered": 0.1639
          }
        ]
      },
      "11000": {
        "core": 0.215,
        "val_bpb": 0.62915,
        "cdpk": {
          "acc": 0.2948,
          "n": 265,
          "d": 899
        },
        "tasks": [
          {
            "task": "hellaswag_zeroshot",
            "shots": 0,
            "type": "multiple_choice",
            "accuracy": 0.498,
            "centered": 0.3307
          },
          {
            "task": "jeopardy",
            "shots": 10,
            "type": "language_modeling",
            "accuracy": 0.024,
            "centered": 0.024
          },
          {
            "task": "bigbench_qa_wikidata",
            "shots": 10,
            "type": "language_modeling",
            "accuracy": 0.328,
            "centered": 0.328
          },
          {
            "task": "arc_easy",
            "shots": 10,
            "type": "multiple_choice",
            "accuracy": 0.646,
            "centered": 0.528
          },
          {
            "task": "arc_challenge",
            "shots": 10,
            "type": "multiple_choice",
            "accuracy": 0.364,
            "centered": 0.152
          },
          {
            "task": "copa",
            "shots": 0,
            "type": "multiple_choice",
            "accuracy": 0.69,
            "centered": 0.38
          },
          {
            "task": "commonsense_qa",
            "shots": 10,
            "type": "multiple_choice",
            "accuracy": 0.27,
            "centered": 0.0875
          },
          {
            "task": "piqa",
            "shots": 10,
            "type": "multiple_choice",
            "accuracy": 0.712,
            "centered": 0.424
          },
          {
            "task": "openbook_qa",
            "shots": 0,
            "type": "multiple_choice",
            "accuracy": 0.376,
            "centered": 0.168
          },
          {
            "task": "lambada_openai",
            "shots": 0,
            "type": "language_modeling",
            "accuracy": 0.312,
            "centered": 0.312
          },
          {
            "task": "hellaswag",
            "shots": 10,
            "type": "multiple_choice",
            "accuracy": 0.476,
            "centered": 0.3013
          },
          {
            "task": "winograd",
            "shots": 0,
            "type": "schema",
            "accuracy": 0.6703,
            "centered": 0.3407
          },
          {
            "task": "winogrande",
            "shots": 0,
            "type": "schema",
            "accuracy": 0.536,
            "centered": 0.072
          },
          {
            "task": "bigbench_dyck_languages",
            "shots": 10,
            "type": "language_modeling",
            "accuracy": 0.12,
            "centered": 0.12
          },
          {
            "task": "agi_eval_lsat_ar",
            "shots": 3,
            "type": "multiple_choice",
            "accuracy": 0.2435,
            "centered": 0.0543
          },
          {
            "task": "bigbench_cs_algorithms",
            "shots": 10,
            "type": "language_modeling",
            "accuracy": 0.422,
            "centered": 0.422
          },
          {
            "task": "bigbench_operators",
            "shots": 10,
            "type": "language_modeling",
            "accuracy": 0.1714,
            "centered": 0.1714
          },
          {
            "task": "bigbench_repeat_copy_logic",
            "shots": 10,
            "type": "language_modeling",
            "accuracy": 0.0,
            "centered": 0.0
          },
          {
            "task": "squad",
            "shots": 10,
            "type": "language_modeling",
            "accuracy": 0.192,
            "centered": 0.192
          },
          {
            "task": "coqa",
            "shots": 0,
            "type": "language_modeling",
            "accuracy": 0.146,
            "centered": 0.146
          },
          {
            "task": "boolq",
            "shots": 10,
            "type": "multiple_choice",
            "accuracy": 0.626,
            "centered": 0.0158
          },
          {
            "task": "bigbench_language_identification",
            "shots": 10,
            "type": "multiple_choice",
            "accuracy": 0.236,
            "centered": 0.1595
          }
        ]
      },
      "12000": {
        "core": 0.218,
        "val_bpb": 0.626479,
        "cdpk": {
          "acc": 0.2859,
          "n": 257,
          "d": 899
        },
        "tasks": [
          {
            "task": "hellaswag_zeroshot",
            "shots": 0,
            "type": "multiple_choice",
            "accuracy": 0.512,
            "centered": 0.3493
          },
          {
            "task": "jeopardy",
            "shots": 10,
            "type": "language_modeling",
            "accuracy": 0.028,
            "centered": 0.028
          },
          {
            "task": "bigbench_qa_wikidata",
            "shots": 10,
            "type": "language_modeling",
            "accuracy": 0.286,
            "centered": 0.286
          },
          {
            "task": "arc_easy",
            "shots": 10,
            "type": "multiple_choice",
            "accuracy": 0.676,
            "centered": 0.568
          },
          {
            "task": "arc_challenge",
            "shots": 10,
            "type": "multiple_choice",
            "accuracy": 0.396,
            "centered": 0.1947
          },
          {
            "task": "copa",
            "shots": 0,
            "type": "multiple_choice",
            "accuracy": 0.63,
            "centered": 0.26
          },
          {
            "task": "commonsense_qa",
            "shots": 10,
            "type": "multiple_choice",
            "accuracy": 0.222,
            "centered": 0.0275
          },
          {
            "task": "piqa",
            "shots": 10,
            "type": "multiple_choice",
            "accuracy": 0.722,
            "centered": 0.444
          },
          {
            "task": "openbook_qa",
            "shots": 0,
            "type": "multiple_choice",
            "accuracy": 0.384,
            "centered": 0.1787
          },
          {
            "task": "lambada_openai",
            "shots": 0,
            "type": "language_modeling",
            "accuracy": 0.292,
            "centered": 0.292
          },
          {
            "task": "hellaswag",
            "shots": 10,
            "type": "multiple_choice",
            "accuracy": 0.502,
            "centered": 0.336
          },
          {
            "task": "winograd",
            "shots": 0,
            "type": "schema",
            "accuracy": 0.63,
            "centered": 0.2601
          },
          {
            "task": "winogrande",
            "shots": 0,
            "type": "schema",
            "accuracy": 0.552,
            "centered": 0.104
          },
          {
            "task": "bigbench_dyck_languages",
            "shots": 10,
            "type": "language_modeling",
            "accuracy": 0.132,
            "centered": 0.132
          },
          {
            "task": "agi_eval_lsat_ar",
            "shots": 3,
            "type": "multiple_choice",
            "accuracy": 0.2957,
            "centered": 0.1196
          },
          {
            "task": "bigbench_cs_algorithms",
            "shots": 10,
            "type": "language_modeling",
            "accuracy": 0.438,
            "centered": 0.438
          },
          {
            "task": "bigbench_operators",
            "shots": 10,
            "type": "language_modeling",
            "accuracy": 0.2095,
            "centered": 0.2095
          },
          {
            "task": "bigbench_repeat_copy_logic",
            "shots": 10,
            "type": "language_modeling",
            "accuracy": 0.0312,
            "centered": 0.0312
          },
          {
            "task": "squad",
            "shots": 10,
            "type": "language_modeling",
            "accuracy": 0.192,
            "centered": 0.192
          },
          {
            "task": "coqa",
            "shots": 0,
            "type": "language_modeling",
            "accuracy": 0.172,
            "centered": 0.172
          },
          {
            "task": "boolq",
            "shots": 10,
            "type": "multiple_choice",
            "accuracy": 0.62,
            "centered": 0.0
          },
          {
            "task": "bigbench_language_identification",
            "shots": 10,
            "type": "multiple_choice",
            "accuracy": 0.248,
            "centered": 0.1727
          }
        ]
      },
      "13000": {
        "core": 0.2251,
        "val_bpb": 0.62361,
        "cdpk": {
          "acc": 0.277,
          "n": 249,
          "d": 899
        },
        "tasks": [
          {
            "task": "hellaswag_zeroshot",
            "shots": 0,
            "type": "multiple_choice",
            "accuracy": 0.53,
            "centered": 0.3733
          },
          {
            "task": "jeopardy",
            "shots": 10,
            "type": "language_modeling",
            "accuracy": 0.016,
            "centered": 0.016
          },
          {
            "task": "bigbench_qa_wikidata",
            "shots": 10,
            "type": "language_modeling",
            "accuracy": 0.262,
            "centered": 0.262
          },
          {
            "task": "arc_easy",
            "shots": 10,
            "type": "multiple_choice",
            "accuracy": 0.69,
            "centered": 0.5867
          },
          {
            "task": "arc_challenge",
            "shots": 10,
            "type": "multiple_choice",
            "accuracy": 0.396,
            "centered": 0.1947
          },
          {
            "task": "copa",
            "shots": 0,
            "type": "multiple_choice",
            "accuracy": 0.66,
            "centered": 0.32
          },
          {
            "task": "commonsense_qa",
            "shots": 10,
            "type": "multiple_choice",
            "accuracy": 0.384,
            "centered": 0.23
          },
          {
            "task": "piqa",
            "shots": 10,
            "type": "multiple_choice",
            "accuracy": 0.742,
            "centered": 0.484
          },
          {
            "task": "openbook_qa",
            "shots": 0,
            "type": "multiple_choice",
            "accuracy": 0.37,
            "centered": 0.16
          },
          {
            "task": "lambada_openai",
            "shots": 0,
            "type": "language_modeling",
            "accuracy": 0.31,
            "centered": 0.31
          },
          {
            "task": "hellaswag",
            "shots": 10,
            "type": "multiple_choice",
            "accuracy": 0.508,
            "centered": 0.344
          },
          {
            "task": "winograd",
            "shots": 0,
            "type": "schema",
            "accuracy": 0.6484,
            "centered": 0.2967
          },
          {
            "task": "winogrande",
            "shots": 0,
            "type": "schema",
            "accuracy": 0.528,
            "centered": 0.056
          },
          {
            "task": "bigbench_dyck_languages",
            "shots": 10,
            "type": "language_modeling",
            "accuracy": 0.128,
            "centered": 0.128
          },
          {
            "task": "agi_eval_lsat_ar",
            "shots": 3,
            "type": "multiple_choice",
            "accuracy": 0.2435,
            "centered": 0.0543
          },
          {
            "task": "bigbench_cs_algorithms",
            "shots": 10,
            "type": "language_modeling",
            "accuracy": 0.434,
            "centered": 0.434
          },
          {
            "task": "bigbench_operators",
            "shots": 10,
            "type": "language_modeling",
            "accuracy": 0.1571,
            "centered": 0.1571
          },
          {
            "task": "bigbench_repeat_copy_logic",
            "shots": 10,
            "type": "language_modeling",
            "accuracy": 0.0,
            "centered": 0.0
          },
          {
            "task": "squad",
            "shots": 10,
            "type": "language_modeling",
            "accuracy": 0.194,
            "centered": 0.194
          },
          {
            "task": "coqa",
            "shots": 0,
            "type": "language_modeling",
            "accuracy": 0.18,
            "centered": 0.18
          },
          {
            "task": "boolq",
            "shots": 10,
            "type": "multiple_choice",
            "accuracy": 0.626,
            "centered": 0.0158
          },
          {
            "task": "bigbench_language_identification",
            "shots": 10,
            "type": "multiple_choice",
            "accuracy": 0.232,
            "centered": 0.1551
          }
        ]
      },
      "14000": {
        "core": 0.2279,
        "val_bpb": 0.620294,
        "cdpk": {
          "acc": 0.2759,
          "n": 248,
          "d": 899
        },
        "tasks": [
          {
            "task": "hellaswag_zeroshot",
            "shots": 0,
            "type": "multiple_choice",
            "accuracy": 0.536,
            "centered": 0.3813
          },
          {
            "task": "jeopardy",
            "shots": 10,
            "type": "language_modeling",
            "accuracy": 0.008,
            "centered": 0.008
          },
          {
            "task": "bigbench_qa_wikidata",
            "shots": 10,
            "type": "language_modeling",
            "accuracy": 0.328,
            "centered": 0.328
          },
          {
            "task": "arc_easy",
            "shots": 10,
            "type": "multiple_choice",
            "accuracy": 0.69,
            "centered": 0.5867
          },
          {
            "task": "arc_challenge",
            "shots": 10,
            "type": "multiple_choice",
            "accuracy": 0.392,
            "centered": 0.1893
          },
          {
            "task": "copa",
            "shots": 0,
            "type": "multiple_choice",
            "accuracy": 0.67,
            "centered": 0.34
          },
          {
            "task": "commonsense_qa",
            "shots": 10,
            "type": "multiple_choice",
            "accuracy": 0.242,
            "centered": 0.0525
          },
          {
            "task": "piqa",
            "shots": 10,
            "type": "multiple_choice",
            "accuracy": 0.71,
            "centered": 0.42
          },
          {
            "task": "openbook_qa",
            "shots": 0,
            "type": "multiple_choice",
            "accuracy": 0.396,
            "centered": 0.1947
          },
          {
            "task": "lambada_openai",
            "shots": 0,
            "type": "language_modeling",
            "accuracy": 0.352,
            "centered": 0.352
          },
          {
            "task": "hellaswag",
            "shots": 10,
            "type": "multiple_choice",
            "accuracy": 0.52,
            "centered": 0.36
          },
          {
            "task": "winograd",
            "shots": 0,
            "type": "schema",
            "accuracy": 0.6667,
            "centered": 0.3333
          },
          {
            "task": "winogrande",
            "shots": 0,
            "type": "schema",
            "accuracy": 0.568,
            "centered": 0.136
          },
          {
            "task": "bigbench_dyck_languages",
            "shots": 10,
            "type": "language_modeling",
            "accuracy": 0.112,
            "centered": 0.112
          },
          {
            "task": "agi_eval_lsat_ar",
            "shots": 3,
            "type": "multiple_choice",
            "accuracy": 0.2652,
            "centered": 0.0815
          },
          {
            "task": "bigbench_cs_algorithms",
            "shots": 10,
            "type": "language_modeling",
            "accuracy": 0.422,
            "centered": 0.422
          },
          {
            "task": "bigbench_operators",
            "shots": 10,
            "type": "language_modeling",
            "accuracy": 0.2,
            "centered": 0.2
          },
          {
            "task": "bigbench_repeat_copy_logic",
            "shots": 10,
            "type": "language_modeling",
            "accuracy": 0.0,
            "centered": 0.0
          },
          {
            "task": "squad",
            "shots": 10,
            "type": "language_modeling",
            "accuracy": 0.198,
            "centered": 0.198
          },
          {
            "task": "coqa",
            "shots": 0,
            "type": "language_modeling",
            "accuracy": 0.17,
            "centered": 0.17
          },
          {
            "task": "boolq",
            "shots": 10,
            "type": "multiple_choice",
            "accuracy": 0.614,
            "centered": -0.0158
          },
          {
            "task": "bigbench_language_identification",
            "shots": 10,
            "type": "multiple_choice",
            "accuracy": 0.24,
            "centered": 0.1639
          }
        ]
      },
      "15000": {
        "core": 0.2176,
        "val_bpb": 0.618883,
        "cdpk": {
          "acc": 0.2892,
          "n": 260,
          "d": 899
        },
        "tasks": [
          {
            "task": "hellaswag_zeroshot",
            "shots": 0,
            "type": "multiple_choice",
            "accuracy": 0.522,
            "centered": 0.3627
          },
          {
            "task": "jeopardy",
            "shots": 10,
            "type": "language_modeling",
            "accuracy": 0.034,
            "centered": 0.034
          },
          {
            "task": "bigbench_qa_wikidata",
            "shots": 10,
            "type": "language_modeling",
            "accuracy": 0.316,
            "centered": 0.316
          },
          {
            "task": "arc_easy",
            "shots": 10,
            "type": "multiple_choice",
            "accuracy": 0.704,
            "centered": 0.6053
          },
          {
            "task": "arc_challenge",
            "shots": 10,
            "type": "multiple_choice",
            "accuracy": 0.384,
            "centered": 0.1787
          },
          {
            "task": "copa",
            "shots": 0,
            "type": "multiple_choice",
            "accuracy": 0.68,
            "centered": 0.36
          },
          {
            "task": "commonsense_qa",
            "shots": 10,
            "type": "multiple_choice",
            "accuracy": 0.216,
            "centered": 0.02
          },
          {
            "task": "piqa",
            "shots": 10,
            "type": "multiple_choice",
            "accuracy": 0.718,
            "centered": 0.436
          },
          {
            "task": "openbook_qa",
            "shots": 0,
            "type": "multiple_choice",
            "accuracy": 0.364,
            "centered": 0.152
          },
          {
            "task": "lambada_openai",
            "shots": 0,
            "type": "language_modeling",
            "accuracy": 0.348,
            "centered": 0.348
          },
          {
            "task": "hellaswag",
            "shots": 10,
            "type": "multiple_choice",
            "accuracy": 0.504,
            "centered": 0.3387
          },
          {
            "task": "winograd",
            "shots": 0,
            "type": "schema",
            "accuracy": 0.663,
            "centered": 0.326
          },
          {
            "task": "winogrande",
            "shots": 0,
            "type": "schema",
            "accuracy": 0.538,
            "centered": 0.076
          },
          {
            "task": "bigbench_dyck_languages",
            "shots": 10,
            "type": "language_modeling",
            "accuracy": 0.116,
            "centered": 0.116
          },
          {
            "task": "agi_eval_lsat_ar",
            "shots": 3,
            "type": "multiple_choice",
            "accuracy": 0.2565,
            "centered": 0.0707
          },
          {
            "task": "bigbench_cs_algorithms",
            "shots": 10,
            "type": "language_modeling",
            "accuracy": 0.386,
            "centered": 0.386
          },
          {
            "task": "bigbench_operators",
            "shots": 10,
            "type": "language_modeling",
            "accuracy": 0.1429,
            "centered": 0.1429
          },
          {
            "task": "bigbench_repeat_copy_logic",
            "shots": 10,
            "type": "language_modeling",
            "accuracy": 0.0312,
            "centered": 0.0312
          },
          {
            "task": "squad",
            "shots": 10,
            "type": "language_modeling",
            "accuracy": 0.192,
            "centered": 0.192
          },
          {
            "task": "coqa",
            "shots": 0,
            "type": "language_modeling",
            "accuracy": 0.174,
            "centered": 0.174
          },
          {
            "task": "boolq",
            "shots": 10,
            "type": "multiple_choice",
            "accuracy": 0.616,
            "centered": -0.0105
          },
          {
            "task": "bigbench_language_identification",
            "shots": 10,
            "type": "multiple_choice",
            "accuracy": 0.21,
            "centered": 0.1309
          }
        ]
      },
      "16000": {
        "core": 0.2177,
        "val_bpb": 0.616538,
        "cdpk": {
          "acc": 0.2836,
          "n": 255,
          "d": 899
        },
        "tasks": [
          {
            "task": "hellaswag_zeroshot",
            "shots": 0,
            "type": "multiple_choice",
            "accuracy": 0.512,
            "centered": 0.3493
          },
          {
            "task": "jeopardy",
            "shots": 10,
            "type": "language_modeling",
            "accuracy": 0.038,
            "centered": 0.038
          },
          {
            "task": "bigbench_qa_wikidata",
            "shots": 10,
            "type": "language_modeling",
            "accuracy": 0.308,
            "centered": 0.308
          },
          {
            "task": "arc_easy",
            "shots": 10,
            "type": "multiple_choice",
            "accuracy": 0.7,
            "centered": 0.6
          },
          {
            "task": "arc_challenge",
            "shots": 10,
            "type": "multiple_choice",
            "accuracy": 0.376,
            "centered": 0.168
          },
          {
            "task": "copa",
            "shots": 0,
            "type": "multiple_choice",
            "accuracy": 0.66,
            "centered": 0.32
          },
          {
            "task": "commonsense_qa",
            "shots": 10,
            "type": "multiple_choice",
            "accuracy": 0.216,
            "centered": 0.02
          },
          {
            "task": "piqa",
            "shots": 10,
            "type": "multiple_choice",
            "accuracy": 0.732,
            "centered": 0.464
          },
          {
            "task": "openbook_qa",
            "shots": 0,
            "type": "multiple_choice",
            "accuracy": 0.384,
            "centered": 0.1787
          },
          {
            "task": "lambada_openai",
            "shots": 0,
            "type": "language_modeling",
            "accuracy": 0.332,
            "centered": 0.332
          },
          {
            "task": "hellaswag",
            "shots": 10,
            "type": "multiple_choice",
            "accuracy": 0.494,
            "centered": 0.3253
          },
          {
            "task": "winograd",
            "shots": 0,
            "type": "schema",
            "accuracy": 0.6484,
            "centered": 0.2967
          },
          {
            "task": "winogrande",
            "shots": 0,
            "type": "schema",
            "accuracy": 0.556,
            "centered": 0.112
          },
          {
            "task": "bigbench_dyck_languages",
            "shots": 10,
            "type": "language_modeling",
            "accuracy": 0.094,
            "centered": 0.094
          },
          {
            "task": "agi_eval_lsat_ar",
            "shots": 3,
            "type": "multiple_choice",
            "accuracy": 0.213,
            "centered": 0.0163
          },
          {
            "task": "bigbench_cs_algorithms",
            "shots": 10,
            "type": "language_modeling",
            "accuracy": 0.44,
            "centered": 0.44
          },
          {
            "task": "bigbench_operators",
            "shots": 10,
            "type": "language_modeling",
            "accuracy": 0.1762,
            "centered": 0.1762
          },
          {
            "task": "bigbench_repeat_copy_logic",
            "shots": 10,
            "type": "language_modeling",
            "accuracy": 0.0,
            "centered": 0.0
          },
          {
            "task": "squad",
            "shots": 10,
            "type": "language_modeling",
            "accuracy": 0.188,
            "centered": 0.188
          },
          {
            "task": "coqa",
            "shots": 0,
            "type": "language_modeling",
            "accuracy": 0.176,
            "centered": 0.176
          },
          {
            "task": "boolq",
            "shots": 10,
            "type": "multiple_choice",
            "accuracy": 0.622,
            "centered": 0.0053
          },
          {
            "task": "bigbench_language_identification",
            "shots": 10,
            "type": "multiple_choice",
            "accuracy": 0.256,
            "centered": 0.1815
          }
        ]
      },
      "17000": {
        "core": 0.237,
        "val_bpb": 0.613831,
        "cdpk": {
          "acc": 0.2925,
          "n": 263,
          "d": 899
        },
        "tasks": [
          {
            "task": "hellaswag_zeroshot",
            "shots": 0,
            "type": "multiple_choice",
            "accuracy": 0.542,
            "centered": 0.3893
          },
          {
            "task": "jeopardy",
            "shots": 10,
            "type": "language_modeling",
            "accuracy": 0.026,
            "centered": 0.026
          },
          {
            "task": "bigbench_qa_wikidata",
            "shots": 10,
            "type": "language_modeling",
            "accuracy": 0.372,
            "centered": 0.372
          },
          {
            "task": "arc_easy",
            "shots": 10,
            "type": "multiple_choice",
            "accuracy": 0.712,
            "centered": 0.616
          },
          {
            "task": "arc_challenge",
            "shots": 10,
            "type": "multiple_choice",
            "accuracy": 0.404,
            "centered": 0.2053
          },
          {
            "task": "copa",
            "shots": 0,
            "type": "multiple_choice",
            "accuracy": 0.74,
            "centered": 0.48
          },
          {
            "task": "commonsense_qa",
            "shots": 10,
            "type": "multiple_choice",
            "accuracy": 0.178,
            "centered": -0.0275
          },
          {
            "task": "piqa",
            "shots": 10,
            "type": "multiple_choice",
            "accuracy": 0.718,
            "centered": 0.436
          },
          {
            "task": "openbook_qa",
            "shots": 0,
            "type": "multiple_choice",
            "accuracy": 0.39,
            "centered": 0.1867
          },
          {
            "task": "lambada_openai",
            "shots": 0,
            "type": "language_modeling",
            "accuracy": 0.356,
            "centered": 0.356
          },
          {
            "task": "hellaswag",
            "shots": 10,
            "type": "multiple_choice",
            "accuracy": 0.522,
            "centered": 0.3627
          },
          {
            "task": "winograd",
            "shots": 0,
            "type": "schema",
            "accuracy": 0.641,
            "centered": 0.2821
          },
          {
            "task": "winogrande",
            "shots": 0,
            "type": "schema",
            "accuracy": 0.544,
            "centered": 0.088
          },
          {
            "task": "bigbench_dyck_languages",
            "shots": 10,
            "type": "language_modeling",
            "accuracy": 0.12,
            "centered": 0.12
          },
          {
            "task": "agi_eval_lsat_ar",
            "shots": 3,
            "type": "multiple_choice",
            "accuracy": 0.2739,
            "centered": 0.0924
          },
          {
            "task": "bigbench_cs_algorithms",
            "shots": 10,
            "type": "language_modeling",
            "accuracy": 0.436,
            "centered": 0.436
          },
          {
            "task": "bigbench_operators",
            "shots": 10,
            "type": "language_modeling",
            "accuracy": 0.1905,
            "centered": 0.1905
          },
          {
            "task": "bigbench_repeat_copy_logic",
            "shots": 10,
            "type": "language_modeling",
            "accuracy": 0.0,
            "centered": 0.0
          },
          {
            "task": "squad",
            "shots": 10,
            "type": "language_modeling",
            "accuracy": 0.218,
            "centered": 0.218
          },
          {
            "task": "coqa",
            "shots": 0,
            "type": "language_modeling",
            "accuracy": 0.192,
            "centered": 0.192
          },
          {
            "task": "boolq",
            "shots": 10,
            "type": "multiple_choice",
            "accuracy": 0.62,
            "centered": 0.0
          },
          {
            "task": "bigbench_language_identification",
            "shots": 10,
            "type": "multiple_choice",
            "accuracy": 0.266,
            "centered": 0.1925
          }
        ]
      },
      "18000": {
        "core": 0.2376,
        "val_bpb": 0.612875,
        "cdpk": {
          "acc": 0.2859,
          "n": 257,
          "d": 899
        },
        "tasks": [
          {
            "task": "hellaswag_zeroshot",
            "shots": 0,
            "type": "multiple_choice",
            "accuracy": 0.516,
            "centered": 0.3547
          },
          {
            "task": "jeopardy",
            "shots": 10,
            "type": "language_modeling",
            "accuracy": 0.03,
            "centered": 0.03
          },
          {
            "task": "bigbench_qa_wikidata",
            "shots": 10,
            "type": "language_modeling",
            "accuracy": 0.36,
            "centered": 0.36
          },
          {
            "task": "arc_easy",
            "shots": 10,
            "type": "multiple_choice",
            "accuracy": 0.728,
            "centered": 0.6373
          },
          {
            "task": "arc_challenge",
            "shots": 10,
            "type": "multiple_choice",
            "accuracy": 0.394,
            "centered": 0.192
          },
          {
            "task": "copa",
            "shots": 0,
            "type": "multiple_choice",
            "accuracy": 0.66,
            "centered": 0.32
          },
          {
            "task": "commonsense_qa",
            "shots": 10,
            "type": "multiple_choice",
            "accuracy": 0.348,
            "centered": 0.185
          },
          {
            "task": "piqa",
            "shots": 10,
            "type": "multiple_choice",
            "accuracy": 0.7,
            "centered": 0.4
          },
          {
            "task": "openbook_qa",
            "shots": 0,
            "type": "multiple_choice",
            "accuracy": 0.408,
            "centered": 0.2107
          },
          {
            "task": "lambada_openai",
            "shots": 0,
            "type": "language_modeling",
            "accuracy": 0.294,
            "centered": 0.294
          },
          {
            "task": "hellaswag",
            "shots": 10,
            "type": "multiple_choice",
            "accuracy": 0.486,
            "centered": 0.3147
          },
          {
            "task": "winograd",
            "shots": 0,
            "type": "schema",
            "accuracy": 0.6813,
            "centered": 0.3626
          },
          {
            "task": "winogrande",
            "shots": 0,
            "type": "schema",
            "accuracy": 0.542,
            "centered": 0.084
          },
          {
            "task": "bigbench_dyck_languages",
            "shots": 10,
            "type": "language_modeling",
            "accuracy": 0.11,
            "centered": 0.11
          },
          {
            "task": "agi_eval_lsat_ar",
            "shots": 3,
            "type": "multiple_choice",
            "accuracy": 0.2696,
            "centered": 0.087
          },
          {
            "task": "bigbench_cs_algorithms",
            "shots": 10,
            "type": "language_modeling",
            "accuracy": 0.41,
            "centered": 0.41
          },
          {
            "task": "bigbench_operators",
            "shots": 10,
            "type": "language_modeling",
            "accuracy": 0.1857,
            "centered": 0.1857
          },
          {
            "task": "bigbench_repeat_copy_logic",
            "shots": 10,
            "type": "language_modeling",
            "accuracy": 0.0312,
            "centered": 0.0312
          },
          {
            "task": "squad",
            "shots": 10,
            "type": "language_modeling",
            "accuracy": 0.218,
            "centered": 0.218
          },
          {
            "task": "coqa",
            "shots": 0,
            "type": "language_modeling",
            "accuracy": 0.18,
            "centered": 0.18
          },
          {
            "task": "boolq",
            "shots": 10,
            "type": "multiple_choice",
            "accuracy": 0.65,
            "centered": 0.0789
          },
          {
            "task": "bigbench_language_identification",
            "shots": 10,
            "type": "multiple_choice",
            "accuracy": 0.256,
            "centered": 0.1815
          }
        ]
      },
      "19000": {
        "core": 0.2235,
        "val_bpb": 0.610755,
        "cdpk": {
          "acc": 0.2925,
          "n": 263,
          "d": 899
        },
        "tasks": [
          {
            "task": "hellaswag_zeroshot",
            "shots": 0,
            "type": "multiple_choice",
            "accuracy": 0.54,
            "centered": 0.3867
          },
          {
            "task": "jeopardy",
            "shots": 10,
            "type": "language_modeling",
            "accuracy": 0.048,
            "centered": 0.048
          },
          {
            "task": "bigbench_qa_wikidata",
            "shots": 10,
            "type": "language_modeling",
            "accuracy": 0.33,
            "centered": 0.33
          },
          {
            "task": "arc_easy",
            "shots": 10,
            "type": "multiple_choice",
            "accuracy": 0.706,
            "centered": 0.608
          },
          {
            "task": "arc_challenge",
            "shots": 10,
            "type": "multiple_choice",
            "accuracy": 0.404,
            "centered": 0.2053
          },
          {
            "task": "copa",
            "shots": 0,
            "type": "multiple_choice",
            "accuracy": 0.64,
            "centered": 0.28
          },
          {
            "task": "commonsense_qa",
            "shots": 10,
            "type": "multiple_choice",
            "accuracy": 0.274,
            "centered": 0.0925
          },
          {
            "task": "piqa",
            "shots": 10,
            "type": "multiple_choice",
            "accuracy": 0.738,
            "centered": 0.476
          },
          {
            "task": "openbook_qa",
            "shots": 0,
            "type": "multiple_choice",
            "accuracy": 0.398,
            "centered": 0.1973
          },
          {
            "task": "lambada_openai",
            "shots": 0,
            "type": "language_modeling",
            "accuracy": 0.334,
            "centered": 0.334
          },
          {
            "task": "hellaswag",
            "shots": 10,
            "type": "multiple_choice",
            "accuracy": 0.522,
            "centered": 0.3627
          },
          {
            "task": "winograd",
            "shots": 0,
            "type": "schema",
            "accuracy": 0.641,
            "centered": 0.2821
          },
          {
            "task": "winogrande",
            "shots": 0,
            "type": "schema",
            "accuracy": 0.54,
            "centered": 0.08
          },
          {
            "task": "bigbench_dyck_languages",
            "shots": 10,
            "type": "language_modeling",
            "accuracy": 0.076,
            "centered": 0.076
          },
          {
            "task": "agi_eval_lsat_ar",
            "shots": 3,
            "type": "multiple_choice",
            "accuracy": 0.2348,
            "centered": 0.0435
          },
          {
            "task": "bigbench_cs_algorithms",
            "shots": 10,
            "type": "language_modeling",
            "accuracy": 0.438,
            "centered": 0.438
          },
          {
            "task": "bigbench_operators",
            "shots": 10,
            "type": "language_modeling",
            "accuracy": 0.1714,
            "centered": 0.1714
          },
          {
            "task": "bigbench_repeat_copy_logic",
            "shots": 10,
            "type": "language_modeling",
            "accuracy": 0.0,
            "centered": 0.0
          },
          {
            "task": "squad",
            "shots": 10,
            "type": "language_modeling",
            "accuracy": 0.18,
            "centered": 0.18
          },
          {
            "task": "coqa",
            "shots": 0,
            "type": "language_modeling",
            "accuracy": 0.168,
            "centered": 0.168
          },
          {
            "task": "boolq",
            "shots": 10,
            "type": "multiple_choice",
            "accuracy": 0.62,
            "centered": 0.0
          },
          {
            "task": "bigbench_language_identification",
            "shots": 10,
            "type": "multiple_choice",
            "accuracy": 0.234,
            "centered": 0.1573
          }
        ]
      },
      "20000": {
        "core": 0.2308,
        "val_bpb": 0.609499,
        "cdpk": {
          "acc": 0.287,
          "n": 258,
          "d": 899
        },
        "tasks": [
          {
            "task": "hellaswag_zeroshot",
            "shots": 0,
            "type": "multiple_choice",
            "accuracy": 0.53,
            "centered": 0.3733
          },
          {
            "task": "jeopardy",
            "shots": 10,
            "type": "language_modeling",
            "accuracy": 0.052,
            "centered": 0.052
          },
          {
            "task": "bigbench_qa_wikidata",
            "shots": 10,
            "type": "language_modeling",
            "accuracy": 0.364,
            "centered": 0.364
          },
          {
            "task": "arc_easy",
            "shots": 10,
            "type": "multiple_choice",
            "accuracy": 0.718,
            "centered": 0.624
          },
          {
            "task": "arc_challenge",
            "shots": 10,
            "type": "multiple_choice",
            "accuracy": 0.404,
            "centered": 0.2053
          },
          {
            "task": "copa",
            "shots": 0,
            "type": "multiple_choice",
            "accuracy": 0.7,
            "centered": 0.4
          },
          {
            "task": "commonsense_qa",
            "shots": 10,
            "type": "multiple_choice",
            "accuracy": 0.274,
            "centered": 0.0925
          },
          {
            "task": "piqa",
            "shots": 10,
            "type": "multiple_choice",
            "accuracy": 0.728,
            "centered": 0.456
          },
          {
            "task": "openbook_qa",
            "shots": 0,
            "type": "multiple_choice",
            "accuracy": 0.384,
            "centered": 0.1787
          },
          {
            "task": "lambada_openai",
            "shots": 0,
            "type": "language_modeling",
            "accuracy": 0.322,
            "centered": 0.322
          },
          {
            "task": "hellaswag",
            "shots": 10,
            "type": "multiple_choice",
            "accuracy": 0.514,
            "centered": 0.352
          },
          {
            "task": "winograd",
            "shots": 0,
            "type": "schema",
            "accuracy": 0.6484,
            "centered": 0.2967
          },
          {
            "task": "winogrande",
            "shots": 0,
            "type": "schema",
            "accuracy": 0.57,
            "centered": 0.14
          },
          {
            "task": "bigbench_dyck_languages",
            "shots": 10,
            "type": "language_modeling",
            "accuracy": 0.094,
            "centered": 0.094
          },
          {
            "task": "agi_eval_lsat_ar",
            "shots": 3,
            "type": "multiple_choice",
            "accuracy": 0.2478,
            "centered": 0.0598
          },
          {
            "task": "bigbench_cs_algorithms",
            "shots": 10,
            "type": "language_modeling",
            "accuracy": 0.426,
            "centered": 0.426
          },
          {
            "task": "bigbench_operators",
            "shots": 10,
            "type": "language_modeling",
            "accuracy": 0.1762,
            "centered": 0.1762
          },
          {
            "task": "bigbench_repeat_copy_logic",
            "shots": 10,
            "type": "language_modeling",
            "accuracy": 0.0,
            "centered": 0.0
          },
          {
            "task": "squad",
            "shots": 10,
            "type": "language_modeling",
            "accuracy": 0.2,
            "centered": 0.2
          },
          {
            "task": "coqa",
            "shots": 0,
            "type": "language_modeling",
            "accuracy": 0.206,
            "centered": 0.206
          },
          {
            "task": "boolq",
            "shots": 10,
            "type": "multiple_choice",
            "accuracy": 0.578,
            "centered": -0.1105
          },
          {
            "task": "bigbench_language_identification",
            "shots": 10,
            "type": "multiple_choice",
            "accuracy": 0.246,
            "centered": 0.1705
          }
        ]
      },
      "21000": {
        "core": 0.2277,
        "val_bpb": 0.607296,
        "cdpk": {
          "acc": 0.3026,
          "n": 272,
          "d": 899
        },
        "tasks": [
          {
            "task": "hellaswag_zeroshot",
            "shots": 0,
            "type": "multiple_choice",
            "accuracy": 0.516,
            "centered": 0.3547
          },
          {
            "task": "jeopardy",
            "shots": 10,
            "type": "language_modeling",
            "accuracy": 0.048,
            "centered": 0.048
          },
          {
            "task": "bigbench_qa_wikidata",
            "shots": 10,
            "type": "language_modeling",
            "accuracy": 0.272,
            "centered": 0.272
          },
          {
            "task": "arc_easy",
            "shots": 10,
            "type": "multiple_choice",
            "accuracy": 0.692,
            "centered": 0.5893
          },
          {
            "task": "arc_challenge",
            "shots": 10,
            "type": "multiple_choice",
            "accuracy": 0.412,
            "centered": 0.216
          },
          {
            "task": "copa",
            "shots": 0,
            "type": "multiple_choice",
            "accuracy": 0.74,
            "centered": 0.48
          },
          {
            "task": "commonsense_qa",
            "shots": 10,
            "type": "multiple_choice",
            "accuracy": 0.188,
            "centered": -0.015
          },
          {
            "task": "piqa",
            "shots": 10,
            "type": "multiple_choice",
            "accuracy": 0.734,
            "centered": 0.468
          },
          {
            "task": "openbook_qa",
            "shots": 0,
            "type": "multiple_choice",
            "accuracy": 0.37,
            "centered": 0.16
          },
          {
            "task": "lambada_openai",
            "shots": 0,
            "type": "language_modeling",
            "accuracy": 0.318,
            "centered": 0.318
          },
          {
            "task": "hellaswag",
            "shots": 10,
            "type": "multiple_choice",
            "accuracy": 0.512,
            "centered": 0.3493
          },
          {
            "task": "winograd",
            "shots": 0,
            "type": "schema",
            "accuracy": 0.6703,
            "centered": 0.3407
          },
          {
            "task": "winogrande",
            "shots": 0,
            "type": "schema",
            "accuracy": 0.546,
            "centered": 0.092
          },
          {
            "task": "bigbench_dyck_languages",
            "shots": 10,
            "type": "language_modeling",
            "accuracy": 0.122,
            "centered": 0.122
          },
          {
            "task": "agi_eval_lsat_ar",
            "shots": 3,
            "type": "multiple_choice",
            "accuracy": 0.2565,
            "centered": 0.0707
          },
          {
            "task": "bigbench_cs_algorithms",
            "shots": 10,
            "type": "language_modeling",
            "accuracy": 0.434,
            "centered": 0.434
          },
          {
            "task": "bigbench_operators",
            "shots": 10,
            "type": "language_modeling",
            "accuracy": 0.1524,
            "centered": 0.1524
          },
          {
            "task": "bigbench_repeat_copy_logic",
            "shots": 10,
            "type": "language_modeling",
            "accuracy": 0.0312,
            "centered": 0.0312
          },
          {
            "task": "squad",
            "shots": 10,
            "type": "language_modeling",
            "accuracy": 0.212,
            "centered": 0.212
          },
          {
            "task": "coqa",
            "shots": 0,
            "type": "language_modeling",
            "accuracy": 0.166,
            "centered": 0.166
          },
          {
            "task": "boolq",
            "shots": 10,
            "type": "multiple_choice",
            "accuracy": 0.61,
            "centered": -0.0263
          },
          {
            "task": "bigbench_language_identification",
            "shots": 10,
            "type": "multiple_choice",
            "accuracy": 0.25,
            "centered": 0.1749
          }
        ]
      },
      "22000": {
        "core": 0.2284,
        "val_bpb": 0.60594,
        "cdpk": {
          "acc": 0.2914,
          "n": 262,
          "d": 899
        },
        "tasks": [
          {
            "task": "hellaswag_zeroshot",
            "shots": 0,
            "type": "multiple_choice",
            "accuracy": 0.53,
            "centered": 0.3733
          },
          {
            "task": "jeopardy",
            "shots": 10,
            "type": "language_modeling",
            "accuracy": 0.046,
            "centered": 0.046
          },
          {
            "task": "bigbench_qa_wikidata",
            "shots": 10,
            "type": "language_modeling",
            "accuracy": 0.326,
            "centered": 0.326
          },
          {
            "task": "arc_easy",
            "shots": 10,
            "type": "multiple_choice",
            "accuracy": 0.714,
            "centered": 0.6187
          },
          {
            "task": "arc_challenge",
            "shots": 10,
            "type": "multiple_choice",
            "accuracy": 0.41,
            "centered": 0.2133
          },
          {
            "task": "copa",
            "shots": 0,
            "type": "multiple_choice",
            "accuracy": 0.7,
            "centered": 0.4
          },
          {
            "task": "commonsense_qa",
            "shots": 10,
            "type": "multiple_choice",
            "accuracy": 0.268,
            "centered": 0.085
          },
          {
            "task": "piqa",
            "shots": 10,
            "type": "multiple_choice",
            "accuracy": 0.728,
            "centered": 0.456
          },
          {
            "task": "openbook_qa",
            "shots": 0,
            "type": "multiple_choice",
            "accuracy": 0.382,
            "centered": 0.176
          },
          {
            "task": "lambada_openai",
            "shots": 0,
            "type": "language_modeling",
            "accuracy": 0.332,
            "centered": 0.332
          },
          {
            "task": "hellaswag",
            "shots": 10,
            "type": "multiple_choice",
            "accuracy": 0.514,
            "centered": 0.352
          },
          {
            "task": "winograd",
            "shots": 0,
            "type": "schema",
            "accuracy": 0.619,
            "centered": 0.2381
          },
          {
            "task": "winogrande",
            "shots": 0,
            "type": "schema",
            "accuracy": 0.572,
            "centered": 0.144
          },
          {
            "task": "bigbench_dyck_languages",
            "shots": 10,
            "type": "language_modeling",
            "accuracy": 0.124,
            "centered": 0.124
          },
          {
            "task": "agi_eval_lsat_ar",
            "shots": 3,
            "type": "multiple_choice",
            "accuracy": 0.2261,
            "centered": 0.0326
          },
          {
            "task": "bigbench_cs_algorithms",
            "shots": 10,
            "type": "language_modeling",
            "accuracy": 0.436,
            "centered": 0.436
          },
          {
            "task": "bigbench_operators",
            "shots": 10,
            "type": "language_modeling",
            "accuracy": 0.1667,
            "centered": 0.1667
          },
          {
            "task": "bigbench_repeat_copy_logic",
            "shots": 10,
            "type": "language_modeling",
            "accuracy": 0.0312,
            "centered": 0.0312
          },
          {
            "task": "squad",
            "shots": 10,
            "type": "language_modeling",
            "accuracy": 0.182,
            "centered": 0.182
          },
          {
            "task": "coqa",
            "shots": 0,
            "type": "language_modeling",
            "accuracy": 0.206,
            "centered": 0.206
          },
          {
            "task": "boolq",
            "shots": 10,
            "type": "multiple_choice",
            "accuracy": 0.594,
            "centered": -0.0684
          },
          {
            "task": "bigbench_language_identification",
            "shots": 10,
            "type": "multiple_choice",
            "accuracy": 0.232,
            "centered": 0.1551
          }
        ]
      },
      "23000": {
        "core": 0.22,
        "val_bpb": 0.604006,
        "cdpk": {
          "acc": 0.277,
          "n": 249,
          "d": 899
        },
        "tasks": [
          {
            "task": "hellaswag_zeroshot",
            "shots": 0,
            "type": "multiple_choice",
            "accuracy": 0.536,
            "centered": 0.3813
          },
          {
            "task": "jeopardy",
            "shots": 10,
            "type": "language_modeling",
            "accuracy": 0.034,
            "centered": 0.034
          },
          {
            "task": "bigbench_qa_wikidata",
            "shots": 10,
            "type": "language_modeling",
            "accuracy": 0.298,
            "centered": 0.298
          },
          {
            "task": "arc_easy",
            "shots": 10,
            "type": "multiple_choice",
            "accuracy": 0.704,
            "centered": 0.6053
          },
          {
            "task": "arc_challenge",
            "shots": 10,
            "type": "multiple_choice",
            "accuracy": 0.392,
            "centered": 0.1893
          },
          {
            "task": "copa",
            "shots": 0,
            "type": "multiple_choice",
            "accuracy": 0.71,
            "centered": 0.42
          },
          {
            "task": "commonsense_qa",
            "shots": 10,
            "type": "multiple_choice",
            "accuracy": 0.2,
            "centered": 0.0
          },
          {
            "task": "piqa",
            "shots": 10,
            "type": "multiple_choice",
            "accuracy": 0.722,
            "centered": 0.444
          },
          {
            "task": "openbook_qa",
            "shots": 0,
            "type": "multiple_choice",
            "accuracy": 0.394,
            "centered": 0.192
          },
          {
            "task": "lambada_openai",
            "shots": 0,
            "type": "language_modeling",
            "accuracy": 0.314,
            "centered": 0.314
          },
          {
            "task": "hellaswag",
            "shots": 10,
            "type": "multiple_choice",
            "accuracy": 0.514,
            "centered": 0.352
          },
          {
            "task": "winograd",
            "shots": 0,
            "type": "schema",
            "accuracy": 0.6557,
            "centered": 0.3114
          },
          {
            "task": "winogrande",
            "shots": 0,
            "type": "schema",
            "accuracy": 0.542,
            "centered": 0.084
          },
          {
            "task": "bigbench_dyck_languages",
            "shots": 10,
            "type": "language_modeling",
            "accuracy": 0.104,
            "centered": 0.104
          },
          {
            "task": "agi_eval_lsat_ar",
            "shots": 3,
            "type": "multiple_choice",
            "accuracy": 0.2261,
            "centered": 0.0326
          },
          {
            "task": "bigbench_cs_algorithms",
            "shots": 10,
            "type": "language_modeling",
            "accuracy": 0.436,
            "centered": 0.436
          },
          {
            "task": "bigbench_operators",
            "shots": 10,
            "type": "language_modeling",
            "accuracy": 0.1571,
            "centered": 0.1571
          },
          {
            "task": "bigbench_repeat_copy_logic",
            "shots": 10,
            "type": "language_modeling",
            "accuracy": 0.0,
            "centered": 0.0
          },
          {
            "task": "squad",
            "shots": 10,
            "type": "language_modeling",
            "accuracy": 0.196,
            "centered": 0.196
          },
          {
            "task": "coqa",
            "shots": 0,
            "type": "language_modeling",
            "accuracy": 0.2,
            "centered": 0.2
          },
          {
            "task": "boolq",
            "shots": 10,
            "type": "multiple_choice",
            "accuracy": 0.588,
            "centered": -0.0842
          },
          {
            "task": "bigbench_language_identification",
            "shots": 10,
            "type": "multiple_choice",
            "accuracy": 0.248,
            "centered": 0.1727
          }
        ]
      },
      "24000": {
        "core": 0.229,
        "val_bpb": 0.602449,
        "cdpk": {
          "acc": 0.2803,
          "n": 252,
          "d": 899
        },
        "tasks": [
          {
            "task": "hellaswag_zeroshot",
            "shots": 0,
            "type": "multiple_choice",
            "accuracy": 0.54,
            "centered": 0.3867
          },
          {
            "task": "jeopardy",
            "shots": 10,
            "type": "language_modeling",
            "accuracy": 0.022,
            "centered": 0.022
          },
          {
            "task": "bigbench_qa_wikidata",
            "shots": 10,
            "type": "language_modeling",
            "accuracy": 0.344,
            "centered": 0.344
          },
          {
            "task": "arc_easy",
            "shots": 10,
            "type": "multiple_choice",
            "accuracy": 0.69,
            "centered": 0.5867
          },
          {
            "task": "arc_challenge",
            "shots": 10,
            "type": "multiple_choice",
            "accuracy": 0.418,
            "centered": 0.224
          },
          {
            "task": "copa",
            "shots": 0,
            "type": "multiple_choice",
            "accuracy": 0.68,
            "centered": 0.36
          },
          {
            "task": "commonsense_qa",
            "shots": 10,
            "type": "multiple_choice",
            "accuracy": 0.222,
            "centered": 0.0275
          },
          {
            "task": "piqa",
            "shots": 10,
            "type": "multiple_choice",
            "accuracy": 0.726,
            "centered": 0.452
          },
          {
            "task": "openbook_qa",
            "shots": 0,
            "type": "multiple_choice",
            "accuracy": 0.384,
            "centered": 0.1787
          },
          {
            "task": "lambada_openai",
            "shots": 0,
            "type": "language_modeling",
            "accuracy": 0.32,
            "centered": 0.32
          },
          {
            "task": "hellaswag",
            "shots": 10,
            "type": "multiple_choice",
            "accuracy": 0.518,
            "centered": 0.3573
          },
          {
            "task": "winograd",
            "shots": 0,
            "type": "schema",
            "accuracy": 0.6703,
            "centered": 0.3407
          },
          {
            "task": "winogrande",
            "shots": 0,
            "type": "schema",
            "accuracy": 0.554,
            "centered": 0.108
          },
          {
            "task": "bigbench_dyck_languages",
            "shots": 10,
            "type": "language_modeling",
            "accuracy": 0.094,
            "centered": 0.094
          },
          {
            "task": "agi_eval_lsat_ar",
            "shots": 3,
            "type": "multiple_choice",
            "accuracy": 0.2304,
            "centered": 0.038
          },
          {
            "task": "bigbench_cs_algorithms",
            "shots": 10,
            "type": "language_modeling",
            "accuracy": 0.426,
            "centered": 0.426
          },
          {
            "task": "bigbench_operators",
            "shots": 10,
            "type": "language_modeling",
            "accuracy": 0.2,
            "centered": 0.2
          },
          {
            "task": "bigbench_repeat_copy_logic",
            "shots": 10,
            "type": "language_modeling",
            "accuracy": 0.0,
            "centered": 0.0
          },
          {
            "task": "squad",
            "shots": 10,
            "type": "language_modeling",
            "accuracy": 0.234,
            "centered": 0.234
          },
          {
            "task": "coqa",
            "shots": 0,
            "type": "language_modeling",
            "accuracy": 0.176,
            "centered": 0.176
          },
          {
            "task": "boolq",
            "shots": 10,
            "type": "multiple_choice",
            "accuracy": 0.61,
            "centered": -0.0263
          },
          {
            "task": "bigbench_language_identification",
            "shots": 10,
            "type": "multiple_choice",
            "accuracy": 0.262,
            "centered": 0.1881
          }
        ]
      },
      "25000": {
        "core": 0.2394,
        "val_bpb": 0.601352,
        "cdpk": {
          "acc": 0.2836,
          "n": 255,
          "d": 899
        },
        "tasks": [
          {
            "task": "hellaswag_zeroshot",
            "shots": 0,
            "type": "multiple_choice",
            "accuracy": 0.544,
            "centered": 0.392
          },
          {
            "task": "jeopardy",
            "shots": 10,
            "type": "language_modeling",
            "accuracy": 0.02,
            "centered": 0.02
          },
          {
            "task": "bigbench_qa_wikidata",
            "shots": 10,
            "type": "language_modeling",
            "accuracy": 0.342,
            "centered": 0.342
          },
          {
            "task": "arc_easy",
            "shots": 10,
            "type": "multiple_choice",
            "accuracy": 0.696,
            "centered": 0.5947
          },
          {
            "task": "arc_challenge",
            "shots": 10,
            "type": "multiple_choice",
            "accuracy": 0.424,
            "centered": 0.232
          },
          {
            "task": "copa",
            "shots": 0,
            "type": "multiple_choice",
            "accuracy": 0.66,
            "centered": 0.32
          },
          {
            "task": "commonsense_qa",
            "shots": 10,
            "type": "multiple_choice",
            "accuracy": 0.344,
            "centered": 0.18
          },
          {
            "task": "piqa",
            "shots": 10,
            "type": "multiple_choice",
            "accuracy": 0.718,
            "centered": 0.436
          },
          {
            "task": "openbook_qa",
            "shots": 0,
            "type": "multiple_choice",
            "accuracy": 0.376,
            "centered": 0.168
          },
          {
            "task": "lambada_openai",
            "shots": 0,
            "type": "language_modeling",
            "accuracy": 0.35,
            "centered": 0.35
          },
          {
            "task": "hellaswag",
            "shots": 10,
            "type": "multiple_choice",
            "accuracy": 0.542,
            "centered": 0.3893
          },
          {
            "task": "winograd",
            "shots": 0,
            "type": "schema",
            "accuracy": 0.6593,
            "centered": 0.3187
          },
          {
            "task": "winogrande",
            "shots": 0,
            "type": "schema",
            "accuracy": 0.558,
            "centered": 0.116
          },
          {
            "task": "bigbench_dyck_languages",
            "shots": 10,
            "type": "language_modeling",
            "accuracy": 0.064,
            "centered": 0.064
          },
          {
            "task": "agi_eval_lsat_ar",
            "shots": 3,
            "type": "multiple_choice",
            "accuracy": 0.3087,
            "centered": 0.1359
          },
          {
            "task": "bigbench_cs_algorithms",
            "shots": 10,
            "type": "language_modeling",
            "accuracy": 0.424,
            "centered": 0.424
          },
          {
            "task": "bigbench_operators",
            "shots": 10,
            "type": "language_modeling",
            "accuracy": 0.1952,
            "centered": 0.1952
          },
          {
            "task": "bigbench_repeat_copy_logic",
            "shots": 10,
            "type": "language_modeling",
            "accuracy": 0.0312,
            "centered": 0.0312
          },
          {
            "task": "squad",
            "shots": 10,
            "type": "language_modeling",
            "accuracy": 0.262,
            "centered": 0.262
          },
          {
            "task": "coqa",
            "shots": 0,
            "type": "language_modeling",
            "accuracy": 0.202,
            "centered": 0.202
          },
          {
            "task": "boolq",
            "shots": 10,
            "type": "multiple_choice",
            "accuracy": 0.584,
            "centered": -0.0947
          },
          {
            "task": "bigbench_language_identification",
            "shots": 10,
            "type": "multiple_choice",
            "accuracy": 0.262,
            "centered": 0.1881
          }
        ]
      },
      "26000": {
        "core": 0.234,
        "val_bpb": 0.601095,
        "cdpk": {
          "acc": 0.2747,
          "n": 247,
          "d": 899
        },
        "tasks": [
          {
            "task": "hellaswag_zeroshot",
            "shots": 0,
            "type": "multiple_choice",
            "accuracy": 0.54,
            "centered": 0.3867
          },
          {
            "task": "jeopardy",
            "shots": 10,
            "type": "language_modeling",
            "accuracy": 0.03,
            "centered": 0.03
          },
          {
            "task": "bigbench_qa_wikidata",
            "shots": 10,
            "type": "language_modeling",
            "accuracy": 0.354,
            "centered": 0.354
          },
          {
            "task": "arc_easy",
            "shots": 10,
            "type": "multiple_choice",
            "accuracy": 0.684,
            "centered": 0.5787
          },
          {
            "task": "arc_challenge",
            "shots": 10,
            "type": "multiple_choice",
            "accuracy": 0.406,
            "centered": 0.208
          },
          {
            "task": "copa",
            "shots": 0,
            "type": "multiple_choice",
            "accuracy": 0.67,
            "centered": 0.34
          },
          {
            "task": "commonsense_qa",
            "shots": 10,
            "type": "multiple_choice",
            "accuracy": 0.246,
            "centered": 0.0575
          },
          {
            "task": "piqa",
            "shots": 10,
            "type": "multiple_choice",
            "accuracy": 0.74,
            "centered": 0.48
          },
          {
            "task": "openbook_qa",
            "shots": 0,
            "type": "multiple_choice",
            "accuracy": 0.388,
            "centered": 0.184
          },
          {
            "task": "lambada_openai",
            "shots": 0,
            "type": "language_modeling",
            "accuracy": 0.326,
            "centered": 0.326
          },
          {
            "task": "hellaswag",
            "shots": 10,
            "type": "multiple_choice",
            "accuracy": 0.526,
            "centered": 0.368
          },
          {
            "task": "winograd",
            "shots": 0,
            "type": "schema",
            "accuracy": 0.6703,
            "centered": 0.3407
          },
          {
            "task": "winogrande",
            "shots": 0,
            "type": "schema",
            "accuracy": 0.58,
            "centered": 0.16
          },
          {
            "task": "bigbench_dyck_languages",
            "shots": 10,
            "type": "language_modeling",
            "accuracy": 0.102,
            "centered": 0.102
          },
          {
            "task": "agi_eval_lsat_ar",
            "shots": 3,
            "type": "multiple_choice",
            "accuracy": 0.2,
            "centered": -0.0
          },
          {
            "task": "bigbench_cs_algorithms",
            "shots": 10,
            "type": "language_modeling",
            "accuracy": 0.442,
            "centered": 0.442
          },
          {
            "task": "bigbench_operators",
            "shots": 10,
            "type": "language_modeling",
            "accuracy": 0.1667,
            "centered": 0.1667
          },
          {
            "task": "bigbench_repeat_copy_logic",
            "shots": 10,
            "type": "language_modeling",
            "accuracy": 0.0,
            "centered": 0.0
          },
          {
            "task": "squad",
            "shots": 10,
            "type": "language_modeling",
            "accuracy": 0.246,
            "centered": 0.246
          },
          {
            "task": "coqa",
            "shots": 0,
            "type": "language_modeling",
            "accuracy": 0.186,
            "centered": 0.186
          },
          {
            "task": "boolq",
            "shots": 10,
            "type": "multiple_choice",
            "accuracy": 0.626,
            "centered": 0.0158
          },
          {
            "task": "bigbench_language_identification",
            "shots": 10,
            "type": "multiple_choice",
            "accuracy": 0.252,
            "centered": 0.1771
          }
        ]
      },
      "27000": {
        "core": 0.2416,
        "val_bpb": 0.600299,
        "cdpk": {
          "acc": 0.2881,
          "n": 259,
          "d": 899
        },
        "tasks": [
          {
            "task": "hellaswag_zeroshot",
            "shots": 0,
            "type": "multiple_choice",
            "accuracy": 0.538,
            "centered": 0.384
          },
          {
            "task": "jeopardy",
            "shots": 10,
            "type": "language_modeling",
            "accuracy": 0.028,
            "centered": 0.028
          },
          {
            "task": "bigbench_qa_wikidata",
            "shots": 10,
            "type": "language_modeling",
            "accuracy": 0.35,
            "centered": 0.35
          },
          {
            "task": "arc_easy",
            "shots": 10,
            "type": "multiple_choice",
            "accuracy": 0.702,
            "centered": 0.6027
          },
          {
            "task": "arc_challenge",
            "shots": 10,
            "type": "multiple_choice",
            "accuracy": 0.406,
            "centered": 0.208
          },
          {
            "task": "copa",
            "shots": 0,
            "type": "multiple_choice",
            "accuracy": 0.66,
            "centered": 0.32
          },
          {
            "task": "commonsense_qa",
            "shots": 10,
            "type": "multiple_choice",
            "accuracy": 0.388,
            "centered": 0.235
          },
          {
            "task": "piqa",
            "shots": 10,
            "type": "multiple_choice",
            "accuracy": 0.736,
            "centered": 0.472
          },
          {
            "task": "openbook_qa",
            "shots": 0,
            "type": "multiple_choice",
            "accuracy": 0.368,
            "centered": 0.1573
          },
          {
            "task": "lambada_openai",
            "shots": 0,
            "type": "language_modeling",
            "accuracy": 0.34,
            "centered": 0.34
          },
          {
            "task": "hellaswag",
            "shots": 10,
            "type": "multiple_choice",
            "accuracy": 0.522,
            "centered": 0.3627
          },
          {
            "task": "winograd",
            "shots": 0,
            "type": "schema",
            "accuracy": 0.674,
            "centered": 0.348
          },
          {
            "task": "winogrande",
            "shots": 0,
            "type": "schema",
            "accuracy": 0.556,
            "centered": 0.112
          },
          {
            "task": "bigbench_dyck_languages",
            "shots": 10,
            "type": "language_modeling",
            "accuracy": 0.132,
            "centered": 0.132
          },
          {
            "task": "agi_eval_lsat_ar",
            "shots": 3,
            "type": "multiple_choice",
            "accuracy": 0.2565,
            "centered": 0.0707
          },
          {
            "task": "bigbench_cs_algorithms",
            "shots": 10,
            "type": "language_modeling",
            "accuracy": 0.414,
            "centered": 0.414
          },
          {
            "task": "bigbench_operators",
            "shots": 10,
            "type": "language_modeling",
            "accuracy": 0.1714,
            "centered": 0.1714
          },
          {
            "task": "bigbench_repeat_copy_logic",
            "shots": 10,
            "type": "language_modeling",
            "accuracy": 0.0,
            "centered": 0.0
          },
          {
            "task": "squad",
            "shots": 10,
            "type": "language_modeling",
            "accuracy": 0.272,
            "centered": 0.272
          },
          {
            "task": "coqa",
            "shots": 0,
            "type": "language_modeling",
            "accuracy": 0.188,
            "centered": 0.188
          },
          {
            "task": "boolq",
            "shots": 10,
            "type": "multiple_choice",
            "accuracy": 0.61,
            "centered": -0.0263
          },
          {
            "task": "bigbench_language_identification",
            "shots": 10,
            "type": "multiple_choice",
            "accuracy": 0.248,
            "centered": 0.1727
          }
        ]
      },
      "28000": {
        "core": 0.239,
        "val_bpb": 0.597373,
        "cdpk": {
          "acc": 0.2736,
          "n": 246,
          "d": 899
        },
        "tasks": [
          {
            "task": "hellaswag_zeroshot",
            "shots": 0,
            "type": "multiple_choice",
            "accuracy": 0.53,
            "centered": 0.3733
          },
          {
            "task": "jeopardy",
            "shots": 10,
            "type": "language_modeling",
            "accuracy": 0.038,
            "centered": 0.038
          },
          {
            "task": "bigbench_qa_wikidata",
            "shots": 10,
            "type": "language_modeling",
            "accuracy": 0.346,
            "centered": 0.346
          },
          {
            "task": "arc_easy",
            "shots": 10,
            "type": "multiple_choice",
            "accuracy": 0.688,
            "centered": 0.584
          },
          {
            "task": "arc_challenge",
            "shots": 10,
            "type": "multiple_choice",
            "accuracy": 0.372,
            "centered": 0.1627
          },
          {
            "task": "copa",
            "shots": 0,
            "type": "multiple_choice",
            "accuracy": 0.68,
            "centered": 0.36
          },
          {
            "task": "commonsense_qa",
            "shots": 10,
            "type": "multiple_choice",
            "accuracy": 0.414,
            "centered": 0.2675
          },
          {
            "task": "piqa",
            "shots": 10,
            "type": "multiple_choice",
            "accuracy": 0.73,
            "centered": 0.46
          },
          {
            "task": "openbook_qa",
            "shots": 0,
            "type": "multiple_choice",
            "accuracy": 0.404,
            "centered": 0.2053
          },
          {
            "task": "lambada_openai",
            "shots": 0,
            "type": "language_modeling",
            "accuracy": 0.33,
            "centered": 0.33
          },
          {
            "task": "hellaswag",
            "shots": 10,
            "type": "multiple_choice",
            "accuracy": 0.514,
            "centered": 0.352
          },
          {
            "task": "winograd",
            "shots": 0,
            "type": "schema",
            "accuracy": 0.641,
            "centered": 0.2821
          },
          {
            "task": "winogrande",
            "shots": 0,
            "type": "schema",
            "accuracy": 0.566,
            "centered": 0.132
          },
          {
            "task": "bigbench_dyck_languages",
            "shots": 10,
            "type": "language_modeling",
            "accuracy": 0.08,
            "centered": 0.08
          },
          {
            "task": "agi_eval_lsat_ar",
            "shots": 3,
            "type": "multiple_choice",
            "accuracy": 0.2348,
            "centered": 0.0435
          },
          {
            "task": "bigbench_cs_algorithms",
            "shots": 10,
            "type": "language_modeling",
            "accuracy": 0.448,
            "centered": 0.448
          },
          {
            "task": "bigbench_operators",
            "shots": 10,
            "type": "language_modeling",
            "accuracy": 0.1571,
            "centered": 0.1571
          },
          {
            "task": "bigbench_repeat_copy_logic",
            "shots": 10,
            "type": "language_modeling",
            "accuracy": 0.0,
            "centered": 0.0
          },
          {
            "task": "squad",
            "shots": 10,
            "type": "language_modeling",
            "accuracy": 0.224,
            "centered": 0.224
          },
          {
            "task": "coqa",
            "shots": 0,
            "type": "language_modeling",
            "accuracy": 0.204,
            "centered": 0.204
          },
          {
            "task": "boolq",
            "shots": 10,
            "type": "multiple_choice",
            "accuracy": 0.634,
            "centered": 0.0368
          },
          {
            "task": "bigbench_language_identification",
            "shots": 10,
            "type": "multiple_choice",
            "accuracy": 0.248,
            "centered": 0.1727
          }
        ]
      },
      "29000": {
        "core": 0.2389,
        "val_bpb": 0.597409,
        "cdpk": {
          "acc": 0.2759,
          "n": 248,
          "d": 899
        },
        "tasks": [
          {
            "task": "hellaswag_zeroshot",
            "shots": 0,
            "type": "multiple_choice",
            "accuracy": 0.552,
            "centered": 0.4027
          },
          {
            "task": "jeopardy",
            "shots": 10,
            "type": "language_modeling",
            "accuracy": 0.058,
            "centered": 0.058
          },
          {
            "task": "bigbench_qa_wikidata",
            "shots": 10,
            "type": "language_modeling",
            "accuracy": 0.354,
            "centered": 0.354
          },
          {
            "task": "arc_easy",
            "shots": 10,
            "type": "multiple_choice",
            "accuracy": 0.7,
            "centered": 0.6
          },
          {
            "task": "arc_challenge",
            "shots": 10,
            "type": "multiple_choice",
            "accuracy": 0.424,
            "centered": 0.232
          },
          {
            "task": "copa",
            "shots": 0,
            "type": "multiple_choice",
            "accuracy": 0.72,
            "centered": 0.44
          },
          {
            "task": "commonsense_qa",
            "shots": 10,
            "type": "multiple_choice",
            "accuracy": 0.204,
            "centered": 0.005
          },
          {
            "task": "piqa",
            "shots": 10,
            "type": "multiple_choice",
            "accuracy": 0.718,
            "centered": 0.436
          },
          {
            "task": "openbook_qa",
            "shots": 0,
            "type": "multiple_choice",
            "accuracy": 0.396,
            "centered": 0.1947
          },
          {
            "task": "lambada_openai",
            "shots": 0,
            "type": "language_modeling",
            "accuracy": 0.29,
            "centered": 0.29
          },
          {
            "task": "hellaswag",
            "shots": 10,
            "type": "multiple_choice",
            "accuracy": 0.54,
            "centered": 0.3867
          },
          {
            "task": "winograd",
            "shots": 0,
            "type": "schema",
            "accuracy": 0.652,
            "centered": 0.304
          },
          {
            "task": "winogrande",
            "shots": 0,
            "type": "schema",
            "accuracy": 0.556,
            "centered": 0.112
          },
          {
            "task": "bigbench_dyck_languages",
            "shots": 10,
            "type": "language_modeling",
            "accuracy": 0.122,
            "centered": 0.122
          },
          {
            "task": "agi_eval_lsat_ar",
            "shots": 3,
            "type": "multiple_choice",
            "accuracy": 0.287,
            "centered": 0.1087
          },
          {
            "task": "bigbench_cs_algorithms",
            "shots": 10,
            "type": "language_modeling",
            "accuracy": 0.436,
            "centered": 0.436
          },
          {
            "task": "bigbench_operators",
            "shots": 10,
            "type": "language_modeling",
            "accuracy": 0.2,
            "centered": 0.2
          },
          {
            "task": "bigbench_repeat_copy_logic",
            "shots": 10,
            "type": "language_modeling",
            "accuracy": 0.0,
            "centered": 0.0
          },
          {
            "task": "squad",
            "shots": 10,
            "type": "language_modeling",
            "accuracy": 0.226,
            "centered": 0.226
          },
          {
            "task": "coqa",
            "shots": 0,
            "type": "language_modeling",
            "accuracy": 0.17,
            "centered": 0.17
          },
          {
            "task": "boolq",
            "shots": 10,
            "type": "multiple_choice",
            "accuracy": 0.626,
            "centered": 0.0158
          },
          {
            "task": "bigbench_language_identification",
            "shots": 10,
            "type": "multiple_choice",
            "accuracy": 0.238,
            "centered": 0.1617
          }
        ]
      },
      "30000": {
        "core": 0.2359,
        "val_bpb": 0.594936,
        "cdpk": {
          "acc": 0.2925,
          "n": 263,
          "d": 899
        },
        "tasks": [
          {
            "task": "hellaswag_zeroshot",
            "shots": 0,
            "type": "multiple_choice",
            "accuracy": 0.552,
            "centered": 0.4027
          },
          {
            "task": "jeopardy",
            "shots": 10,
            "type": "language_modeling",
            "accuracy": 0.058,
            "centered": 0.058
          },
          {
            "task": "bigbench_qa_wikidata",
            "shots": 10,
            "type": "language_modeling",
            "accuracy": 0.32,
            "centered": 0.32
          },
          {
            "task": "arc_easy",
            "shots": 10,
            "type": "multiple_choice",
            "accuracy": 0.716,
            "centered": 0.6213
          },
          {
            "task": "arc_challenge",
            "shots": 10,
            "type": "multiple_choice",
            "accuracy": 0.42,
            "centered": 0.2267
          },
          {
            "task": "copa",
            "shots": 0,
            "type": "multiple_choice",
            "accuracy": 0.68,
            "centered": 0.36
          },
          {
            "task": "commonsense_qa",
            "shots": 10,
            "type": "multiple_choice",
            "accuracy": 0.222,
            "centered": 0.0275
          },
          {
            "task": "piqa",
            "shots": 10,
            "type": "multiple_choice",
            "accuracy": 0.716,
            "centered": 0.432
          },
          {
            "task": "openbook_qa",
            "shots": 0,
            "type": "multiple_choice",
            "accuracy": 0.426,
            "centered": 0.2347
          },
          {
            "task": "lambada_openai",
            "shots": 0,
            "type": "language_modeling",
            "accuracy": 0.366,
            "centered": 0.366
          },
          {
            "task": "hellaswag",
            "shots": 10,
            "type": "multiple_choice",
            "accuracy": 0.536,
            "centered": 0.3813
          },
          {
            "task": "winograd",
            "shots": 0,
            "type": "schema",
            "accuracy": 0.619,
            "centered": 0.2381
          },
          {
            "task": "winogrande",
            "shots": 0,
            "type": "schema",
            "accuracy": 0.546,
            "centered": 0.092
          },
          {
            "task": "bigbench_dyck_languages",
            "shots": 10,
            "type": "language_modeling",
            "accuracy": 0.074,
            "centered": 0.074
          },
          {
            "task": "agi_eval_lsat_ar",
            "shots": 3,
            "type": "multiple_choice",
            "accuracy": 0.287,
            "centered": 0.1087
          },
          {
            "task": "bigbench_cs_algorithms",
            "shots": 10,
            "type": "language_modeling",
            "accuracy": 0.438,
            "centered": 0.438
          },
          {
            "task": "bigbench_operators",
            "shots": 10,
            "type": "language_modeling",
            "accuracy": 0.2048,
            "centered": 0.2048
          },
          {
            "task": "bigbench_repeat_copy_logic",
            "shots": 10,
            "type": "language_modeling",
            "accuracy": 0.0,
            "centered": 0.0
          },
          {
            "task": "squad",
            "shots": 10,
            "type": "language_modeling",
            "accuracy": 0.302,
            "centered": 0.302
          },
          {
            "task": "coqa",
            "shots": 0,
            "type": "language_modeling",
            "accuracy": 0.186,
            "centered": 0.186
          },
          {
            "task": "boolq",
            "shots": 10,
            "type": "multiple_choice",
            "accuracy": 0.598,
            "centered": -0.0579
          },
          {
            "task": "bigbench_language_identification",
            "shots": 10,
            "type": "multiple_choice",
            "accuracy": 0.25,
            "centered": 0.1749
          }
        ]
      },
      "31000": {
        "core": 0.2422,
        "val_bpb": 0.594326,
        "cdpk": {
          "acc": 0.2825,
          "n": 254,
          "d": 899
        },
        "tasks": [
          {
            "task": "hellaswag_zeroshot",
            "shots": 0,
            "type": "multiple_choice",
            "accuracy": 0.542,
            "centered": 0.3893
          },
          {
            "task": "jeopardy",
            "shots": 10,
            "type": "language_modeling",
            "accuracy": 0.068,
            "centered": 0.068
          },
          {
            "task": "bigbench_qa_wikidata",
            "shots": 10,
            "type": "language_modeling",
            "accuracy": 0.328,
            "centered": 0.328
          },
          {
            "task": "arc_easy",
            "shots": 10,
            "type": "multiple_choice",
            "accuracy": 0.688,
            "centered": 0.584
          },
          {
            "task": "arc_challenge",
            "shots": 10,
            "type": "multiple_choice",
            "accuracy": 0.402,
            "centered": 0.2027
          },
          {
            "task": "copa",
            "shots": 0,
            "type": "multiple_choice",
            "accuracy": 0.69,
            "centered": 0.38
          },
          {
            "task": "commonsense_qa",
            "shots": 10,
            "type": "multiple_choice",
            "accuracy": 0.322,
            "centered": 0.1525
          },
          {
            "task": "piqa",
            "shots": 10,
            "type": "multiple_choice",
            "accuracy": 0.702,
            "centered": 0.404
          },
          {
            "task": "openbook_qa",
            "shots": 0,
            "type": "multiple_choice",
            "accuracy": 0.392,
            "centered": 0.1893
          },
          {
            "task": "lambada_openai",
            "shots": 0,
            "type": "language_modeling",
            "accuracy": 0.342,
            "centered": 0.342
          },
          {
            "task": "hellaswag",
            "shots": 10,
            "type": "multiple_choice",
            "accuracy": 0.542,
            "centered": 0.3893
          },
          {
            "task": "winograd",
            "shots": 0,
            "type": "schema",
            "accuracy": 0.6593,
            "centered": 0.3187
          },
          {
            "task": "winogrande",
            "shots": 0,
            "type": "schema",
            "accuracy": 0.574,
            "centered": 0.148
          },
          {
            "task": "bigbench_dyck_languages",
            "shots": 10,
            "type": "language_modeling",
            "accuracy": 0.112,
            "centered": 0.112
          },
          {
            "task": "agi_eval_lsat_ar",
            "shots": 3,
            "type": "multiple_choice",
            "accuracy": 0.2609,
            "centered": 0.0761
          },
          {
            "task": "bigbench_cs_algorithms",
            "shots": 10,
            "type": "language_modeling",
            "accuracy": 0.42,
            "centered": 0.42
          },
          {
            "task": "bigbench_operators",
            "shots": 10,
            "type": "language_modeling",
            "accuracy": 0.1333,
            "centered": 0.1333
          },
          {
            "task": "bigbench_repeat_copy_logic",
            "shots": 10,
            "type": "language_modeling",
            "accuracy": 0.0,
            "centered": 0.0
          },
          {
            "task": "squad",
            "shots": 10,
            "type": "language_modeling",
            "accuracy": 0.254,
            "centered": 0.254
          },
          {
            "task": "coqa",
            "shots": 0,
            "type": "language_modeling",
            "accuracy": 0.206,
            "centered": 0.206
          },
          {
            "task": "boolq",
            "shots": 10,
            "type": "multiple_choice",
            "accuracy": 0.634,
            "centered": 0.0368
          },
          {
            "task": "bigbench_language_identification",
            "shots": 10,
            "type": "multiple_choice",
            "accuracy": 0.268,
            "centered": 0.1947
          }
        ]
      },
      "32000": {
        "core": 0.2383,
        "val_bpb": 0.59219,
        "cdpk": {
          "acc": 0.287,
          "n": 258,
          "d": 899
        },
        "tasks": [
          {
            "task": "hellaswag_zeroshot",
            "shots": 0,
            "type": "multiple_choice",
            "accuracy": 0.548,
            "centered": 0.3973
          },
          {
            "task": "jeopardy",
            "shots": 10,
            "type": "language_modeling",
            "accuracy": 0.04,
            "centered": 0.04
          },
          {
            "task": "bigbench_qa_wikidata",
            "shots": 10,
            "type": "language_modeling",
            "accuracy": 0.294,
            "centered": 0.294
          },
          {
            "task": "arc_easy",
            "shots": 10,
            "type": "multiple_choice",
            "accuracy": 0.668,
            "centered": 0.5573
          },
          {
            "task": "arc_challenge",
            "shots": 10,
            "type": "multiple_choice",
            "accuracy": 0.398,
            "centered": 0.1973
          },
          {
            "task": "copa",
            "shots": 0,
            "type": "multiple_choice",
            "accuracy": 0.62,
            "centered": 0.24
          },
          {
            "task": "commonsense_qa",
            "shots": 10,
            "type": "multiple_choice",
            "accuracy": 0.412,
            "centered": 0.265
          },
          {
            "task": "piqa",
            "shots": 10,
            "type": "multiple_choice",
            "accuracy": 0.738,
            "centered": 0.476
          },
          {
            "task": "openbook_qa",
            "shots": 0,
            "type": "multiple_choice",
            "accuracy": 0.41,
            "centered": 0.2133
          },
          {
            "task": "lambada_openai",
            "shots": 0,
            "type": "language_modeling",
            "accuracy": 0.334,
            "centered": 0.334
          },
          {
            "task": "hellaswag",
            "shots": 10,
            "type": "multiple_choice",
            "accuracy": 0.548,
            "centered": 0.3973
          },
          {
            "task": "winograd",
            "shots": 0,
            "type": "schema",
            "accuracy": 0.663,
            "centered": 0.326
          },
          {
            "task": "winogrande",
            "shots": 0,
            "type": "schema",
            "accuracy": 0.564,
            "centered": 0.128
          },
          {
            "task": "bigbench_dyck_languages",
            "shots": 10,
            "type": "language_modeling",
            "accuracy": 0.114,
            "centered": 0.114
          },
          {
            "task": "agi_eval_lsat_ar",
            "shots": 3,
            "type": "multiple_choice",
            "accuracy": 0.2783,
            "centered": 0.0978
          },
          {
            "task": "bigbench_cs_algorithms",
            "shots": 10,
            "type": "language_modeling",
            "accuracy": 0.388,
            "centered": 0.388
          },
          {
            "task": "bigbench_operators",
            "shots": 10,
            "type": "language_modeling",
            "accuracy": 0.1714,
            "centered": 0.1714
          },
          {
            "task": "bigbench_repeat_copy_logic",
            "shots": 10,
            "type": "language_modeling",
            "accuracy": 0.0,
            "centered": 0.0
          },
          {
            "task": "squad",
            "shots": 10,
            "type": "language_modeling",
            "accuracy": 0.264,
            "centered": 0.264
          },
          {
            "task": "coqa",
            "shots": 0,
            "type": "language_modeling",
            "accuracy": 0.202,
            "centered": 0.202
          },
          {
            "task": "boolq",
            "shots": 10,
            "type": "multiple_choice",
            "accuracy": 0.61,
            "centered": -0.0263
          },
          {
            "task": "bigbench_language_identification",
            "shots": 10,
            "type": "multiple_choice",
            "accuracy": 0.242,
            "centered": 0.1661
          }
        ]
      },
      "33000": {
        "core": 0.2348,
        "val_bpb": 0.590626,
        "cdpk": {
          "acc": 0.2825,
          "n": 254,
          "d": 899
        },
        "tasks": [
          {
            "task": "hellaswag_zeroshot",
            "shots": 0,
            "type": "multiple_choice",
            "accuracy": 0.55,
            "centered": 0.4
          },
          {
            "task": "jeopardy",
            "shots": 10,
            "type": "language_modeling",
            "accuracy": 0.04,
            "centered": 0.04
          },
          {
            "task": "bigbench_qa_wikidata",
            "shots": 10,
            "type": "language_modeling",
            "accuracy": 0.336,
            "centered": 0.336
          },
          {
            "task": "arc_easy",
            "shots": 10,
            "type": "multiple_choice",
            "accuracy": 0.696,
            "centered": 0.5947
          },
          {
            "task": "arc_challenge",
            "shots": 10,
            "type": "multiple_choice",
            "accuracy": 0.402,
            "centered": 0.2027
          },
          {
            "task": "copa",
            "shots": 0,
            "type": "multiple_choice",
            "accuracy": 0.67,
            "centered": 0.34
          },
          {
            "task": "commonsense_qa",
            "shots": 10,
            "type": "multiple_choice",
            "accuracy": 0.222,
            "centered": 0.0275
          },
          {
            "task": "piqa",
            "shots": 10,
            "type": "multiple_choice",
            "accuracy": 0.708,
            "centered": 0.416
          },
          {
            "task": "openbook_qa",
            "shots": 0,
            "type": "multiple_choice",
            "accuracy": 0.38,
            "centered": 0.1733
          },
          {
            "task": "lambada_openai",
            "shots": 0,
            "type": "language_modeling",
            "accuracy": 0.326,
            "centered": 0.326
          },
          {
            "task": "hellaswag",
            "shots": 10,
            "type": "multiple_choice",
            "accuracy": 0.528,
            "centered": 0.3707
          },
          {
            "task": "winograd",
            "shots": 0,
            "type": "schema",
            "accuracy": 0.663,
            "centered": 0.326
          },
          {
            "task": "winogrande",
            "shots": 0,
            "type": "schema",
            "accuracy": 0.558,
            "centered": 0.116
          },
          {
            "task": "bigbench_dyck_languages",
            "shots": 10,
            "type": "language_modeling",
            "accuracy": 0.108,
            "centered": 0.108
          },
          {
            "task": "agi_eval_lsat_ar",
            "shots": 3,
            "type": "multiple_choice",
            "accuracy": 0.3043,
            "centered": 0.1304
          },
          {
            "task": "bigbench_cs_algorithms",
            "shots": 10,
            "type": "language_modeling",
            "accuracy": 0.392,
            "centered": 0.392
          },
          {
            "task": "bigbench_operators",
            "shots": 10,
            "type": "language_modeling",
            "accuracy": 0.1381,
            "centered": 0.1381
          },
          {
            "task": "bigbench_repeat_copy_logic",
            "shots": 10,
            "type": "language_modeling",
            "accuracy": 0.0312,
            "centered": 0.0312
          },
          {
            "task": "squad",
            "shots": 10,
            "type": "language_modeling",
            "accuracy": 0.23,
            "centered": 0.23
          },
          {
            "task": "coqa",
            "shots": 0,
            "type": "language_modeling",
            "accuracy": 0.188,
            "centered": 0.188
          },
          {
            "task": "boolq",
            "shots": 10,
            "type": "multiple_choice",
            "accuracy": 0.666,
            "centered": 0.1211
          },
          {
            "task": "bigbench_language_identification",
            "shots": 10,
            "type": "multiple_choice",
            "accuracy": 0.234,
            "centered": 0.1573
          }
        ]
      },
      "34000": {
        "core": 0.2474,
        "val_bpb": 0.590357,
        "cdpk": null,
        "tasks": [
          {
            "task": "hellaswag_zeroshot",
            "shots": 0,
            "type": "multiple_choice",
            "accuracy": 0.558,
            "centered": 0.4107
          },
          {
            "task": "jeopardy",
            "shots": 10,
            "type": "language_modeling",
            "accuracy": 0.04,
            "centered": 0.04
          },
          {
            "task": "bigbench_qa_wikidata",
            "shots": 10,
            "type": "language_modeling",
            "accuracy": 0.336,
            "centered": 0.336
          },
          {
            "task": "arc_easy",
            "shots": 10,
            "type": "multiple_choice",
            "accuracy": 0.698,
            "centered": 0.5973
          },
          {
            "task": "arc_challenge",
            "shots": 10,
            "type": "multiple_choice",
            "accuracy": 0.404,
            "centered": 0.2053
          },
          {
            "task": "copa",
            "shots": 0,
            "type": "multiple_choice",
            "accuracy": 0.7,
            "centered": 0.4
          },
          {
            "task": "commonsense_qa",
            "shots": 10,
            "type": "multiple_choice",
            "accuracy": 0.344,
            "centered": 0.18
          },
          {
            "task": "piqa",
            "shots": 10,
            "type": "multiple_choice",
            "accuracy": 0.716,
            "centered": 0.432
          },
          {
            "task": "openbook_qa",
            "shots": 0,
            "type": "multiple_choice",
            "accuracy": 0.406,
            "centered": 0.208
          },
          {
            "task": "lambada_openai",
            "shots": 0,
            "type": "language_modeling",
            "accuracy": 0.33,
            "centered": 0.33
          },
          {
            "task": "hellaswag",
            "shots": 10,
            "type": "multiple_choice",
            "accuracy": 0.536,
            "centered": 0.3813
          },
          {
            "task": "winograd",
            "shots": 0,
            "type": "schema",
            "accuracy": 0.6667,
            "centered": 0.3333
          },
          {
            "task": "winogrande",
            "shots": 0,
            "type": "schema",
            "accuracy": 0.582,
            "centered": 0.164
          },
          {
            "task": "bigbench_dyck_languages",
            "shots": 10,
            "type": "language_modeling",
            "accuracy": 0.11,
            "centered": 0.11
          },
          {
            "task": "agi_eval_lsat_ar",
            "shots": 3,
            "type": "multiple_choice",
            "accuracy": 0.2609,
            "centered": 0.0761
          },
          {
            "task": "bigbench_cs_algorithms",
            "shots": 10,
            "type": "language_modeling",
            "accuracy": 0.416,
            "centered": 0.416
          },
          {
            "task": "bigbench_operators",
            "shots": 10,
            "type": "language_modeling",
            "accuracy": 0.1762,
            "centered": 0.1762
          },
          {
            "task": "bigbench_repeat_copy_logic",
            "shots": 10,
            "type": "language_modeling",
            "accuracy": 0.0312,
            "centered": 0.0312
          },
          {
            "task": "squad",
            "shots": 10,
            "type": "language_modeling",
            "accuracy": 0.242,
            "centered": 0.242
          },
          {
            "task": "coqa",
            "shots": 0,
            "type": "language_modeling",
            "accuracy": 0.184,
            "centered": 0.184
          },
          {
            "task": "boolq",
            "shots": 10,
            "type": "multiple_choice",
            "accuracy": 0.63,
            "centered": 0.0263
          },
          {
            "task": "bigbench_language_identification",
            "shots": 10,
            "type": "multiple_choice",
            "accuracy": 0.24,
            "centered": 0.1639
          }
        ]
      }
    }
  },
  "random-8gpu (split)": {
    "wandb_name": "exp-rand16-d26-8gpu",
    "run_ids": [
      "aw30lam9",
      "4ul335c6"
    ],
    "host": "192.222.53.38",
    "data_variant": "Random/staging subset",
    "eval_steps": {
      "1000": {
        "core": 0.116,
        "val_bpb": 0.968255,
        "cdpk": {
          "acc": 0.2692,
          "n": 242,
          "d": 899
        },
        "tasks": [
          {
            "task": "hellaswag_zeroshot",
            "shots": 0,
            "type": "multiple_choice",
            "accuracy": 0.34,
            "centered": 0.12
          },
          {
            "task": "jeopardy",
            "shots": 10,
            "type": "language_modeling",
            "accuracy": 0.0,
            "centered": 0.0
          },
          {
            "task": "bigbench_qa_wikidata",
            "shots": 10,
            "type": "language_modeling",
            "accuracy": 0.29,
            "centered": 0.29
          },
          {
            "task": "arc_easy",
            "shots": 10,
            "type": "multiple_choice",
            "accuracy": 0.51,
            "centered": 0.3467
          },
          {
            "task": "arc_challenge",
            "shots": 10,
            "type": "multiple_choice",
            "accuracy": 0.27,
            "centered": 0.0267
          },
          {
            "task": "copa",
            "shots": 0,
            "type": "multiple_choice",
            "accuracy": 0.56,
            "centered": 0.12
          },
          {
            "task": "commonsense_qa",
            "shots": 10,
            "type": "multiple_choice",
            "accuracy": 0.258,
            "centered": 0.0725
          },
          {
            "task": "piqa",
            "shots": 10,
            "type": "multiple_choice",
            "accuracy": 0.618,
            "centered": 0.236
          },
          {
            "task": "openbook_qa",
            "shots": 0,
            "type": "multiple_choice",
            "accuracy": 0.314,
            "centered": 0.0853
          },
          {
            "task": "lambada_openai",
            "shots": 0,
            "type": "language_modeling",
            "accuracy": 0.268,
            "centered": 0.268
          },
          {
            "task": "hellaswag",
            "shots": 10,
            "type": "multiple_choice",
            "accuracy": 0.326,
            "centered": 0.1013
          },
          {
            "task": "winograd",
            "shots": 0,
            "type": "schema",
            "accuracy": 0.6081,
            "centered": 0.2161
          },
          {
            "task": "winogrande",
            "shots": 0,
            "type": "schema",
            "accuracy": 0.506,
            "centered": 0.012
          },
          {
            "task": "bigbench_dyck_languages",
            "shots": 10,
            "type": "language_modeling",
            "accuracy": 0.1,
            "centered": 0.1
          },
          {
            "task": "agi_eval_lsat_ar",
            "shots": 3,
            "type": "multiple_choice",
            "accuracy": 0.2913,
            "centered": 0.1141
          },
          {
            "task": "bigbench_cs_algorithms",
            "shots": 10,
            "type": "language_modeling",
            "accuracy": 0.436,
            "centered": 0.436
          },
          {
            "task": "bigbench_operators",
            "shots": 10,
            "type": "language_modeling",
            "accuracy": 0.0857,
            "centered": 0.0857
          },
          {
            "task": "bigbench_repeat_copy_logic",
            "shots": 10,
            "type": "language_modeling",
            "accuracy": 0.0,
            "centered": 0.0
          },
          {
            "task": "squad",
            "shots": 10,
            "type": "language_modeling",
            "accuracy": 0.034,
            "centered": 0.034
          },
          {
            "task": "coqa",
            "shots": 0,
            "type": "language_modeling",
            "accuracy": 0.098,
            "centered": 0.098
          },
          {
            "task": "boolq",
            "shots": 10,
            "type": "multiple_choice",
            "accuracy": 0.462,
            "centered": -0.4158
          },
          {
            "task": "bigbench_language_identification",
            "shots": 10,
            "type": "multiple_choice",
            "accuracy": 0.278,
            "centered": 0.2057
          }
        ]
      },
      "2000": {
        "core": 0.1552,
        "val_bpb": 0.922632,
        "cdpk": {
          "acc": 0.267,
          "n": 240,
          "d": 899
        },
        "tasks": [
          {
            "task": "hellaswag_zeroshot",
            "shots": 0,
            "type": "multiple_choice",
            "accuracy": 0.382,
            "centered": 0.176
          },
          {
            "task": "jeopardy",
            "shots": 10,
            "type": "language_modeling",
            "accuracy": 0.036,
            "centered": 0.036
          },
          {
            "task": "bigbench_qa_wikidata",
            "shots": 10,
            "type": "language_modeling",
            "accuracy": 0.374,
            "centered": 0.374
          },
          {
            "task": "arc_easy",
            "shots": 10,
            "type": "multiple_choice",
            "accuracy": 0.552,
            "centered": 0.4027
          },
          {
            "task": "arc_challenge",
            "shots": 10,
            "type": "multiple_choice",
            "accuracy": 0.282,
            "centered": 0.0427
          },
          {
            "task": "copa",
            "shots": 0,
            "type": "multiple_choice",
            "accuracy": 0.62,
            "centered": 0.24
          },
          {
            "task": "commonsense_qa",
            "shots": 10,
            "type": "multiple_choice",
            "accuracy": 0.308,
            "centered": 0.135
          },
          {
            "task": "piqa",
            "shots": 10,
            "type": "multiple_choice",
            "accuracy": 0.652,
            "centered": 0.304
          },
          {
            "task": "openbook_qa",
            "shots": 0,
            "type": "multiple_choice",
            "accuracy": 0.33,
            "centered": 0.1067
          },
          {
            "task": "lambada_openai",
            "shots": 0,
            "type": "language_modeling",
            "accuracy": 0.326,
            "centered": 0.326
          },
          {
            "task": "hellaswag",
            "shots": 10,
            "type": "multiple_choice",
            "accuracy": 0.372,
            "centered": 0.1627
          },
          {
            "task": "winograd",
            "shots": 0,
            "type": "schema",
            "accuracy": 0.652,
            "centered": 0.304
          },
          {
            "task": "winogrande",
            "shots": 0,
            "type": "schema",
            "accuracy": 0.458,
            "centered": -0.084
          },
          {
            "task": "bigbench_dyck_languages",
            "shots": 10,
            "type": "language_modeling",
            "accuracy": 0.13,
            "centered": 0.13
          },
          {
            "task": "agi_eval_lsat_ar",
            "shots": 3,
            "type": "multiple_choice",
            "accuracy": 0.2696,
            "centered": 0.087
          },
          {
            "task": "bigbench_cs_algorithms",
            "shots": 10,
            "type": "language_modeling",
            "accuracy": 0.368,
            "centered": 0.368
          },
          {
            "task": "bigbench_operators",
            "shots": 10,
            "type": "language_modeling",
            "accuracy": 0.1429,
            "centered": 0.1429
          },
          {
            "task": "bigbench_repeat_copy_logic",
            "shots": 10,
            "type": "language_modeling",
            "accuracy": 0.0312,
            "centered": 0.0312
          },
          {
            "task": "squad",
            "shots": 10,
            "type": "language_modeling",
            "accuracy": 0.138,
            "centered": 0.138
          },
          {
            "task": "coqa",
            "shots": 0,
            "type": "language_modeling",
            "accuracy": 0.156,
            "centered": 0.156
          },
          {
            "task": "boolq",
            "shots": 10,
            "type": "multiple_choice",
            "accuracy": 0.488,
            "centered": -0.3474
          },
          {
            "task": "bigbench_language_identification",
            "shots": 10,
            "type": "multiple_choice",
            "accuracy": 0.258,
            "centered": 0.1837
          }
        ]
      },
      "3000": {
        "core": 0.1903,
        "val_bpb": 0.898076,
        "cdpk": {
          "acc": 0.277,
          "n": 249,
          "d": 899
        },
        "tasks": [
          {
            "task": "hellaswag_zeroshot",
            "shots": 0,
            "type": "multiple_choice",
            "accuracy": 0.394,
            "centered": 0.192
          },
          {
            "task": "jeopardy",
            "shots": 10,
            "type": "language_modeling",
            "accuracy": 0.06,
            "centered": 0.06
          },
          {
            "task": "bigbench_qa_wikidata",
            "shots": 10,
            "type": "language_modeling",
            "accuracy": 0.45,
            "centered": 0.45
          },
          {
            "task": "arc_easy",
            "shots": 10,
            "type": "multiple_choice",
            "accuracy": 0.564,
            "centered": 0.4187
          },
          {
            "task": "arc_challenge",
            "shots": 10,
            "type": "multiple_choice",
            "accuracy": 0.298,
            "centered": 0.064
          },
          {
            "task": "copa",
            "shots": 0,
            "type": "multiple_choice",
            "accuracy": 0.65,
            "centered": 0.3
          },
          {
            "task": "commonsense_qa",
            "shots": 10,
            "type": "multiple_choice",
            "accuracy": 0.298,
            "centered": 0.1225
          },
          {
            "task": "piqa",
            "shots": 10,
            "type": "multiple_choice",
            "accuracy": 0.69,
            "centered": 0.38
          },
          {
            "task": "openbook_qa",
            "shots": 0,
            "type": "multiple_choice",
            "accuracy": 0.338,
            "centered": 0.1173
          },
          {
            "task": "lambada_openai",
            "shots": 0,
            "type": "language_modeling",
            "accuracy": 0.35,
            "centered": 0.35
          },
          {
            "task": "hellaswag",
            "shots": 10,
            "type": "multiple_choice",
            "accuracy": 0.384,
            "centered": 0.1787
          },
          {
            "task": "winograd",
            "shots": 0,
            "type": "schema",
            "accuracy": 0.619,
            "centered": 0.2381
          },
          {
            "task": "winogrande",
            "shots": 0,
            "type": "schema",
            "accuracy": 0.534,
            "centered": 0.068
          },
          {
            "task": "bigbench_dyck_languages",
            "shots": 10,
            "type": "language_modeling",
            "accuracy": 0.134,
            "centered": 0.134
          },
          {
            "task": "agi_eval_lsat_ar",
            "shots": 3,
            "type": "multiple_choice",
            "accuracy": 0.2783,
            "centered": 0.0978
          },
          {
            "task": "bigbench_cs_algorithms",
            "shots": 10,
            "type": "language_modeling",
            "accuracy": 0.43,
            "centered": 0.43
          },
          {
            "task": "bigbench_operators",
            "shots": 10,
            "type": "language_modeling",
            "accuracy": 0.1571,
            "centered": 0.1571
          },
          {
            "task": "bigbench_repeat_copy_logic",
            "shots": 10,
            "type": "language_modeling",
            "accuracy": 0.0312,
            "centered": 0.0312
          },
          {
            "task": "squad",
            "shots": 10,
            "type": "language_modeling",
            "accuracy": 0.208,
            "centered": 0.208
          },
          {
            "task": "coqa",
            "shots": 0,
            "type": "language_modeling",
            "accuracy": 0.198,
            "centered": 0.198
          },
          {
            "task": "boolq",
            "shots": 10,
            "type": "multiple_choice",
            "accuracy": 0.54,
            "centered": -0.2105
          },
          {
            "task": "bigbench_language_identification",
            "shots": 10,
            "type": "multiple_choice",
            "accuracy": 0.274,
            "centered": 0.2013
          }
        ]
      },
      "4000": {
        "core": 0.2,
        "val_bpb": 0.885078,
        "cdpk": {
          "acc": 0.2703,
          "n": 243,
          "d": 899
        },
        "tasks": [
          {
            "task": "hellaswag_zeroshot",
            "shots": 0,
            "type": "multiple_choice",
            "accuracy": 0.418,
            "centered": 0.224
          },
          {
            "task": "jeopardy",
            "shots": 10,
            "type": "language_modeling",
            "accuracy": 0.042,
            "centered": 0.042
          },
          {
            "task": "bigbench_qa_wikidata",
            "shots": 10,
            "type": "language_modeling",
            "accuracy": 0.436,
            "centered": 0.436
          },
          {
            "task": "arc_easy",
            "shots": 10,
            "type": "multiple_choice",
            "accuracy": 0.646,
            "centered": 0.528
          },
          {
            "task": "arc_challenge",
            "shots": 10,
            "type": "multiple_choice",
            "accuracy": 0.336,
            "centered": 0.1147
          },
          {
            "task": "copa",
            "shots": 0,
            "type": "multiple_choice",
            "accuracy": 0.66,
            "centered": 0.32
          },
          {
            "task": "commonsense_qa",
            "shots": 10,
            "type": "multiple_choice",
            "accuracy": 0.314,
            "centered": 0.1425
          },
          {
            "task": "piqa",
            "shots": 10,
            "type": "multiple_choice",
            "accuracy": 0.674,
            "centered": 0.348
          },
          {
            "task": "openbook_qa",
            "shots": 0,
            "type": "multiple_choice",
            "accuracy": 0.358,
            "centered": 0.144
          },
          {
            "task": "lambada_openai",
            "shots": 0,
            "type": "language_modeling",
            "accuracy": 0.326,
            "centered": 0.326
          },
          {
            "task": "hellaswag",
            "shots": 10,
            "type": "multiple_choice",
            "accuracy": 0.4,
            "centered": 0.2
          },
          {
            "task": "winograd",
            "shots": 0,
            "type": "schema",
            "accuracy": 0.6227,
            "centered": 0.2454
          },
          {
            "task": "winogrande",
            "shots": 0,
            "type": "schema",
            "accuracy": 0.532,
            "centered": 0.064
          },
          {
            "task": "bigbench_dyck_languages",
            "shots": 10,
            "type": "language_modeling",
            "accuracy": 0.142,
            "centered": 0.142
          },
          {
            "task": "agi_eval_lsat_ar",
            "shots": 3,
            "type": "multiple_choice",
            "accuracy": 0.2652,
            "centered": 0.0815
          },
          {
            "task": "bigbench_cs_algorithms",
            "shots": 10,
            "type": "language_modeling",
            "accuracy": 0.426,
            "centered": 0.426
          },
          {
            "task": "bigbench_operators",
            "shots": 10,
            "type": "language_modeling",
            "accuracy": 0.1476,
            "centered": 0.1476
          },
          {
            "task": "bigbench_repeat_copy_logic",
            "shots": 10,
            "type": "language_modeling",
            "accuracy": 0.0,
            "centered": 0.0
          },
          {
            "task": "squad",
            "shots": 10,
            "type": "language_modeling",
            "accuracy": 0.194,
            "centered": 0.194
          },
          {
            "task": "coqa",
            "shots": 0,
            "type": "language_modeling",
            "accuracy": 0.194,
            "centered": 0.194
          },
          {
            "task": "boolq",
            "shots": 10,
            "type": "multiple_choice",
            "accuracy": 0.586,
            "centered": -0.0895
          },
          {
            "task": "bigbench_language_identification",
            "shots": 10,
            "type": "multiple_choice",
            "accuracy": 0.246,
            "centered": 0.1705
          }
        ]
      },
      "5000": {
        "core": 0.1838,
        "val_bpb": 0.876222,
        "cdpk": {
          "acc": 0.2447,
          "n": 220,
          "d": 899
        },
        "tasks": [
          {
            "task": "hellaswag_zeroshot",
            "shots": 0,
            "type": "multiple_choice",
            "accuracy": 0.426,
            "centered": 0.2347
          },
          {
            "task": "jeopardy",
            "shots": 10,
            "type": "language_modeling",
            "accuracy": 0.064,
            "centered": 0.064
          },
          {
            "task": "bigbench_qa_wikidata",
            "shots": 10,
            "type": "language_modeling",
            "accuracy": 0.456,
            "centered": 0.456
          },
          {
            "task": "arc_easy",
            "shots": 10,
            "type": "multiple_choice",
            "accuracy": 0.634,
            "centered": 0.512
          },
          {
            "task": "arc_challenge",
            "shots": 10,
            "type": "multiple_choice",
            "accuracy": 0.334,
            "centered": 0.112
          },
          {
            "task": "copa",
            "shots": 0,
            "type": "multiple_choice",
            "accuracy": 0.66,
            "centered": 0.32
          },
          {
            "task": "commonsense_qa",
            "shots": 10,
            "type": "multiple_choice",
            "accuracy": 0.208,
            "centered": 0.01
          },
          {
            "task": "piqa",
            "shots": 10,
            "type": "multiple_choice",
            "accuracy": 0.65,
            "centered": 0.3
          },
          {
            "task": "openbook_qa",
            "shots": 0,
            "type": "multiple_choice",
            "accuracy": 0.358,
            "centered": 0.144
          },
          {
            "task": "lambada_openai",
            "shots": 0,
            "type": "language_modeling",
            "accuracy": 0.382,
            "centered": 0.382
          },
          {
            "task": "hellaswag",
            "shots": 10,
            "type": "multiple_choice",
            "accuracy": 0.41,
            "centered": 0.2133
          },
          {
            "task": "winograd",
            "shots": 0,
            "type": "schema",
            "accuracy": 0.6044,
            "centered": 0.2088
          },
          {
            "task": "winogrande",
            "shots": 0,
            "type": "schema",
            "accuracy": 0.512,
            "centered": 0.024
          },
          {
            "task": "bigbench_dyck_languages",
            "shots": 10,
            "type": "language_modeling",
            "accuracy": 0.152,
            "centered": 0.152
          },
          {
            "task": "agi_eval_lsat_ar",
            "shots": 3,
            "type": "multiple_choice",
            "accuracy": 0.2478,
            "centered": 0.0598
          },
          {
            "task": "bigbench_cs_algorithms",
            "shots": 10,
            "type": "language_modeling",
            "accuracy": 0.43,
            "centered": 0.43
          },
          {
            "task": "bigbench_operators",
            "shots": 10,
            "type": "language_modeling",
            "accuracy": 0.181,
            "centered": 0.181
          },
          {
            "task": "bigbench_repeat_copy_logic",
            "shots": 10,
            "type": "language_modeling",
            "accuracy": 0.0,
            "centered": 0.0
          },
          {
            "task": "squad",
            "shots": 10,
            "type": "language_modeling",
            "accuracy": 0.216,
            "centered": 0.216
          },
          {
            "task": "coqa",
            "shots": 0,
            "type": "language_modeling",
            "accuracy": 0.164,
            "centered": 0.164
          },
          {
            "task": "boolq",
            "shots": 10,
            "type": "multiple_choice",
            "accuracy": 0.486,
            "centered": -0.3526
          },
          {
            "task": "bigbench_language_identification",
            "shots": 10,
            "type": "multiple_choice",
            "accuracy": 0.284,
            "centered": 0.2123
          }
        ]
      },
      "6000": {
        "core": 0.2013,
        "val_bpb": 0.870165,
        "cdpk": {
          "acc": 0.2803,
          "n": 252,
          "d": 899
        },
        "tasks": [
          {
            "task": "hellaswag_zeroshot",
            "shots": 0,
            "type": "multiple_choice",
            "accuracy": 0.416,
            "centered": 0.2213
          },
          {
            "task": "jeopardy",
            "shots": 10,
            "type": "language_modeling",
            "accuracy": 0.042,
            "centered": 0.042
          },
          {
            "task": "bigbench_qa_wikidata",
            "shots": 10,
            "type": "language_modeling",
            "accuracy": 0.482,
            "centered": 0.482
          },
          {
            "task": "arc_easy",
            "shots": 10,
            "type": "multiple_choice",
            "accuracy": 0.608,
            "centered": 0.4773
          },
          {
            "task": "arc_challenge",
            "shots": 10,
            "type": "multiple_choice",
            "accuracy": 0.338,
            "centered": 0.1173
          },
          {
            "task": "copa",
            "shots": 0,
            "type": "multiple_choice",
            "accuracy": 0.7,
            "centered": 0.4
          },
          {
            "task": "commonsense_qa",
            "shots": 10,
            "type": "multiple_choice",
            "accuracy": 0.354,
            "centered": 0.1925
          },
          {
            "task": "piqa",
            "shots": 10,
            "type": "multiple_choice",
            "accuracy": 0.664,
            "centered": 0.328
          },
          {
            "task": "openbook_qa",
            "shots": 0,
            "type": "multiple_choice",
            "accuracy": 0.36,
            "centered": 0.1467
          },
          {
            "task": "lambada_openai",
            "shots": 0,
            "type": "language_modeling",
            "accuracy": 0.356,
            "centered": 0.356
          },
          {
            "task": "hellaswag",
            "shots": 10,
            "type": "multiple_choice",
            "accuracy": 0.394,
            "centered": 0.192
          },
          {
            "task": "winograd",
            "shots": 0,
            "type": "schema",
            "accuracy": 0.6484,
            "centered": 0.2967
          },
          {
            "task": "winogrande",
            "shots": 0,
            "type": "schema",
            "accuracy": 0.504,
            "centered": 0.008
          },
          {
            "task": "bigbench_dyck_languages",
            "shots": 10,
            "type": "language_modeling",
            "accuracy": 0.11,
            "centered": 0.11
          },
          {
            "task": "agi_eval_lsat_ar",
            "shots": 3,
            "type": "multiple_choice",
            "accuracy": 0.2913,
            "centered": 0.1141
          },
          {
            "task": "bigbench_cs_algorithms",
            "shots": 10,
            "type": "language_modeling",
            "accuracy": 0.444,
            "centered": 0.444
          },
          {
            "task": "bigbench_operators",
            "shots": 10,
            "type": "language_modeling",
            "accuracy": 0.1524,
            "centered": 0.1524
          },
          {
            "task": "bigbench_repeat_copy_logic",
            "shots": 10,
            "type": "language_modeling",
            "accuracy": 0.0312,
            "centered": 0.0312
          },
          {
            "task": "squad",
            "shots": 10,
            "type": "language_modeling",
            "accuracy": 0.158,
            "centered": 0.158
          },
          {
            "task": "coqa",
            "shots": 0,
            "type": "language_modeling",
            "accuracy": 0.202,
            "centered": 0.202
          },
          {
            "task": "boolq",
            "shots": 10,
            "type": "multiple_choice",
            "accuracy": 0.526,
            "centered": -0.2474
          },
          {
            "task": "bigbench_language_identification",
            "shots": 10,
            "type": "multiple_choice",
            "accuracy": 0.276,
            "centered": 0.2035
          }
        ]
      },
      "7000": {
        "core": 0.2002,
        "val_bpb": 0.866301,
        "cdpk": {
          "acc": 0.2681,
          "n": 241,
          "d": 899
        },
        "tasks": [
          {
            "task": "hellaswag_zeroshot",
            "shots": 0,
            "type": "multiple_choice",
            "accuracy": 0.428,
            "centered": 0.2373
          },
          {
            "task": "jeopardy",
            "shots": 10,
            "type": "language_modeling",
            "accuracy": 0.05,
            "centered": 0.05
          },
          {
            "task": "bigbench_qa_wikidata",
            "shots": 10,
            "type": "language_modeling",
            "accuracy": 0.464,
            "centered": 0.464
          },
          {
            "task": "arc_easy",
            "shots": 10,
            "type": "multiple_choice",
            "accuracy": 0.596,
            "centered": 0.4613
          },
          {
            "task": "arc_challenge",
            "shots": 10,
            "type": "multiple_choice",
            "accuracy": 0.332,
            "centered": 0.1093
          },
          {
            "task": "copa",
            "shots": 0,
            "type": "multiple_choice",
            "accuracy": 0.66,
            "centered": 0.32
          },
          {
            "task": "commonsense_qa",
            "shots": 10,
            "type": "multiple_choice",
            "accuracy": 0.216,
            "centered": 0.02
          },
          {
            "task": "piqa",
            "shots": 10,
            "type": "multiple_choice",
            "accuracy": 0.656,
            "centered": 0.312
          },
          {
            "task": "openbook_qa",
            "shots": 0,
            "type": "multiple_choice",
            "accuracy": 0.354,
            "centered": 0.1387
          },
          {
            "task": "lambada_openai",
            "shots": 0,
            "type": "language_modeling",
            "accuracy": 0.38,
            "centered": 0.38
          },
          {
            "task": "hellaswag",
            "shots": 10,
            "type": "multiple_choice",
            "accuracy": 0.426,
            "centered": 0.2347
          },
          {
            "task": "winograd",
            "shots": 0,
            "type": "schema",
            "accuracy": 0.6337,
            "centered": 0.2674
          },
          {
            "task": "winogrande",
            "shots": 0,
            "type": "schema",
            "accuracy": 0.5,
            "centered": 0.0
          },
          {
            "task": "bigbench_dyck_languages",
            "shots": 10,
            "type": "language_modeling",
            "accuracy": 0.148,
            "centered": 0.148
          },
          {
            "task": "agi_eval_lsat_ar",
            "shots": 3,
            "type": "multiple_choice",
            "accuracy": 0.287,
            "centered": 0.1087
          },
          {
            "task": "bigbench_cs_algorithms",
            "shots": 10,
            "type": "language_modeling",
            "accuracy": 0.438,
            "centered": 0.438
          },
          {
            "task": "bigbench_operators",
            "shots": 10,
            "type": "language_modeling",
            "accuracy": 0.1476,
            "centered": 0.1476
          },
          {
            "task": "bigbench_repeat_copy_logic",
            "shots": 10,
            "type": "language_modeling",
            "accuracy": 0.0,
            "centered": 0.0
          },
          {
            "task": "squad",
            "shots": 10,
            "type": "language_modeling",
            "accuracy": 0.232,
            "centered": 0.232
          },
          {
            "task": "coqa",
            "shots": 0,
            "type": "language_modeling",
            "accuracy": 0.186,
            "centered": 0.186
          },
          {
            "task": "boolq",
            "shots": 10,
            "type": "multiple_choice",
            "accuracy": 0.598,
            "centered": -0.0579
          },
          {
            "task": "bigbench_language_identification",
            "shots": 10,
            "type": "multiple_choice",
            "accuracy": 0.28,
            "centered": 0.2079
          }
        ]
      },
      "8000": {
        "core": 0.2038,
        "val_bpb": 0.863745,
        "cdpk": {
          "acc": 0.257,
          "n": 231,
          "d": 899
        },
        "tasks": [
          {
            "task": "hellaswag_zeroshot",
            "shots": 0,
            "type": "multiple_choice",
            "accuracy": 0.422,
            "centered": 0.2293
          },
          {
            "task": "jeopardy",
            "shots": 10,
            "type": "language_modeling",
            "accuracy": 0.118,
            "centered": 0.118
          },
          {
            "task": "bigbench_qa_wikidata",
            "shots": 10,
            "type": "language_modeling",
            "accuracy": 0.486,
            "centered": 0.486
          },
          {
            "task": "arc_easy",
            "shots": 10,
            "type": "multiple_choice",
            "accuracy": 0.634,
            "centered": 0.512
          },
          {
            "task": "arc_challenge",
            "shots": 10,
            "type": "multiple_choice",
            "accuracy": 0.332,
            "centered": 0.1093
          },
          {
            "task": "copa",
            "shots": 0,
            "type": "multiple_choice",
            "accuracy": 0.65,
            "centered": 0.3
          },
          {
            "task": "commonsense_qa",
            "shots": 10,
            "type": "multiple_choice",
            "accuracy": 0.264,
            "centered": 0.08
          },
          {
            "task": "piqa",
            "shots": 10,
            "type": "multiple_choice",
            "accuracy": 0.672,
            "centered": 0.344
          },
          {
            "task": "openbook_qa",
            "shots": 0,
            "type": "multiple_choice",
            "accuracy": 0.382,
            "centered": 0.176
          },
          {
            "task": "lambada_openai",
            "shots": 0,
            "type": "language_modeling",
            "accuracy": 0.394,
            "centered": 0.394
          },
          {
            "task": "hellaswag",
            "shots": 10,
            "type": "multiple_choice",
            "accuracy": 0.412,
            "centered": 0.216
          },
          {
            "task": "winograd",
            "shots": 0,
            "type": "schema",
            "accuracy": 0.6484,
            "centered": 0.2967
          },
          {
            "task": "winogrande",
            "shots": 0,
            "type": "schema",
            "accuracy": 0.536,
            "centered": 0.072
          },
          {
            "task": "bigbench_dyck_languages",
            "shots": 10,
            "type": "language_modeling",
            "accuracy": 0.15,
            "centered": 0.15
          },
          {
            "task": "agi_eval_lsat_ar",
            "shots": 3,
            "type": "multiple_choice",
            "accuracy": 0.2957,
            "centered": 0.1196
          },
          {
            "task": "bigbench_cs_algorithms",
            "shots": 10,
            "type": "language_modeling",
            "accuracy": 0.436,
            "centered": 0.436
          },
          {
            "task": "bigbench_operators",
            "shots": 10,
            "type": "language_modeling",
            "accuracy": 0.1571,
            "centered": 0.1571
          },
          {
            "task": "bigbench_repeat_copy_logic",
            "shots": 10,
            "type": "language_modeling",
            "accuracy": 0.0,
            "centered": 0.0
          },
          {
            "task": "squad",
            "shots": 10,
            "type": "language_modeling",
            "accuracy": 0.248,
            "centered": 0.248
          },
          {
            "task": "coqa",
            "shots": 0,
            "type": "language_modeling",
            "accuracy": 0.194,
            "centered": 0.194
          },
          {
            "task": "boolq",
            "shots": 10,
            "type": "multiple_choice",
            "accuracy": 0.488,
            "centered": -0.3474
          },
          {
            "task": "bigbench_language_identification",
            "shots": 10,
            "type": "multiple_choice",
            "accuracy": 0.266,
            "centered": 0.1925
          }
        ]
      },
      "9000": {
        "core": 0.208,
        "val_bpb": 0.862859,
        "cdpk": {
          "acc": 0.2781,
          "n": 250,
          "d": 899
        },
        "tasks": [
          {
            "task": "hellaswag_zeroshot",
            "shots": 0,
            "type": "multiple_choice",
            "accuracy": 0.428,
            "centered": 0.2373
          },
          {
            "task": "jeopardy",
            "shots": 10,
            "type": "language_modeling",
            "accuracy": 0.068,
            "centered": 0.068
          },
          {
            "task": "bigbench_qa_wikidata",
            "shots": 10,
            "type": "language_modeling",
            "accuracy": 0.496,
            "centered": 0.496
          },
          {
            "task": "arc_easy",
            "shots": 10,
            "type": "multiple_choice",
            "accuracy": 0.662,
            "centered": 0.5493
          },
          {
            "task": "arc_challenge",
            "shots": 10,
            "type": "multiple_choice",
            "accuracy": 0.348,
            "centered": 0.1307
          },
          {
            "task": "copa",
            "shots": 0,
            "type": "multiple_choice",
            "accuracy": 0.67,
            "centered": 0.34
          },
          {
            "task": "commonsense_qa",
            "shots": 10,
            "type": "multiple_choice",
            "accuracy": 0.218,
            "centered": 0.0225
          },
          {
            "task": "piqa",
            "shots": 10,
            "type": "multiple_choice",
            "accuracy": 0.694,
            "centered": 0.388
          },
          {
            "task": "openbook_qa",
            "shots": 0,
            "type": "multiple_choice",
            "accuracy": 0.35,
            "centered": 0.1333
          },
          {
            "task": "lambada_openai",
            "shots": 0,
            "type": "language_modeling",
            "accuracy": 0.368,
            "centered": 0.368
          },
          {
            "task": "hellaswag",
            "shots": 10,
            "type": "multiple_choice",
            "accuracy": 0.42,
            "centered": 0.2267
          },
          {
            "task": "winograd",
            "shots": 0,
            "type": "schema",
            "accuracy": 0.6154,
            "centered": 0.2308
          },
          {
            "task": "winogrande",
            "shots": 0,
            "type": "schema",
            "accuracy": 0.538,
            "centered": 0.076
          },
          {
            "task": "bigbench_dyck_languages",
            "shots": 10,
            "type": "language_modeling",
            "accuracy": 0.04,
            "centered": 0.04
          },
          {
            "task": "agi_eval_lsat_ar",
            "shots": 3,
            "type": "multiple_choice",
            "accuracy": 0.2565,
            "centered": 0.0707
          },
          {
            "task": "bigbench_cs_algorithms",
            "shots": 10,
            "type": "language_modeling",
            "accuracy": 0.414,
            "centered": 0.414
          },
          {
            "task": "bigbench_operators",
            "shots": 10,
            "type": "language_modeling",
            "accuracy": 0.119,
            "centered": 0.119
          },
          {
            "task": "bigbench_repeat_copy_logic",
            "shots": 10,
            "type": "language_modeling",
            "accuracy": 0.0,
            "centered": 0.0
          },
          {
            "task": "squad",
            "shots": 10,
            "type": "language_modeling",
            "accuracy": 0.246,
            "centered": 0.246
          },
          {
            "task": "coqa",
            "shots": 0,
            "type": "language_modeling",
            "accuracy": 0.202,
            "centered": 0.202
          },
          {
            "task": "boolq",
            "shots": 10,
            "type": "multiple_choice",
            "accuracy": 0.628,
            "centered": 0.0211
          },
          {
            "task": "bigbench_language_identification",
            "shots": 10,
            "type": "multiple_choice",
            "accuracy": 0.27,
            "centered": 0.1969
          }
        ]
      },
      "10000": {
        "core": 0.2076,
        "val_bpb": 0.858284,
        "cdpk": {
          "acc": 0.2792,
          "n": 251,
          "d": 899
        },
        "tasks": [
          {
            "task": "hellaswag_zeroshot",
            "shots": 0,
            "type": "multiple_choice",
            "accuracy": 0.448,
            "centered": 0.264
          },
          {
            "task": "jeopardy",
            "shots": 10,
            "type": "language_modeling",
            "accuracy": 0.03,
            "centered": 0.03
          },
          {
            "task": "bigbench_qa_wikidata",
            "shots": 10,
            "type": "language_modeling",
            "accuracy": 0.482,
            "centered": 0.482
          },
          {
            "task": "arc_easy",
            "shots": 10,
            "type": "multiple_choice",
            "accuracy": 0.63,
            "centered": 0.5067
          },
          {
            "task": "arc_challenge",
            "shots": 10,
            "type": "multiple_choice",
            "accuracy": 0.344,
            "centered": 0.1253
          },
          {
            "task": "copa",
            "shots": 0,
            "type": "multiple_choice",
            "accuracy": 0.64,
            "centered": 0.28
          },
          {
            "task": "commonsense_qa",
            "shots": 10,
            "type": "multiple_choice",
            "accuracy": 0.242,
            "centered": 0.0525
          },
          {
            "task": "piqa",
            "shots": 10,
            "type": "multiple_choice",
            "accuracy": 0.67,
            "centered": 0.34
          },
          {
            "task": "openbook_qa",
            "shots": 0,
            "type": "multiple_choice",
            "accuracy": 0.344,
            "centered": 0.1253
          },
          {
            "task": "lambada_openai",
            "shots": 0,
            "type": "language_modeling",
            "accuracy": 0.416,
            "centered": 0.416
          },
          {
            "task": "hellaswag",
            "shots": 10,
            "type": "multiple_choice",
            "accuracy": 0.428,
            "centered": 0.2373
          },
          {
            "task": "winograd",
            "shots": 0,
            "type": "schema",
            "accuracy": 0.6557,
            "centered": 0.3114
          },
          {
            "task": "winogrande",
            "shots": 0,
            "type": "schema",
            "accuracy": 0.5,
            "centered": 0.0
          },
          {
            "task": "bigbench_dyck_languages",
            "shots": 10,
            "type": "language_modeling",
            "accuracy": 0.076,
            "centered": 0.076
          },
          {
            "task": "agi_eval_lsat_ar",
            "shots": 3,
            "type": "multiple_choice",
            "accuracy": 0.3174,
            "centered": 0.1467
          },
          {
            "task": "bigbench_cs_algorithms",
            "shots": 10,
            "type": "language_modeling",
            "accuracy": 0.414,
            "centered": 0.414
          },
          {
            "task": "bigbench_operators",
            "shots": 10,
            "type": "language_modeling",
            "accuracy": 0.1381,
            "centered": 0.1381
          },
          {
            "task": "bigbench_repeat_copy_logic",
            "shots": 10,
            "type": "language_modeling",
            "accuracy": 0.0,
            "centered": 0.0
          },
          {
            "task": "squad",
            "shots": 10,
            "type": "language_modeling",
            "accuracy": 0.254,
            "centered": 0.254
          },
          {
            "task": "coqa",
            "shots": 0,
            "type": "language_modeling",
            "accuracy": 0.198,
            "centered": 0.198
          },
          {
            "task": "boolq",
            "shots": 10,
            "type": "multiple_choice",
            "accuracy": 0.602,
            "centered": -0.0474
          },
          {
            "task": "bigbench_language_identification",
            "shots": 10,
            "type": "multiple_choice",
            "accuracy": 0.288,
            "centered": 0.2167
          }
        ]
      },
      "11000": {
        "core": 0.216,
        "val_bpb": 0.851616,
        "cdpk": {
          "acc": 0.2825,
          "n": 254,
          "d": 899
        },
        "tasks": [
          {
            "task": "hellaswag_zeroshot",
            "shots": 0,
            "type": "multiple_choice",
            "accuracy": 0.464,
            "centered": 0.2853
          },
          {
            "task": "jeopardy",
            "shots": 10,
            "type": "language_modeling",
            "accuracy": 0.106,
            "centered": 0.106
          },
          {
            "task": "bigbench_qa_wikidata",
            "shots": 10,
            "type": "language_modeling",
            "accuracy": 0.502,
            "centered": 0.502
          },
          {
            "task": "arc_easy",
            "shots": 10,
            "type": "multiple_choice",
            "accuracy": 0.652,
            "centered": 0.536
          },
          {
            "task": "arc_challenge",
            "shots": 10,
            "type": "multiple_choice",
            "accuracy": 0.332,
            "centered": 0.1093
          },
          {
            "task": "copa",
            "shots": 0,
            "type": "multiple_choice",
            "accuracy": 0.64,
            "centered": 0.28
          },
          {
            "task": "commonsense_qa",
            "shots": 10,
            "type": "multiple_choice",
            "accuracy": 0.192,
            "centered": -0.01
          },
          {
            "task": "piqa",
            "shots": 10,
            "type": "multiple_choice",
            "accuracy": 0.678,
            "centered": 0.356
          },
          {
            "task": "openbook_qa",
            "shots": 0,
            "type": "multiple_choice",
            "accuracy": 0.386,
            "centered": 0.1813
          },
          {
            "task": "lambada_openai",
            "shots": 0,
            "type": "language_modeling",
            "accuracy": 0.404,
            "centered": 0.404
          },
          {
            "task": "hellaswag",
            "shots": 10,
            "type": "multiple_choice",
            "accuracy": 0.444,
            "centered": 0.2587
          },
          {
            "task": "winograd",
            "shots": 0,
            "type": "schema",
            "accuracy": 0.663,
            "centered": 0.326
          },
          {
            "task": "winogrande",
            "shots": 0,
            "type": "schema",
            "accuracy": 0.5,
            "centered": 0.0
          },
          {
            "task": "bigbench_dyck_languages",
            "shots": 10,
            "type": "language_modeling",
            "accuracy": 0.124,
            "centered": 0.124
          },
          {
            "task": "agi_eval_lsat_ar",
            "shots": 3,
            "type": "multiple_choice",
            "accuracy": 0.2957,
            "centered": 0.1196
          },
          {
            "task": "bigbench_cs_algorithms",
            "shots": 10,
            "type": "language_modeling",
            "accuracy": 0.428,
            "centered": 0.428
          },
          {
            "task": "bigbench_operators",
            "shots": 10,
            "type": "language_modeling",
            "accuracy": 0.1333,
            "centered": 0.1333
          },
          {
            "task": "bigbench_repeat_copy_logic",
            "shots": 10,
            "type": "language_modeling",
            "accuracy": 0.0,
            "centered": 0.0
          },
          {
            "task": "squad",
            "shots": 10,
            "type": "language_modeling",
            "accuracy": 0.246,
            "centered": 0.246
          },
          {
            "task": "coqa",
            "shots": 0,
            "type": "language_modeling",
            "accuracy": 0.186,
            "centered": 0.186
          },
          {
            "task": "boolq",
            "shots": 10,
            "type": "multiple_choice",
            "accuracy": 0.618,
            "centered": -0.0053
          },
          {
            "task": "bigbench_language_identification",
            "shots": 10,
            "type": "multiple_choice",
            "accuracy": 0.26,
            "centered": 0.1859
          }
        ]
      },
      "12000": {
        "core": 0.2163,
        "val_bpb": 0.844113,
        "cdpk": {
          "acc": 0.277,
          "n": 249,
          "d": 899
        },
        "tasks": [
          {
            "task": "hellaswag_zeroshot",
            "shots": 0,
            "type": "multiple_choice",
            "accuracy": 0.438,
            "centered": 0.2507
          },
          {
            "task": "jeopardy",
            "shots": 10,
            "type": "language_modeling",
            "accuracy": 0.056,
            "centered": 0.056
          },
          {
            "task": "bigbench_qa_wikidata",
            "shots": 10,
            "type": "language_modeling",
            "accuracy": 0.496,
            "centered": 0.496
          },
          {
            "task": "arc_easy",
            "shots": 10,
            "type": "multiple_choice",
            "accuracy": 0.628,
            "centered": 0.504
          },
          {
            "task": "arc_challenge",
            "shots": 10,
            "type": "multiple_choice",
            "accuracy": 0.324,
            "centered": 0.0987
          },
          {
            "task": "copa",
            "shots": 0,
            "type": "multiple_choice",
            "accuracy": 0.68,
            "centered": 0.36
          },
          {
            "task": "commonsense_qa",
            "shots": 10,
            "type": "multiple_choice",
            "accuracy": 0.306,
            "centered": 0.1325
          },
          {
            "task": "piqa",
            "shots": 10,
            "type": "multiple_choice",
            "accuracy": 0.692,
            "centered": 0.384
          },
          {
            "task": "openbook_qa",
            "shots": 0,
            "type": "multiple_choice",
            "accuracy": 0.404,
            "centered": 0.2053
          },
          {
            "task": "lambada_openai",
            "shots": 0,
            "type": "language_modeling",
            "accuracy": 0.404,
            "centered": 0.404
          },
          {
            "task": "hellaswag",
            "shots": 10,
            "type": "multiple_choice",
            "accuracy": 0.444,
            "centered": 0.2587
          },
          {
            "task": "winograd",
            "shots": 0,
            "type": "schema",
            "accuracy": 0.6593,
            "centered": 0.3187
          },
          {
            "task": "winogrande",
            "shots": 0,
            "type": "schema",
            "accuracy": 0.524,
            "centered": 0.048
          },
          {
            "task": "bigbench_dyck_languages",
            "shots": 10,
            "type": "language_modeling",
            "accuracy": 0.118,
            "centered": 0.118
          },
          {
            "task": "agi_eval_lsat_ar",
            "shots": 3,
            "type": "multiple_choice",
            "accuracy": 0.2696,
            "centered": 0.087
          },
          {
            "task": "bigbench_cs_algorithms",
            "shots": 10,
            "type": "language_modeling",
            "accuracy": 0.426,
            "centered": 0.426
          },
          {
            "task": "bigbench_operators",
            "shots": 10,
            "type": "language_modeling",
            "accuracy": 0.1714,
            "centered": 0.1714
          },
          {
            "task": "bigbench_repeat_copy_logic",
            "shots": 10,
            "type": "language_modeling",
            "accuracy": 0.0,
            "centered": 0.0
          },
          {
            "task": "squad",
            "shots": 10,
            "type": "language_modeling",
            "accuracy": 0.28,
            "centered": 0.28
          },
          {
            "task": "coqa",
            "shots": 0,
            "type": "language_modeling",
            "accuracy": 0.182,
            "centered": 0.182
          },
          {
            "task": "boolq",
            "shots": 10,
            "type": "multiple_choice",
            "accuracy": 0.534,
            "centered": -0.2263
          },
          {
            "task": "bigbench_language_identification",
            "shots": 10,
            "type": "multiple_choice",
            "accuracy": 0.276,
            "centered": 0.2035
          }
        ]
      },
      "13000": {
        "core": 0.2206,
        "val_bpb": 0.838301,
        "cdpk": {
          "acc": 0.2948,
          "n": 265,
          "d": 899
        },
        "tasks": [
          {
            "task": "hellaswag_zeroshot",
            "shots": 0,
            "type": "multiple_choice",
            "accuracy": 0.464,
            "centered": 0.2853
          },
          {
            "task": "jeopardy",
            "shots": 10,
            "type": "language_modeling",
            "accuracy": 0.18,
            "centered": 0.18
          },
          {
            "task": "bigbench_qa_wikidata",
            "shots": 10,
            "type": "language_modeling",
            "accuracy": 0.504,
            "centered": 0.504
          },
          {
            "task": "arc_easy",
            "shots": 10,
            "type": "multiple_choice",
            "accuracy": 0.66,
            "centered": 0.5467
          },
          {
            "task": "arc_challenge",
            "shots": 10,
            "type": "multiple_choice",
            "accuracy": 0.332,
            "centered": 0.1093
          },
          {
            "task": "copa",
            "shots": 0,
            "type": "multiple_choice",
            "accuracy": 0.65,
            "centered": 0.3
          },
          {
            "task": "commonsense_qa",
            "shots": 10,
            "type": "multiple_choice",
            "accuracy": 0.192,
            "centered": -0.01
          },
          {
            "task": "piqa",
            "shots": 10,
            "type": "multiple_choice",
            "accuracy": 0.706,
            "centered": 0.412
          },
          {
            "task": "openbook_qa",
            "shots": 0,
            "type": "multiple_choice",
            "accuracy": 0.364,
            "centered": 0.152
          },
          {
            "task": "lambada_openai",
            "shots": 0,
            "type": "language_modeling",
            "accuracy": 0.39,
            "centered": 0.39
          },
          {
            "task": "hellaswag",
            "shots": 10,
            "type": "multiple_choice",
            "accuracy": 0.452,
            "centered": 0.2693
          },
          {
            "task": "winograd",
            "shots": 0,
            "type": "schema",
            "accuracy": 0.685,
            "centered": 0.37
          },
          {
            "task": "winogrande",
            "shots": 0,
            "type": "schema",
            "accuracy": 0.544,
            "centered": 0.088
          },
          {
            "task": "bigbench_dyck_languages",
            "shots": 10,
            "type": "language_modeling",
            "accuracy": 0.104,
            "centered": 0.104
          },
          {
            "task": "agi_eval_lsat_ar",
            "shots": 3,
            "type": "multiple_choice",
            "accuracy": 0.2739,
            "centered": 0.0924
          },
          {
            "task": "bigbench_cs_algorithms",
            "shots": 10,
            "type": "language_modeling",
            "accuracy": 0.424,
            "centered": 0.424
          },
          {
            "task": "bigbench_operators",
            "shots": 10,
            "type": "language_modeling",
            "accuracy": 0.1667,
            "centered": 0.1667
          },
          {
            "task": "bigbench_repeat_copy_logic",
            "shots": 10,
            "type": "language_modeling",
            "accuracy": 0.0312,
            "centered": 0.0312
          },
          {
            "task": "squad",
            "shots": 10,
            "type": "language_modeling",
            "accuracy": 0.32,
            "centered": 0.32
          },
          {
            "task": "coqa",
            "shots": 0,
            "type": "language_modeling",
            "accuracy": 0.202,
            "centered": 0.202
          },
          {
            "task": "boolq",
            "shots": 10,
            "type": "multiple_choice",
            "accuracy": 0.508,
            "centered": -0.2947
          },
          {
            "task": "bigbench_language_identification",
            "shots": 10,
            "type": "multiple_choice",
            "accuracy": 0.282,
            "centered": 0.2101
          }
        ]
      },
      "14000": {
        "core": 0.2184,
        "val_bpb": 0.828823,
        "cdpk": {
          "acc": 0.2781,
          "n": 250,
          "d": 899
        },
        "tasks": [
          {
            "task": "hellaswag_zeroshot",
            "shots": 0,
            "type": "multiple_choice",
            "accuracy": 0.458,
            "centered": 0.2773
          },
          {
            "task": "jeopardy",
            "shots": 10,
            "type": "language_modeling",
            "accuracy": 0.188,
            "centered": 0.188
          },
          {
            "task": "bigbench_qa_wikidata",
            "shots": 10,
            "type": "language_modeling",
            "accuracy": 0.496,
            "centered": 0.496
          },
          {
            "task": "arc_easy",
            "shots": 10,
            "type": "multiple_choice",
            "accuracy": 0.65,
            "centered": 0.5333
          },
          {
            "task": "arc_challenge",
            "shots": 10,
            "type": "multiple_choice",
            "accuracy": 0.346,
            "centered": 0.128
          },
          {
            "task": "copa",
            "shots": 0,
            "type": "multiple_choice",
            "accuracy": 0.74,
            "centered": 0.48
          },
          {
            "task": "commonsense_qa",
            "shots": 10,
            "type": "multiple_choice",
            "accuracy": 0.18,
            "centered": -0.025
          },
          {
            "task": "piqa",
            "shots": 10,
            "type": "multiple_choice",
            "accuracy": 0.69,
            "centered": 0.38
          },
          {
            "task": "openbook_qa",
            "shots": 0,
            "type": "multiple_choice",
            "accuracy": 0.376,
            "centered": 0.168
          },
          {
            "task": "lambada_openai",
            "shots": 0,
            "type": "language_modeling",
            "accuracy": 0.428,
            "centered": 0.428
          },
          {
            "task": "hellaswag",
            "shots": 10,
            "type": "multiple_choice",
            "accuracy": 0.426,
            "centered": 0.2347
          },
          {
            "task": "winograd",
            "shots": 0,
            "type": "schema",
            "accuracy": 0.6374,
            "centered": 0.2747
          },
          {
            "task": "winogrande",
            "shots": 0,
            "type": "schema",
            "accuracy": 0.54,
            "centered": 0.08
          },
          {
            "task": "bigbench_dyck_languages",
            "shots": 10,
            "type": "language_modeling",
            "accuracy": 0.092,
            "centered": 0.092
          },
          {
            "task": "agi_eval_lsat_ar",
            "shots": 3,
            "type": "multiple_choice",
            "accuracy": 0.2304,
            "centered": 0.038
          },
          {
            "task": "bigbench_cs_algorithms",
            "shots": 10,
            "type": "language_modeling",
            "accuracy": 0.428,
            "centered": 0.428
          },
          {
            "task": "bigbench_operators",
            "shots": 10,
            "type": "language_modeling",
            "accuracy": 0.1524,
            "centered": 0.1524
          },
          {
            "task": "bigbench_repeat_copy_logic",
            "shots": 10,
            "type": "language_modeling",
            "accuracy": 0.0312,
            "centered": 0.0312
          },
          {
            "task": "squad",
            "shots": 10,
            "type": "language_modeling",
            "accuracy": 0.314,
            "centered": 0.314
          },
          {
            "task": "coqa",
            "shots": 0,
            "type": "language_modeling",
            "accuracy": 0.21,
            "centered": 0.21
          },
          {
            "task": "boolq",
            "shots": 10,
            "type": "multiple_choice",
            "accuracy": 0.508,
            "centered": -0.2947
          },
          {
            "task": "bigbench_language_identification",
            "shots": 10,
            "type": "multiple_choice",
            "accuracy": 0.264,
            "centered": 0.1903
          }
        ]
      },
      "15000": {
        "core": 0.2165,
        "val_bpb": 0.824079,
        "cdpk": {
          "acc": 0.2714,
          "n": 244,
          "d": 899
        },
        "tasks": [
          {
            "task": "hellaswag_zeroshot",
            "shots": 0,
            "type": "multiple_choice",
            "accuracy": 0.452,
            "centered": 0.2693
          },
          {
            "task": "jeopardy",
            "shots": 10,
            "type": "language_modeling",
            "accuracy": 0.22,
            "centered": 0.22
          },
          {
            "task": "bigbench_qa_wikidata",
            "shots": 10,
            "type": "language_modeling",
            "accuracy": 0.534,
            "centered": 0.534
          },
          {
            "task": "arc_easy",
            "shots": 10,
            "type": "multiple_choice",
            "accuracy": 0.642,
            "centered": 0.5227
          },
          {
            "task": "arc_challenge",
            "shots": 10,
            "type": "multiple_choice",
            "accuracy": 0.32,
            "centered": 0.0933
          },
          {
            "task": "copa",
            "shots": 0,
            "type": "multiple_choice",
            "accuracy": 0.71,
            "centered": 0.42
          },
          {
            "task": "commonsense_qa",
            "shots": 10,
            "type": "multiple_choice",
            "accuracy": 0.174,
            "centered": -0.0325
          },
          {
            "task": "piqa",
            "shots": 10,
            "type": "multiple_choice",
            "accuracy": 0.696,
            "centered": 0.392
          },
          {
            "task": "openbook_qa",
            "shots": 0,
            "type": "multiple_choice",
            "accuracy": 0.372,
            "centered": 0.1627
          },
          {
            "task": "lambada_openai",
            "shots": 0,
            "type": "language_modeling",
            "accuracy": 0.446,
            "centered": 0.446
          },
          {
            "task": "hellaswag",
            "shots": 10,
            "type": "multiple_choice",
            "accuracy": 0.47,
            "centered": 0.2933
          },
          {
            "task": "winograd",
            "shots": 0,
            "type": "schema",
            "accuracy": 0.652,
            "centered": 0.304
          },
          {
            "task": "winogrande",
            "shots": 0,
            "type": "schema",
            "accuracy": 0.502,
            "centered": 0.004
          },
          {
            "task": "bigbench_dyck_languages",
            "shots": 10,
            "type": "language_modeling",
            "accuracy": 0.136,
            "centered": 0.136
          },
          {
            "task": "agi_eval_lsat_ar",
            "shots": 3,
            "type": "multiple_choice",
            "accuracy": 0.2261,
            "centered": 0.0326
          },
          {
            "task": "bigbench_cs_algorithms",
            "shots": 10,
            "type": "language_modeling",
            "accuracy": 0.39,
            "centered": 0.39
          },
          {
            "task": "bigbench_operators",
            "shots": 10,
            "type": "language_modeling",
            "accuracy": 0.1333,
            "centered": 0.1333
          },
          {
            "task": "bigbench_repeat_copy_logic",
            "shots": 10,
            "type": "language_modeling",
            "accuracy": 0.0312,
            "centered": 0.0312
          },
          {
            "task": "squad",
            "shots": 10,
            "type": "language_modeling",
            "accuracy": 0.314,
            "centered": 0.314
          },
          {
            "task": "coqa",
            "shots": 0,
            "type": "language_modeling",
            "accuracy": 0.21,
            "centered": 0.21
          },
          {
            "task": "boolq",
            "shots": 10,
            "type": "multiple_choice",
            "accuracy": 0.516,
            "centered": -0.2737
          },
          {
            "task": "bigbench_language_identification",
            "shots": 10,
            "type": "multiple_choice",
            "accuracy": 0.238,
            "centered": 0.1617
          }
        ]
      },
      "16000": {
        "core": 0.2305,
        "val_bpb": 0.814735,
        "cdpk": {
          "acc": 0.2714,
          "n": 244,
          "d": 899
        },
        "tasks": [
          {
            "task": "hellaswag_zeroshot",
            "shots": 0,
            "type": "multiple_choice",
            "accuracy": 0.474,
            "centered": 0.2987
          },
          {
            "task": "jeopardy",
            "shots": 10,
            "type": "language_modeling",
            "accuracy": 0.176,
            "centered": 0.176
          },
          {
            "task": "bigbench_qa_wikidata",
            "shots": 10,
            "type": "language_modeling",
            "accuracy": 0.564,
            "centered": 0.564
          },
          {
            "task": "arc_easy",
            "shots": 10,
            "type": "multiple_choice",
            "accuracy": 0.65,
            "centered": 0.5333
          },
          {
            "task": "arc_challenge",
            "shots": 10,
            "type": "multiple_choice",
            "accuracy": 0.358,
            "centered": 0.144
          },
          {
            "task": "copa",
            "shots": 0,
            "type": "multiple_choice",
            "accuracy": 0.72,
            "centered": 0.44
          },
          {
            "task": "commonsense_qa",
            "shots": 10,
            "type": "multiple_choice",
            "accuracy": 0.196,
            "centered": -0.005
          },
          {
            "task": "piqa",
            "shots": 10,
            "type": "multiple_choice",
            "accuracy": 0.684,
            "centered": 0.368
          },
          {
            "task": "openbook_qa",
            "shots": 0,
            "type": "multiple_choice",
            "accuracy": 0.364,
            "centered": 0.152
          },
          {
            "task": "lambada_openai",
            "shots": 0,
            "type": "language_modeling",
            "accuracy": 0.416,
            "centered": 0.416
          },
          {
            "task": "hellaswag",
            "shots": 10,
            "type": "multiple_choice",
            "accuracy": 0.468,
            "centered": 0.2907
          },
          {
            "task": "winograd",
            "shots": 0,
            "type": "schema",
            "accuracy": 0.6703,
            "centered": 0.3407
          },
          {
            "task": "winogrande",
            "shots": 0,
            "type": "schema",
            "accuracy": 0.6,
            "centered": 0.2
          },
          {
            "task": "bigbench_dyck_languages",
            "shots": 10,
            "type": "language_modeling",
            "accuracy": 0.106,
            "centered": 0.106
          },
          {
            "task": "agi_eval_lsat_ar",
            "shots": 3,
            "type": "multiple_choice",
            "accuracy": 0.2348,
            "centered": 0.0435
          },
          {
            "task": "bigbench_cs_algorithms",
            "shots": 10,
            "type": "language_modeling",
            "accuracy": 0.392,
            "centered": 0.392
          },
          {
            "task": "bigbench_operators",
            "shots": 10,
            "type": "language_modeling",
            "accuracy": 0.1667,
            "centered": 0.1667
          },
          {
            "task": "bigbench_repeat_copy_logic",
            "shots": 10,
            "type": "language_modeling",
            "accuracy": 0.0,
            "centered": 0.0
          },
          {
            "task": "squad",
            "shots": 10,
            "type": "language_modeling",
            "accuracy": 0.316,
            "centered": 0.316
          },
          {
            "task": "coqa",
            "shots": 0,
            "type": "language_modeling",
            "accuracy": 0.214,
            "centered": 0.214
          },
          {
            "task": "boolq",
            "shots": 10,
            "type": "multiple_choice",
            "accuracy": 0.518,
            "centered": -0.2684
          },
          {
            "task": "bigbench_language_identification",
            "shots": 10,
            "type": "multiple_choice",
            "accuracy": 0.258,
            "centered": 0.1837
          }
        ]
      },
      "17000": {
        "core": 0.2566,
        "val_bpb": 0.806373,
        "cdpk": {
          "acc": 0.2659,
          "n": 239,
          "d": 899
        },
        "tasks": [
          {
            "task": "hellaswag_zeroshot",
            "shots": 0,
            "type": "multiple_choice",
            "accuracy": 0.48,
            "centered": 0.3067
          },
          {
            "task": "jeopardy",
            "shots": 10,
            "type": "language_modeling",
            "accuracy": 0.202,
            "centered": 0.202
          },
          {
            "task": "bigbench_qa_wikidata",
            "shots": 10,
            "type": "language_modeling",
            "accuracy": 0.514,
            "centered": 0.514
          },
          {
            "task": "arc_easy",
            "shots": 10,
            "type": "multiple_choice",
            "accuracy": 0.67,
            "centered": 0.56
          },
          {
            "task": "arc_challenge",
            "shots": 10,
            "type": "multiple_choice",
            "accuracy": 0.334,
            "centered": 0.112
          },
          {
            "task": "copa",
            "shots": 0,
            "type": "multiple_choice",
            "accuracy": 0.71,
            "centered": 0.42
          },
          {
            "task": "commonsense_qa",
            "shots": 10,
            "type": "multiple_choice",
            "accuracy": 0.244,
            "centered": 0.055
          },
          {
            "task": "piqa",
            "shots": 10,
            "type": "multiple_choice",
            "accuracy": 0.718,
            "centered": 0.436
          },
          {
            "task": "openbook_qa",
            "shots": 0,
            "type": "multiple_choice",
            "accuracy": 0.394,
            "centered": 0.192
          },
          {
            "task": "lambada_openai",
            "shots": 0,
            "type": "language_modeling",
            "accuracy": 0.42,
            "centered": 0.42
          },
          {
            "task": "hellaswag",
            "shots": 10,
            "type": "multiple_choice",
            "accuracy": 0.472,
            "centered": 0.296
          },
          {
            "task": "winograd",
            "shots": 0,
            "type": "schema",
            "accuracy": 0.6923,
            "centered": 0.3846
          },
          {
            "task": "winogrande",
            "shots": 0,
            "type": "schema",
            "accuracy": 0.55,
            "centered": 0.1
          },
          {
            "task": "bigbench_dyck_languages",
            "shots": 10,
            "type": "language_modeling",
            "accuracy": 0.116,
            "centered": 0.116
          },
          {
            "task": "agi_eval_lsat_ar",
            "shots": 3,
            "type": "multiple_choice",
            "accuracy": 0.3,
            "centered": 0.125
          },
          {
            "task": "bigbench_cs_algorithms",
            "shots": 10,
            "type": "language_modeling",
            "accuracy": 0.416,
            "centered": 0.416
          },
          {
            "task": "bigbench_operators",
            "shots": 10,
            "type": "language_modeling",
            "accuracy": 0.1619,
            "centered": 0.1619
          },
          {
            "task": "bigbench_repeat_copy_logic",
            "shots": 10,
            "type": "language_modeling",
            "accuracy": 0.0312,
            "centered": 0.0312
          },
          {
            "task": "squad",
            "shots": 10,
            "type": "language_modeling",
            "accuracy": 0.376,
            "centered": 0.376
          },
          {
            "task": "coqa",
            "shots": 0,
            "type": "language_modeling",
            "accuracy": 0.23,
            "centered": 0.23
          },
          {
            "task": "boolq",
            "shots": 10,
            "type": "multiple_choice",
            "accuracy": 0.61,
            "centered": -0.0263
          },
          {
            "task": "bigbench_language_identification",
            "shots": 10,
            "type": "multiple_choice",
            "accuracy": 0.288,
            "centered": 0.2167
          }
        ]
      },
      "18000": {
        "core": 0.257,
        "val_bpb": 0.798356,
        "cdpk": {
          "acc": 0.2692,
          "n": 242,
          "d": 899
        },
        "tasks": [
          {
            "task": "hellaswag_zeroshot",
            "shots": 0,
            "type": "multiple_choice",
            "accuracy": 0.478,
            "centered": 0.304
          },
          {
            "task": "jeopardy",
            "shots": 10,
            "type": "language_modeling",
            "accuracy": 0.21,
            "centered": 0.21
          },
          {
            "task": "bigbench_qa_wikidata",
            "shots": 10,
            "type": "language_modeling",
            "accuracy": 0.534,
            "centered": 0.534
          },
          {
            "task": "arc_easy",
            "shots": 10,
            "type": "multiple_choice",
            "accuracy": 0.652,
            "centered": 0.536
          },
          {
            "task": "arc_challenge",
            "shots": 10,
            "type": "multiple_choice",
            "accuracy": 0.334,
            "centered": 0.112
          },
          {
            "task": "copa",
            "shots": 0,
            "type": "multiple_choice",
            "accuracy": 0.73,
            "centered": 0.46
          },
          {
            "task": "commonsense_qa",
            "shots": 10,
            "type": "multiple_choice",
            "accuracy": 0.346,
            "centered": 0.1825
          },
          {
            "task": "piqa",
            "shots": 10,
            "type": "multiple_choice",
            "accuracy": 0.718,
            "centered": 0.436
          },
          {
            "task": "openbook_qa",
            "shots": 0,
            "type": "multiple_choice",
            "accuracy": 0.386,
            "centered": 0.1813
          },
          {
            "task": "lambada_openai",
            "shots": 0,
            "type": "language_modeling",
            "accuracy": 0.414,
            "centered": 0.414
          },
          {
            "task": "hellaswag",
            "shots": 10,
            "type": "multiple_choice",
            "accuracy": 0.486,
            "centered": 0.3147
          },
          {
            "task": "winograd",
            "shots": 0,
            "type": "schema",
            "accuracy": 0.6703,
            "centered": 0.3407
          },
          {
            "task": "winogrande",
            "shots": 0,
            "type": "schema",
            "accuracy": 0.56,
            "centered": 0.12
          },
          {
            "task": "bigbench_dyck_languages",
            "shots": 10,
            "type": "language_modeling",
            "accuracy": 0.118,
            "centered": 0.118
          },
          {
            "task": "agi_eval_lsat_ar",
            "shots": 3,
            "type": "multiple_choice",
            "accuracy": 0.2348,
            "centered": 0.0435
          },
          {
            "task": "bigbench_cs_algorithms",
            "shots": 10,
            "type": "language_modeling",
            "accuracy": 0.412,
            "centered": 0.412
          },
          {
            "task": "bigbench_operators",
            "shots": 10,
            "type": "language_modeling",
            "accuracy": 0.181,
            "centered": 0.181
          },
          {
            "task": "bigbench_repeat_copy_logic",
            "shots": 10,
            "type": "language_modeling",
            "accuracy": 0.0,
            "centered": 0.0
          },
          {
            "task": "squad",
            "shots": 10,
            "type": "language_modeling",
            "accuracy": 0.358,
            "centered": 0.358
          },
          {
            "task": "coqa",
            "shots": 0,
            "type": "language_modeling",
            "accuracy": 0.23,
            "centered": 0.23
          },
          {
            "task": "boolq",
            "shots": 10,
            "type": "multiple_choice",
            "accuracy": 0.606,
            "centered": -0.0368
          },
          {
            "task": "bigbench_language_identification",
            "shots": 10,
            "type": "multiple_choice",
            "accuracy": 0.276,
            "centered": 0.2035
          }
        ]
      },
      "19000": {
        "core": 0.2636,
        "val_bpb": 0.78974,
        "cdpk": {
          "acc": 0.2636,
          "n": 237,
          "d": 899
        },
        "tasks": [
          {
            "task": "hellaswag_zeroshot",
            "shots": 0,
            "type": "multiple_choice",
            "accuracy": 0.494,
            "centered": 0.3253
          },
          {
            "task": "jeopardy",
            "shots": 10,
            "type": "language_modeling",
            "accuracy": 0.232,
            "centered": 0.232
          },
          {
            "task": "bigbench_qa_wikidata",
            "shots": 10,
            "type": "language_modeling",
            "accuracy": 0.55,
            "centered": 0.55
          },
          {
            "task": "arc_easy",
            "shots": 10,
            "type": "multiple_choice",
            "accuracy": 0.656,
            "centered": 0.5413
          },
          {
            "task": "arc_challenge",
            "shots": 10,
            "type": "multiple_choice",
            "accuracy": 0.356,
            "centered": 0.1413
          },
          {
            "task": "copa",
            "shots": 0,
            "type": "multiple_choice",
            "accuracy": 0.71,
            "centered": 0.42
          },
          {
            "task": "commonsense_qa",
            "shots": 10,
            "type": "multiple_choice",
            "accuracy": 0.382,
            "centered": 0.2275
          },
          {
            "task": "piqa",
            "shots": 10,
            "type": "multiple_choice",
            "accuracy": 0.722,
            "centered": 0.444
          },
          {
            "task": "openbook_qa",
            "shots": 0,
            "type": "multiple_choice",
            "accuracy": 0.398,
            "centered": 0.1973
          },
          {
            "task": "lambada_openai",
            "shots": 0,
            "type": "language_modeling",
            "accuracy": 0.446,
            "centered": 0.446
          },
          {
            "task": "hellaswag",
            "shots": 10,
            "type": "multiple_choice",
            "accuracy": 0.496,
            "centered": 0.328
          },
          {
            "task": "winograd",
            "shots": 0,
            "type": "schema",
            "accuracy": 0.6557,
            "centered": 0.3114
          },
          {
            "task": "winogrande",
            "shots": 0,
            "type": "schema",
            "accuracy": 0.576,
            "centered": 0.152
          },
          {
            "task": "bigbench_dyck_languages",
            "shots": 10,
            "type": "language_modeling",
            "accuracy": 0.144,
            "centered": 0.144
          },
          {
            "task": "agi_eval_lsat_ar",
            "shots": 3,
            "type": "multiple_choice",
            "accuracy": 0.2391,
            "centered": 0.0489
          },
          {
            "task": "bigbench_cs_algorithms",
            "shots": 10,
            "type": "language_modeling",
            "accuracy": 0.408,
            "centered": 0.408
          },
          {
            "task": "bigbench_operators",
            "shots": 10,
            "type": "language_modeling",
            "accuracy": 0.1381,
            "centered": 0.1381
          },
          {
            "task": "bigbench_repeat_copy_logic",
            "shots": 10,
            "type": "language_modeling",
            "accuracy": 0.0312,
            "centered": 0.0312
          },
          {
            "task": "squad",
            "shots": 10,
            "type": "language_modeling",
            "accuracy": 0.362,
            "centered": 0.362
          },
          {
            "task": "coqa",
            "shots": 0,
            "type": "language_modeling",
            "accuracy": 0.232,
            "centered": 0.232
          },
          {
            "task": "boolq",
            "shots": 10,
            "type": "multiple_choice",
            "accuracy": 0.584,
            "centered": -0.0947
          },
          {
            "task": "bigbench_language_identification",
            "shots": 10,
            "type": "multiple_choice",
            "accuracy": 0.286,
            "centered": 0.2145
          }
        ]
      }
    }
  }
}